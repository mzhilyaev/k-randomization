%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt]{article}
%\documentclass[11pt,draft]{amsart}

% Custom styling.
\usepackage{custom}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}
%% Title matter
\title{Notes}
\author{Maxim Zhilyaev}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amssymb}

\newcommand{\bbx}{\pmb{x}}
\newcommand{\bbz}{\pmb{z}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\reals}{\mathbb{R}} % Real number symbol
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}} % Complex numbers
\newcommand{\integers}{\mathbb{Z}} % Integer symbol
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}} % Rational numbers
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\naturals}{\mathbb{N}} % Natural numbers
\newcommand{\N}{\mathbb{N}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Clear Reports}

Assume that the data comes from a universe $\cX = [d]$ of $d$ elements. Each individual $i \in [n]$ of $n$ users has a data element $x_i\in \cX$ . We will write a data entry in bold $\bbx_i \in \{0, 1\}^d $ to be the one-hot vector where $x_i$ is zero in every position except position $\bbx_i \in \cX$ , where it is one. Furthermore, we will denote a dataset $\bbx = \{x_1,\dots ,x_n\}$ to be a collection of all users' one-hot vectors. We will have each user donate his data $\bbx_i$. Further, we will inject some fake reports $z_j\in \cX$ for $j \in [m]$, and corresponding one-hot vector notation $\bbz_j$, where each data entry is chosen uniformly at random from $\cX$. We then pass $\{\bbx_i : i \in [n]\}$ and $\{\bbz_j : j \in [m]\} $ to an anonymizer that shuffles the data and makes it impossible to determine whether a data record is real or fake. We call this algorithm 
\[
M(\bbx_1,  \dots , \bbx_n) = \pi (\bbx_1, \dots , \bbx_n, \bbz_1, \dots , \bbz_m) \text{ where } \pi \text{ permutes its elements}. 
\]
We then compute the privacy loss of such an algorithm $M$. Equivalently, we could write the output as a histogram over the entire database, as in $M(\bbx_1,  \dots , \bbx_n) = \sum^n_{i=1} \bbx_i + \sum^m_{j=1} \bbz_j$. Note that rather than inject random noise to these counts, as in central differential privacy, we want to consider \emph{anonymized differential privacy}, where data records are transmitted through a mix net to break any identifiers with each data entry and the server sees the aggregated records in some random order. In this model, there is no trusted server that injects noise to ensure DP. Rather, the user needs to only trust the anonymizer to shuffle real and fake records.

We then consider the privacy loss for a general mechanism $M$. Consider an outcome $h \in \N^d$, which is a histogram over the full dataset domain and neighboring datasets $\bbx$ and $\bbx'$.

\begin{align}
L(h) = \log \left ( \frac{\Pr[M(\bbx) = h]}{\Pr[M(\bbx') = h]} \right )
\end{align}

If we can bound $L(h)$ by $\epsilon$ for any outcome $h$ then we say that $M$ is $\epsilon$-DP. If we can bound $L(h)$ by $\epsilon$ with probability at least $1 - \delta$ where the randomness is over $h \sim M(\bbx)$, then we say that $M$ is $(\epsilon, \delta)$-DP.

We now focus on $M$ being the mechanism described above, which injects $m$ fake reports. We can then write the privacy loss in the following way where we assume, without loss of generality, that $\bbx$ and $\bbx'$ only differ in the first record, i.e. $\bbx_i = \bbx'_i$ for all $i \ne 1$.

\begin{align*}
L(h) &= \log \left ( \frac{\Pr[x_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]}{\Pr[x'_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]} \right ) \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \bbx_1 - \sum^n_{i=2} \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h - \bbx'_1 - \sum^n_{i=2} x_i ]} \right )  \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \sum^n_{i=1}  \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h  - \sum^n_{i=1} \bbx_i  - (\bbx'_1 - \bbx_1) ]} \right )
\end{align*}

We denote  $\hat{h}$ to be the histogram of the fake records only $\hat{h} = h - \sum^n_{i=1} \bbx_i$, with respective counts in each histogram bin $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_d\}$.  Then the privacy loss ratio can be written as:

\begin{align*}
L(h) =  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h}  + \bbx_1 - \bbx'_1) ]} \right )
\end{align*}



The one-hot vectors $\bbx_1$  and $\bbx'_1$ may only differ in two positions, let these positions be $\ell$ and $\ell'$. 
$\bbx_1$ and $\bbx'_1$ must have opposite bit-values in positions $i$ and $i'$ (otherwise these vectors are identical). 
Without loss of generality assume $x_{1,\ell} = 1, x_{1,\ell'} = 0$ and $x'_{1,\ell} = 0, x'_{1,\ell'} = 1$.  
Adding $\bbx_1$ adds 1 to $h_i$, while subtracting $\bbx'_1$ removes 1 from $h_\ell'$.
Hence,  if $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d\} $, then  $\hat{h} + \bbx_1 -\bbx'_1 =  \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} -1, \dots, \hat{h}_d\} $.

Further, note that the count the fake bits $\hat{h}_\ell = \sum^m_{j=1} \bbz_{j,\ell}$ is a binomial distribution $h_\ell \sim \text{Bin}(m, 1/d)$, and the distribution of the fake bit counts across the bins takes the multinomial form $ \hat{h} \sim \text{Multinomial}(m,(1/d,\cdots, 1/d))$. We then aim to bound the following quantity.

\begin{align*}
 L(h) &= \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h} + \bbx_1 - \bbx'_1]} \right ) \\
 & = \log  \left ( \frac{ {m \choose \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d}}{ { m \choose hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} - 1, \dots, \hat{h}_d}} \right ) \\
 & = \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) 
\end{align*}

It must be stressed that for a given pair of $(\bbx_1, \bbx'_1)$, the corresponding position pair $(\ell,\ell')$ where their bits are different is fixed, and the privacy loss only surfaces while observing the counts in the corresponding histogram bins $(h_\ell, h_{\ell'})$.  It's entirely possible to see high ratio between counts in some other histogram bins, but it wouldn't contribute to the privacy loss for a concrete pair $(\bbx_1, \bbx'_1)$.  This observation allows us to focus only on a single pair of the histogram bins, ignoring the rest of the histogram as immaterial.  

By applying a Chernoff bound, we have a bound (symmetric for the upper and lower tail) for the sum of the fake bits in any bin $\hat{h}_k = \sum^m_{j=1} z_{j,k},  k \in [d]$

\begin{align*}
 \Pr \left [ \left | \hat{h}_k - \frac{m}{d} \right | > t \frac{m}{d} \right ]  \le 2 e^{- \frac{m}{d} \frac{t^2}{3}}, \qquad \text{ for  } 0 < t < 1.
\end{align*}

Choose $t$ to fit the expression below, hence $t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }$.
%\begin{align}
%2e^{- \frac{m}{d} \frac{t^2}{3}} = \frac{\delta}{2}  \\
%- \frac{m}{d} \frac{t^2}{3} = \log{\frac{\delta}{4}}  \\
% \frac{m}{d} \frac{t^2}{3} = \log{\frac{4}{\delta}} \\
% t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }
%\end{align}
Using this expression for $t$ turns our Chernoff bound into the following,
 
 \begin{align}
 Pr \left [ \left | \bar{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \frac{\delta}{2}
\end{align}

Given any pair of the histogram bins at positions $(\ell,\ell')$, the probability of observing large deviation from the mean in at least one bin obeys the unions bound.
\[
\Pr \left [ \max_{k \in (\ell,\ell')} \left | \hat{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \delta
\]

We then condition on the event that both counts $\hat{h}_l$ or $\hat{h}_{l'}$ fall in the interval $m/d \pm \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}$, which event occurs with probability at least $1 - \delta$.  
Conditioned on there being the given number of fake records, we can upper bound the privacy ratio $L(h)$

 \begin{align}
 L(h) =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \le \log  \left ( \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \le \epsilon 
\end{align}

From the above, we then get a condition on the number of fake records, m, to ensure DP.
 \begin{align*}
& \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^\epsilon  \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) - 1 \ge 0 \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} \left ( \sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1) \right ) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  \ge 0 \\
\implies &\sqrt{  \frac{m}{d}}  \ge  \frac { \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  } { e^\epsilon - 1 } \\
\implies & \frac{m}{d}  \ge 3\log {\frac{4}{\delta}} \left ( \frac{e^\epsilon +1}{e^\epsilon - 1} \right )^2
\end{align*}

As for the lower bound of $L(h)$, it's met if the upper bound is met.
 \begin{align*}
 L(h) & =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \ge \log  \left ( \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \ge - \epsilon  \\
\implies &  \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \ge e^{-\epsilon} \\
 \implies &  \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} \le e^{\epsilon}  \\
\implies &   \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} < \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^{\epsilon} 
\end{align*}


%%%%%%%%%%%%%%%%%%% - but flipping with fake records

\section{Fake records and bit flipping}

We now apply the exact same protocol, whereby users produce $n$ real and $m$ fake reports, but require each 1-hot vector to bit bit-flipped with frequency $q$.  A randomization procedure $\cR(y)$ flips each bit of an arbitrary 1-hot-vector $y$ with probability $q$ and keeps it the same with probability $p=1-q$.  The resulting mechanism $M_r$ becomes a permutation of randomized true and fake records:
\[
M_r(\bbx_1,  \dots , \bbx_n) = \pi (\cR(\bbx_1), \dots , \cR(\bbx_n), \cR(\bbz_1), \dots , \cR(\bbz_m)) \text{ where } \pi \text{ permutes its elements}. 
\]

Without los of generality assume $\bbx_1$ is replaced with $\bbx'_1$ to receive a neighboring data set $\bbx'$.  The outcome is a histogram $g \in \N^d$ containing sums of randomized bits in each dimension, and the privacy loss:

\begin{equation} \label{eq:lgbound}
L(g) = \log \left ( \frac{\Pr[M_r(\bbx) = g]}{\Pr[M_r(\bbx') = g]} \right )
\end{equation}

The combined set $\bbx + \bbz$ gives raise to a histogram $h \in \N^d$ received by applying the before discussed mechanism M (clear true records plus fake records).  Hence, the $Pr [ M_r(x) = g ]$ can be written as a sum of probabilities over the domain of $h$:

 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ]  \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   }{    \sum_{h' \in \N^d}  Pr \left [  M(\bbx') = h' \right ] \cdot   Pr[ g | h']   } \right )
\end{align*}

Noting that
\[
 Pr \left [  M(\bbx) = h \right ]  =  Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]
 \] 
 
And regrouping  the privacy loss ratio to have summands with same $Pr \left [  M(\bbx) = h \right ]$ in identical positions in numerator and denominator, and applying \eqref{lem:rsbound} we have:
 \begin{align*}
& \log \left ( \max_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \text{ , and } \\
& L(g) \ge  log \left ( \min_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right ) \\
\end{align*}

Probabilities $Pr \left [  M(\bbx) = h \right ]$ and $ Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]$ cancel each other out in each ratio, hence giving us the bounds of the privacy loss over domain of $h$ .

\begin{align*}
\log \left ( \max_{h \in \N^d} \left ( \frac{  Pr [ g | h ]   } { Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \ge  log \left ( \min_{h \in \N^d}  \left ( \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right )
\end{align*}

Since bits are flipped independently, the probability of finding certain number of bits in a particular histogram bin $g_l$ depends only on how many not-yet-randomized set bits there are in the dimension $l$, that is the value of $h_l$.  Such independence allows to re-write $Pr [ g | h]$ as a product of probabilities for each dimension.
 \[
Pr [ M_r(x) = g ] = Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] = \prod_{i=1}^d Pr[ g_i | h_i] 
\]

Without loss of generality assume that $\bbx_1$ and $\bbx'_1$ differ in the first and second positions, that is $\bbx_{1,1} = 1, \bbx_{1,2} = 0$ and $\bbx'_{1,1} = 0, \bbx'_{1,2} = 1$, then
\begin{align*}
 & h  - \bbx_1 + \bbx'_1 = \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} \\
 \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{ Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] } { Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} ]  } \\
  \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{  Pr[ g_1 | h_1] Pr[ g_2 | h_2] \prod_{i=3}^d Pr[ g_i | h_i]  } {  Pr[ g_1 | h_1 - 1] Pr[ g_2 | h_2 + 1] \prod_{i=3}^d Pr[ g_i | h_i]  }  \\
   \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } =   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  } 
\end{align*}

Plugging the above formula into \eqref{eq:lgbound}, the privacy loss bounds become:
\begin{align*}
& \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right ) \right ) \ge L(g) \ge  \min_{h \in \N^d}  \left ( \log \left (  \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }   \right ) \right ) \\
\implies &  \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \max_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right) \ge L(g) \text{ , and } \\
&  L(g) \ge \min_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \min_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right)
\end{align*}

Basically, the privacy loss is bound by the sum of privacy losses in each of the affected dimensions.  Which enables relatively simple path to the bound.   We employ the results of lemma \eqref{lem:rs2}, which says that if $\cR$ is $(\epsilon, \delta)$-private on a collection $r$ bits, it's also $(\epsilon, \delta)$-private on collection of $r+1$ bits.  Therefore, a privacy loss could be bounded for the $m$ fake records only, and that will provide sufficient noise for the extra $n$ real records.  

Suppose that $r$ fake records (out of $m$) happened to have zero bits in the affected dimentions ($1$ and $2$).  We will show later that $r \to m$,  for  large $d$.  Then we are bounding the product of privacy loss ratios in both dimentions to stay between $e^{-\epsilon}$ and $e^\epsilon$ with probability $1-\delta$.
\begin{align} \label{lem:fk101}
P \left (   e^\epsilon \ge \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \ge \frac{1}{e^\epsilon} \right ) \le 1 - \delta
\end{align}

We achieve condition of \eqref{lem:rs201} by boiunding the ratio in each column separately with probability $1-\delta/2$.
\begin{align} \label{lem:fk101}
P \left (   e^\epsilon \ge \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \ge \frac{1}{e^\epsilon} \right ) \le 1 - \delta
\end{align}



\section{Appendix: Bit flipping in single dimension}


There are $n+1$ single bit records being sent through a shuffler.  Before sending, each bit is randomized with $\cR$ - that is, flipped with probability $q$  and kept unchanged with probability $p=1-q$.  the outcome $S$ is the sum of randomized bits. Let $D$ be a set of $n$ bits and construct a neighboring pair of datasets by adding to $D$ a set bit and a 0 zero bit.  Then the privacy loss ratio at any given value of $S$ is expressed as:
\begin{equation} \label{eq:} 
R(S|D)= \frac{P(S|D \cup 0)}{P(S|D \cup 1)}
\end{equation}

Each probability allows conditioning on possible values the added bit could generate:
\begin{align}\label{lem:rs100}
P(S|D \cup 0) = p \cdot P(S | D ) + q \cdot P( S - 1 | D) 
\end{align}

Indeed, if $0$ bit is randomized to itself (with probability $p$), then $S$ must be generated by $D$ alone, while if $0$ bit was flipped (with probability $q$) then $D$ must generate $S-1$ total bit sum. Similarly 
\begin{align}
P(S|D \cup 1) = p  \cdot P(S -1 | D ) + q \cdot P( S  | D) 
\end{align}

Combining two conditioning expressions into the privacy loss ratio one arrives to:
\begin{equation} \label{eq:plratio}
R(S|D)=  \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  } = \frac{ p \frac{P(S | D )}{P(S - 1| D )} + q } { p + q \frac{P(S | D )}{P(S - 1| D )} }
\end{equation}

Let $\rho(S) = \frac{P(S | D )}{P(S - 1| D )}$ be a \textbf{probability ratio} between adjacent values of $S$.  It's related to $R(S)$ as in:
\begin{equation} \label{eq:plratio1}
R(S|D) =  \frac{ p \frac{P(S | D )}{P(S - 1| D )} + q } { p + q \frac{P(S | D )}{P(S - 1| D )} } = \frac{q + p\rho(S)}{p + q\rho(S)}
\end{equation}
Let $g(x) = \frac{q + px}{p + qx}$, the function $g$ is increasing over $x>0$, since
\[ g'(x) = \frac{p-q}{(p+qx)^2} > 0. \]
Therefore, properties of monotonicity and extrema established for $\rho(S)$  carry over to $R(S)$ as well.
 
 If  $D$ contains $m$ set bits, then the distribution of $S$ is a sum of two binomial distributions (a Poisson Binomial distribution): 
 \[ S  \sim Bin(m,p) + Bin(n-m,q) \]
 
 \begin{lem} \label{lem:rs1}
When 0-bit is replaced with a set bit, the resulting privacy loss ratio $R(S)$ decreases monotonically as $S$ grows, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}
\begin{pf}
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}
\end{pf}

 \begin{lem} \label{lem:rs1}
When 0-bit is replaced with a set bit, the resulting privacy loss ratio $R(S)$ decreases monotonically as $S$ grows, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}
\begin{pf}
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}
\end{pf}

\begin{lem} \label{lem:rs3}
Denote a collections of $n$ bits containing $r$ set bits and $n-r$ zero bits as $D_r$. Further denote the corresponding quantities:
\begin{itemize}
\item privacy loss ratio at a particular value $S$ as $R(S|D_r) = \frac{P(S|D_r)}{P(SD'_r)}z$
\item probability ratio at a particular value $S$ as $\rho(S|D_r) = \frac{P(S|D_r)}{P(S-1|D_r)}$
\item expected value of $S$ as $\mu_r = p \cdot r + q \cdot (n-r)$
\end{itemize}
Choose a distance $l$ such that $l \ge npq$, then 
\[ \rho[\mu_r - l|D_r] \le \rho[\mu_0 - (l+2)|D_0] \]
That is, the probability ratio for any collection is bound by the probability ratio of the zero collection. 
\end{lem}
\begin{pf}
PROOF IS INVOLVED AND WILL BE GIVEN LATER IN APPENDIX.  Max needs to fix Dave's notations, skipping for now.
\end{pf}

From lemma \eqref{lem:rs3} immediately follow corollaries below
\begin{cor}
For left deviations $l \ge npq$ from the mean the privacy loss ratio for the collection of $n$ bits is bounded by the privacy loss ratio for the collection of $n$ zero bits
\[ R[\mu_r - l|D_r] \le R[\mu_0 - (l+2)|D_0] \]
\end{cor}
\begin{pf}
From  \eqref{eq:plratio1} \[ R(S|D_r) = \frac{q + p\rho_r(S)}{p + q\rho_r(S)} \] 
Given that $\rho_r(S) \le \rho_0(S)$, we have 
\begin{align*}
R(S|D_r) = \frac{q + p\rho_r(S)}{p + q\rho_r(S)} \le R(S|D_0) = \frac{q + p\rho_0(S)}{p + q\rho_0(S)} \\
qp + q^2\rho_0(S) + p^2 \rho_r(S) + pq \cdot \rho_0(S) \cdot \rho_r(S) \le qp + p^2\rho_0(S) + q^2 \rho_r(S) + pq \cdot \rho_0(S) \cdot \rho_r(S) \\
\rho_r(S) (p^2 - q^2) \le \rho_0(S) (p^2 - q^2)  \\
\rho_r(S) \le \rho_0(S)
\end{align*} 
\end{pf}

The next corollary bounds the right tail of distribution
\begin{cor}
For right deviations $l \ge npq$ from the mean, the privacy loss ratio for the collection of $n$ bits is bounded by the privacy loss ratio for the collection of $n$ set bits
\[ R[\mu_r + l|D_r] \le R[\mu_n + l+2 |D_n] \]
\end{cor}
\begin{pf}
TODO - proving by symmetry between $D_0$ and $D_n$ distributions. 
\end{pf}

Using the bound \eqref{lem:rs201} and the fact that all-zero and all-set bit collections provide identical bounds to the privacy loss ratio for the left and the right side of distribution, we finally arrive to an important theorem.
\begin{prop}
randomization procedure $\cR$ is $(\epsilon, \delta)$-private on a collection $n$ bits, when the flipping frequency $q$ obeys the bound below
\begin{equation} \label{eq:fullbound}
q \ge  \frac { 3 \cdot ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon} - 2 \frac{p-q}{npq} \right ]^2 }
\end{equation}
\end{prop}
Note that the term $2 \frac{p-q}{npq}$ appeared due to $l+2$ correction of both corollaries above.  For sufficiently large $n$, this term diminishes to zero, which simplifies the bound to the form of lemma  \egref{lem:rs203}

\begin{lem} \label{lem:rs2}
If randomization procedure $\cR$ is $(\epsilon, \delta)$-private on a collection $n$ bits, it's also $(\epsilon, \delta)$-private on a collection of $n+1$ bits.  In other words, if $n$ bit are protected with flipping frequency $q$, then all collections of greater size are also protected with same $q$. 
\end{lem}
\begin{pf}
Let $D_n$ be a collection of $n$ bits and derive a neighboring collection $D'_n$ by replacing a single bit.  Since $\cR$ is $(\epsilon, \delta)$-private for $n$ bits, then for any set of outcomes $W$
\begin{align}\label{lem:rs101}
 && P(\cR(D'_n) \in W) \le \exp(\epsilon)P(\cR(D_n) \in W) + \delta \\
 \implies && P(\cR(D'_n) \in W) - \exp(\epsilon)P(\cR(D_n) \in W) \le \delta
\end{align}
 
 Now add a 0 bit to both $D_n$ and $D'_n$.  The probability of the extended collections $D_{n+1}$ and $D'_{n+1}$ generating a particular outcome $S$ is given by  \eqref{lem:rs100}
 \begin{align}\label{lem:rs102}
P(\cR(D_{n+1}) = S) = P(\cR(D_n)=S) \cdot p + P(\cR(D_n)=S-1) \cdot q
\end{align}

Assume that $\cR(D_{n+1})$ is not $(\epsilon, \delta)$-private, then there must exists a set $W'$ such that 
\begin{align}\label{lem:rs103}
P(\cR(D'_{n+1}) \in W') > \exp(\epsilon)P(\cR(D_{n+1}) \in W') + \delta
\end{align}
 
Suppose $W'$ contains $r$ distinct outcomes $W'=\{S_1, S_2, \cdots, S_r\}$, then
\begin{align}
P(\cR(D_{n+1}) \in W') = \sum_i^r P( \cR(D_{n+1})  = S_i) \\
=  \sum_i^r \left [ P( \cR(D_{n})  = S_i) + P(\cR(D_n)=S_i-1) \cdot q \right ]  \text{ by  \eqref{lem:rs102} } \\
= p \sum_i^r P( \cR(D_{n})  = S_i)  + q \sum_i^r  P( \cR(D_{n})  = S_i - 1)  \label{lem:rs103}
\end{align}
 
 Define a set $W'' = \{S_1 - 1, S_2 - 1, \cdots, S_r - 1\}$, then \eqref{lem:rs100} can be rewritten in the form membership probabilities.  
 \begin{align}
P(\cR(D_{n+1}) \in W') = p \sum_i^r P( \cR(D_{n})  = S_i)  + q \sum_i^r  P( \cR(D_{n})  = S_i - 1) \\
= p \cdot P(\cR(D_n) \in W') + q \cdot P(\cR(D_n) \in W'')
\end{align}

In the same fashion we arrive to the expression of $P(\cR(D'_{n+1}) \in W')$
 \begin{align}
P(\cR(D'_{n+1}) \in W') = p \cdot P(\cR(D'_n) \in W') + q \cdot P(\cR(D'_n) \in W'')
\end{align}

Plugging the above probabilities into  \eqref{lem:rs103}, we arrive to an inequality that must hold if $\cR(D_{n+1})$ is not $(\epsilon, \delta)$-private.
 \begin{align}
 P(\cR(D'_{n+1}) \in W') > \exp(\epsilon)P(\cR(D_{n+1}) \in W') + \delta \\
P(\cR(D'_{n+1}) \in W') - \exp(\epsilon)P(\cR(D_{n+1}) \in W') > \delta \\
p \cdot P(\cR(D'_n) \in W') + q \cdot P(\cR(D'_n) \in W'') - p \cdot P(\cR(D_n) \in W') - q \cdot P(\cR(D_n) \in W'') > \delta \\
p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] > \delta   \label{lem:rs105}
\end{align}

However \eqref{lem:rs101} implies that 
 \begin{align*}
&& \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] \le \delta \\
\text{and} &&  \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le \delta \\
\implies && p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le p\delta + q\delta \\
\implies && p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le \delta (p+q) \\
\implies && p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le \delta
\end{align*}

Which contradicts  \eqref{lem:rs105} and proves the lemma.
\end{pf}


\subsection{properties of zero valued collection}
A homogenous collection of $n$ zero bits is an important spacial case, hence we present findings for it below.  Let $D$ consists of $n$ zero bits, then the neighbor $D'$ is achieved by replacing a zero bit with set bit. The outcome of applying procedure c is a sum of randomized bits $S$. The following relationships hold.

\begin{align}
\mu = E(S) = q \cdot n \\
P(S=i | D ) = \binom{n}{i}q^ip^{n-i} \\
P(S=i | D' ) = \binom{n-1}{i}q^{i+1}p^{n-1-i} +   \binom{n-1}{i-1}q^{i-1}p^{n-i+1}  \\
R(i)  = \frac{P(s=i | D')}{P(s=i | D)} = \frac{ \binom{n}{i}q^ip^{n-i} }{  \binom{n-1}{i}q^{i+1}p^{n-1-i} +   \binom{n-1}{i-1}q^{i-1}p^{n-i+1} } \\
\frac{1}{R(i)} =  \frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q}
\end{align}

By applying Chernoff bound to the distribution of $s$, we receive
 \begin{align}
P(|S - \mu| > t\mu) \le 2 e^{- \frac{t^2\mu}{3}}
\end{align}

Setting $t=\sqrt{\frac{3}{\mu} ln\frac{2}{\delta}}$ one arrives to
 \begin{align}
P \left (|S - \mu| >\sqrt{3nq \cdot ln\frac{2}{\delta}} \right ) \le \delta
\end{align}
 
Setting $l=\sqrt{3nq \cdot ln\frac{2}{\delta}}$, one is ensured that values of $P(S \in [\mu - l, \mu + l ]) \ge 1- \delta$. Conditioned on  $S \in [\mu - l, \mu + l ]$ we  bound the privacy loss ratio $R(i)$ in this interval in the following way:
\begin{align}
  && e^\epsilon \ge R(i) \ge e^{-\epsilon} \\ 
 \implies &&  e^{-\epsilon} \le \frac{1}{R(i)} \le e^\epsilon \\
  \implies &&  e^{-\epsilon} \le \frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q} \le e^\epsilon
\end{align}

We first bound the left side of the inequality, setting $i = \mu - l$
\begin{align}
\frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{n-(\mu-l)}{n}\frac{q}{p} + \frac{\mu-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{n-(nq-l)}{n}\frac{q}{p} + \frac{nq-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{np + l}{n}\frac{q}{p} + \frac{nq-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
q + \frac{l}{n}\frac{q}{p} +  p - \frac{l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
1 - \frac{l}{n} \frac{p-q}{pq}  \ge e^{-\epsilon}. \\
l \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q}  
\end{align}

Plugging expression for $l$ one arrives to the bound of $q$
\begin{align}
\sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q} \\
 \frac {(p - q)^2} {q \cdot p^2} \le   \frac { n \left [ 1 - e^{-\epsilon}\right ]^2 } { 3  ln\frac{2}{\delta}}  \\
 \text{ since }  \frac {(p - q)^2} { p^2} \le 1 \text{ , then }  \\
  \frac {(p - q)^2} {q \cdot p^2} \le \frac{1}{q}  \le   \frac { n \left [ 1 - e^{-\epsilon}\right ]^2 } { 3  ln\frac{2}{\delta}}  \\
  q \ge  \frac { 3  ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon}\right ]^2 } \label{lem:rs201}
\end{align}

In a similar fashion, one arrives to the right side bound
\begin{align}
i = \mu + l \\
\frac{l}{n} \frac{p-q}{pq}  \le e^\epsilon - 1 \\
\sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ e^\epsilon - 1  \right ] \frac { npq}{p - q} \\
  q \ge  \frac { 3  ln\frac{2}{\delta}} { n \left [ e^{\epsilon} - 1\right ]^2 }  \label{lem:rs202}
\end{align}

Note that since $x + 1/x \ge 2$, then 
\[ e^{\epsilon} - 1 \ge 1 - e^{-\epsilon} \]
Which implies that if $q$ bound  \eqref{lem:rs201} is met, then \eqref{lem:rs202} is also met, which leads to the following lemma

 \begin{lem} \label{lem:rs203}
 Bit flipping randomization procedure $\cR$ applied to a collection of $n$ zero bits is $(\epsilon, \delta)$-private if the bit flipping frequency $q$ satisfies \eqref{lem:rs201}
 \begin{align*}
  q \ge  \frac { 3 \cdot ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon}\right ]^2 }\end{align*}
\end{lem}

By using the symmetry property of all-zero and all-set bits distributions, one arrives at the the identical statement for all-set bits collection
 \begin{lem} \label{lem:rs204}
 Bit flipping randomization procedure $\cR$ applied to a collection of $n$ set bits is $(\epsilon, \delta)$-private if the bit flipping frequency $q$ satisfies \eqref{lem:rs201}
 \begin{align*}
  q \ge  \frac { 3 \cdot ln\frac{2}{\delta}} { n l\eft [ 1 - e^{-\epsilon}\right ]^2 }
  \end{align*}
\end{lem
}

%%% Bounding ratio of sums by max ratio of terms

\section{Appendix: Ratios of sums: properties}

Here we establish some results around bounding and comparing ratios of sums, which will be useful in working with the privacy ratio.


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}


 \newpage
 \section{IGNORE BELOW THIS LINE}
 
 Suppose a new 1-bit is added to both collections, then $R_{n+1}(S)$ is derived by conditioning
 \begin{align*}
R_{n+1}(S) = \frac{ P(S|D \cup 1) }{ P(S|D' \cup 1) } = \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p}
\end{align*}
 
By lemma \eqref{lem:rsbound} and lemma  \eqref{lem:rs1} we have
\begin{align}
 & \frac{P(S-1|D)}{P(S-1|D'} \ge  \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p} \ge \frac{P(S|D)}{P(SD'} \\
\implies & R_n(S-1) \ge R_{n+1}(S) \ge R_n(S) \\
\implies & R_n(S) \ge R_{n+1}(S+1) \ge R_n(S+1) \label{lem:rsb2}
\end{align}
 
Suppose $R_n$ is $(\epsilon, \delta)$-private. Since $R_n$ is monotonically decreasing with $S$ (lemma  \eqref{lem:rs1} ), there exist two values $\alpha + \beta \le \delta$, such that $R_n$ is upper bounded on the left at a particular limiting value $S_\alpha$ 
 \begin{align}  \label{lem:rsb3}
 R_n(S_\alpha) \le e^\epsilon \text{ and } P_n(S \le S_\alpha) \le \alpha 
\end{align}
And it's low bounded on the right at a particular limiting value $S_\beta$
 \begin{align}  \label{lem:rsb4}
 R_n(S_\beta) \ge \frac{1}{e^\epsilon} \text{ and } P_n(S \ge S_\beta) \le \beta
\end{align}
 
Consider the left (upper) bound first, and recall that according to \eqref{lem:rsb2}
\[
 R_n(S_\alpha) \ge R_{n+1}(S_\alpha+1) \ge R_n(S_\alpha+1)
 \]
 
$R_{n+1}(S_\alpha+1)$ is bounded because  $R_n(S_\alpha)$ is bounded per \eqref{lem:rsb3}.  Hence, $R_{n+1}$ could only be over the bound at $S_\alpha$, however the cumulative sum of probabilities up to $S_\alpha$ is always less for $n+1$ bits than for $n$ bits.
 \begin{align}  \label{lem:rsb5}
P_{n+1}(S \le S_\alpha) \le P_{n}(S \le S_\alpha)
\end{align}
We shall prove \eqref{lem:rsb5} in a moment. The important fact is that $R_n$ upper bounds $R_{n+1}$ at the left tail of distribution of $S$. 

Similarly, 
 
 
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}



\begin{lem} \label{lem:rsreduce}
 The privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}

 the privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.







Note that $\frac{P(S | D )}{P(S - 1| D )}$ as a \textbf{probability ratio} between adjacent values of $S$.  It's easy to see that privacy loss ratio maximizes when \textbf{probability ratio} maximizes. 

Recall that $p>q$ and consider two positive values $A$ and $B$ 
\begin{align*}
\frac{ p \cdot A + q } { p + q \cdot A }  \ge \frac{ p \cdot B + q } { p + q \cdot B } \\
p^2A + q^2B \ge p^2 B + q^2 A  \\
A (p^2 - q^2) \ge B (p^2 - q^2)  \\
A \ge B
\end{align*}

The above confirms that the distributions with largest \textbf{probability ratio} also exhibit larger privacy loss ratio. Hence we can focus on studying \textbf{probability ratio}  instead of privacy loss ratio and choose those $D$ that demonstrate sharpest decrease of probabilities in the left tail.




Consider a collection $D$ of $N$ bits, subjected to randomization procedure $\cR$, whereby a bit is flipped with probability $q$ and kept unchanged with probability $p=1-q$.  The outcome of $\cR$ is a $a$ - sum of bits after randomization. The neigboring set $D_m$ is recieved form Assuming that $D$ contains $m$ set bits, we consider a privacy loss ratio $R_s$ computed for the outcome $s$:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 


consisting of $m$ ones and $N-m$ zeros.  Denote probability of number of successes for that collection as $P(S|D)$.
The probability ratio at $s$  is given by:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 

Denote expectation of $s$ as $\mu$:
\[ \mu = mp + (N-m)q \]

For simplicity, denote probabilities at $s$ for $D$ as:
\[ P(s|D) = P_{s} \]

It's known that for all $s < \mu$ , the ratio $R_s$ is greater than $1$ and increasing:

\textbf{Property 1.}
\begin{align}
R_{s-1} = \frac{P_{s-1}}{P_{s-2}} > R_s  = \frac{P_{s}}{P_{s-1}} \\
P^2_{s-1} > P_sP_{s-2} 
\end{align}

Create two collections by adding to D one $1$ and one $0$.  Call them $D_1$ and $D_0$ respectively. The probability of observing $s$ from $D_1$ the is given by:
\[ P(s | D_1) = pP_{s-1} + qP_s \]

Similarly for the second collection (with extra 0):

\[ P(s | D_0) = qP_{s-1} + pP_s \]

Now consider the probability ratio for the collections $D_1$ and $D_0$ collections at some $s$:
\begin{align}
R_s(D_1) = \frac{ pP_{s-1} + qP_s }{pP_{s-2} + qP_{s-1}} \\
R_s(D_0) = \frac{ qP_{s-1} + pP_s }{qP_{s-2} + pP_{s-1}}
\end{align}
 $N$ user bits are subjected to  


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}



\section{JUNK}


Given the independence 



 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ] =  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right ) \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right )  }{    \sum_{h' \in \N^d} \left ( Pr \left [  M(\bbx') = h' \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h'_i]  \right )  } \right )
\end{align*}



More formally, the value of $g_l$ is a sum of binomial distributions 
Where $r$ is the number of set bits (both clear and fake) in the dimension $l$.   This allows us to 



The value of $g_l$ distributed as a sum of two binomial variables.  
\[ g_l  \sim Bin(h_l,p) + Bin(n+m-h_l,q) \]
 
 
Applying \eqref{lem:rsbound} gives bounds of $R(S)$

\begin{equation} \label{eq:rsbounds}
 \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  } R(S|D)=  \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  }
\end{equation}

\end{document}

