%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt]{article}
%\documentclass[11pt,draft]{amsart}

% Custom styling.
\usepackage{custom}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}
%% Title matter
\title{Notes}
\author{Maxim Zhilyaev}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amssymb}

\newcommand{\bbx}{\pmb{x}}
\newcommand{\bbz}{\pmb{z}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\reals}{\mathbb{R}} % Real number symbol
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}} % Complex numbers
\newcommand{\integers}{\mathbb{Z}} % Integer symbol
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}} % Rational numbers
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\naturals}{\mathbb{N}} % Natural numbers
\newcommand{\N}{\mathbb{N}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Clear Reports}

Assume that the data comes from a universe $\cX = [d]$ of $d$ elements. Each individual $i \in [n]$ of $n$ users has a data element $x_i\in \cX$ . We will write a data entry in bold $\bbx_i \in \{0, 1\}^d $ to be the one-hot vector where $x_i$ is zero in every position except position $\bbx_i \in \cX$ , where it is one. Furthermore, we will denote a dataset $\bbx = \{x_1,\dots ,x_n\}$ to be a collection of all users' one-hot vectors. We will have each user donate his data $\bbx_i$. Further, we will inject some fake reports $z_j\in \cX$ for $j \in [m]$, and corresponding one-hot vector notation $\bbz_j$, where each data entry is chosen uniformly at random from $\cX$. We then pass $\{\bbx_i : i \in [n]\}$ and $\{\bbz_j : j \in [m]\} $ to an anonymizer that shuffles the data and makes it impossible to determine whether a data record is real or fake. We call this algorithm 
\[
M(\bbx_1,  \dots , \bbx_n) = \pi (\bbx_1, \dots , \bbx_n, \bbz_1, \dots , \bbz_m) \text{ where } \pi \text{ permutes its elements}. 
\]
We then compute the privacy loss of such an algorithm $M$. Equivalently, we could write the output as a histogram over the entire database, as in $M(\bbx_1,  \dots , \bbx_n) = \sum^n_{i=1} \bbx_i + \sum^m_{j=1} \bbz_j$. Note that rather than inject random noise to these counts, as in central differential privacy, we want to consider \emph{anonymized differential privacy}, where data records are transmitted through a mix net to break any identifiers with each data entry and the server sees the aggregated records in some random order. In this model, there is no trusted server that injects noise to ensure DP. Rather, the user needs to only trust the anonymizer to shuffle real and fake records.

We then consider the privacy loss for a general mechanism $M$. Consider an outcome $h \in \N^d$, which is a histogram over the full dataset domain and neighboring datasets $\bbx$ and $\bbx'$.

\begin{align}
L(h) = \log \left ( \frac{\Pr[M(\bbx) = h]}{\Pr[M(\bbx') = h]} \right )
\end{align}

If we can bound $L(h)$ by $\epsilon$ for any outcome $h$ then we say that $M$ is $\epsilon$-DP. If we can bound $L(h)$ by $\epsilon$ with probability at least $1 - \delta$ where the randomness is over $h \sim M(\bbx)$, then we say that $M$ is $(\epsilon, \delta)$-DP.

We now focus on $M$ being the mechanism described above, which injects $m$ fake reports. We can then write the privacy loss in the following way where we assume, without loss of generality, that $\bbx$ and $\bbx'$ only differ in the first record, i.e. $\bbx_i = \bbx'_i$ for all $i \ne 1$.

\begin{align*}
L(h) &= \log \left ( \frac{\Pr[x_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]}{\Pr[x'_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]} \right ) \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \bbx_1 - \sum^n_{i=2} \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h - \bbx'_1 - \sum^n_{i=2} x_i ]} \right )  \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \sum^n_{i=1}  \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h  - \sum^n_{i=1} \bbx_i  - (\bbx'_1 - \bbx_1) ]} \right )
\end{align*}

We denote  $\hat{h}$ to be the histogram of the fake records only $\hat{h} = h - \sum^n_{i=1} \bbx_i$, with respective counts in each histogram bin $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_d\}$.  Then the privacy loss ratio can be written as:

\begin{align*}
L(h) =  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h}  + \bbx_1 - \bbx'_1) ]} \right )
\end{align*}



The one-hot vectors $\bbx_1$  and $\bbx'_1$ may only differ in two positions, let these positions be $\ell$ and $\ell'$. 
$\bbx_1$ and $\bbx'_1$ must have opposite bit-values in positions $i$ and $i'$ (otherwise these vectors are identical). 
Without loss of generality assume $x_{1,\ell} = 1, x_{1,\ell'} = 0$ and $x'_{1,\ell} = 0, x'_{1,\ell'} = 1$.  
Adding $\bbx_1$ adds 1 to $h_i$, while subtracting $\bbx'_1$ removes 1 from $h_\ell'$.
Hence,  if $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d\} $, then  $\hat{h} + \bbx_1 -\bbx'_1 =  \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} -1, \dots, \hat{h}_d\} $.

Further, note that the count the fake bits $\hat{h}_\ell = \sum^m_{j=1} \bbz_{j,\ell}$ is a binomial distribution $h_\ell \sim \text{Bin}(m, 1/d)$, and the distribution of the fake bit counts across the bins takes the multinomial form $ \hat{h} \sim \text{Multinomial}(m,(1/d,\cdots, 1/d))$. We then aim to bound the following quantity.

\begin{align*}
 L(h) &= \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h} + \bbx_1 - \bbx'_1]} \right ) \\
 & = \log  \left ( \frac{ {m \choose \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d}}{ { m \choose hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} - 1, \dots, \hat{h}_d}} \right ) \\
 & = \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) 
\end{align*}

It must be stressed that for a given pair of $(\bbx_1, \bbx'_1)$, the corresponding position pair $(\ell,\ell')$ where their bits are different is fixed, and the privacy loss only surfaces while observing the counts in the corresponding histogram bins $(h_\ell, h_{\ell'})$.  It's entirely possible to see high ratio between counts in some other histogram bins, but it wouldn't contribute to the privacy loss for a concrete pair $(\bbx_1, \bbx'_1)$.  This observation allows us to focus only on a single pair of the histogram bins, ignoring the rest of the histogram as immaterial.  

By applying a Chernoff bound, we have a bound (symmetric for the upper and lower tail) for the sum of the fake bits in any bin $\hat{h}_k = \sum^m_{j=1} z_{j,k},  k \in [d]$

\begin{align*}
 \Pr \left [ \left | \hat{h}_k - \frac{m}{d} \right | > t \frac{m}{d} \right ]  \le 2 e^{- \frac{m}{d} \frac{t^2}{3}}, \qquad \text{ for  } 0 < t < 1.
\end{align*}

Choose $t$ to fit the expression below, hence $t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }$.
%\begin{align}
%2e^{- \frac{m}{d} \frac{t^2}{3}} = \frac{\delta}{2}  \\
%- \frac{m}{d} \frac{t^2}{3} = \log{\frac{\delta}{4}}  \\
% \frac{m}{d} \frac{t^2}{3} = \log{\frac{4}{\delta}} \\
% t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }
%\end{align}
Using this expression for $t$ turns our Chernoff bound into the following,
 
 \begin{align}
 Pr \left [ \left | \bar{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \frac{\delta}{2}
\end{align}

Given any pair of the histogram bins at positions $(\ell,\ell')$, the probability of observing large deviation from the mean in at least one bin obeys the unions bound.
\[
\Pr \left [ \max_{k \in (\ell,\ell')} \left | \hat{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \delta
\]

We then condition on the event that both counts $\hat{h}_l$ or $\hat{h}_{l'}$ fall in the interval $m/d \pm \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}$, which event occurs with probability at least $1 - \delta$.  
Conditioned on there being the given number of fake records, we can upper bound the privacy ratio $L(h)$

 \begin{align}
 L(h) =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \le \log  \left ( \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \le \epsilon 
\end{align}

From the above, we then get a condition on the number of fake records, m, to ensure DP.
 \begin{align*}
& \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^\epsilon  \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) - 1 \ge 0 \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} \left ( \sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1) \right ) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  \ge 0 \\
\implies &\sqrt{  \frac{m}{d}}  \ge  \frac { \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  } { e^\epsilon - 1 } \\
\implies & \frac{m}{d}  \ge 3\log {\frac{4}{\delta}} \left ( \frac{e^\epsilon +1}{e^\epsilon - 1} \right )^2
\end{align*}

As for the lower bound of $L(h)$, it's met if the upper bound is met.
 \begin{align*}
 L(h) & =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \ge \log  \left ( \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \ge - \epsilon  \\
\implies &  \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \ge e^{-\epsilon} \\
 \implies &  \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} \le e^{\epsilon}  \\
\implies &   \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} < \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^{\epsilon} 
\end{align*}


%%%%%%%%%%%%%%%%%%% - but flipping with fake records

\section{Fake records and bit flipping}

We now apply the exact same protocol, whereby users produce $n$ real and $m$ fake reports, but require each 1-hot vector to bit bit-flipped with frequency $q$.  A randomization procedure $\cR(y)$ flips each bit of an arbitrary 1-hot-vector $y$ with probability $q$ and keeps it the same with probability $p=1-q$.  The resulting mechanism $M_r$ becomes a permutation of randomized true and fake records:
\[
M_r(\bbx_1,  \dots , \bbx_n) = \pi (\cR(\bbx_1), \dots , \cR(\bbx_n), \cR(\bbz_1), \dots , \cR(\bbz_m)) \text{ where } \pi \text{ permutes its elements}. 
\]

Without los of generality assume $\bbx_1$ is replaced with $\bbx'_1$ to receive a neighboring data set $\bbx'$.  The outcome is a histogram $g \in \N^d$ containing sums of randomized bits in each dimension, and the privacy loss:

\begin{equation} \label{eq:lgbound}
L(g) = \log \left ( \frac{\Pr[M_r(\bbx) = g]}{\Pr[M_r(\bbx') = g]} \right )
\end{equation}

The combined set $\bbx + \bbz$ gives raise to a histogram $h \in \N^d$ received by applying the before discussed mechanism M (clear true records plus fake records).  Hence, the $Pr [ M_r(x) = g ]$ can be written as a sum of probabilities over the domain of $h$:

 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ]  \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   }{    \sum_{h' \in \N^d}  Pr \left [  M(\bbx') = h' \right ] \cdot   Pr[ g | h']   } \right )
\end{align*}

Noting that
\[
 Pr \left [  M(\bbx) = h \right ]  =  Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]
 \] 
 
And regrouping  the privacy loss ratio to have summands with same $Pr \left [  M(\bbx) = h \right ]$ in identical positions in numerator and denominator, and applying \eqref{lem:rsbound} we have:
 \begin{align*}
& \log \left ( \max_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \text{ , and } \\
& L(g) \ge  log \left ( \min_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right ) \\
\end{align*}

Probabilities $Pr \left [  M(\bbx) = h \right ]$ and $ Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]$ cancel each other out in each ratio, hence giving us the bounds of the privacy loss over domain of $h$ .

\begin{align*}
\log \left ( \max_{h \in \N^d} \left ( \frac{  Pr [ g | h ]   } { Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \ge  log \left ( \min_{h \in \N^d}  \left ( \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right )
\end{align*}

Since bits are flipped independently, the probability of finding certain number of bits in a particular histogram bin $g_l$ depends only on how many not-yet-randomized set bits there are in the dimension $l$, that is the value of $h_l$.  Such independence allows to re-write $Pr [ g | h]$ as a product of probabilities for each dimension.
 \[
Pr [ M_r(x) = g ] = Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] = \prod_{i=1}^d Pr[ g_i | h_i] 
\]

Without loss of generality assume that $\bbx_1$ and $\bbx'_1$ differ in the first and second positions, that is $\bbx_{1,1} = 1, \bbx_{1,2} = 0$ and $\bbx'_{1,1} = 0, \bbx'_{1,2} = 1$, then
\begin{align*}
 & h  - \bbx_1 + \bbx'_1 = \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} \\
 \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{ Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] } { Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} ]  } \\
  \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{  Pr[ g_1 | h_1] Pr[ g_2 | h_2] \prod_{i=3}^d Pr[ g_i | h_i]  } {  Pr[ g_1 | h_1 - 1] Pr[ g_2 | h_2 + 1] \prod_{i=3}^d Pr[ g_i | h_i]  }  \\
   \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } =   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  } 
\end{align*}

Plugging the above formula into \eqref{eq:lgbound}, the privacy loss bounds become:
\begin{align*}
& \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right ) \right ) \ge L(g) \ge  \min_{h \in \N^d}  \left ( \log \left (  \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }   \right ) \right ) \\
\implies &  \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \max_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right) \ge L(g) \text{ , and } \\
&  L(g) \ge \min_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \min_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right)
\end{align*}

Basically, the privacy loss is bound by the sum of privacy losses in each of the affected dimensions.  Which enables relatively simple path to the bound.





%%% Bounding ratio of sums by max ratio of terms

\section{Appendix: Ratios of sums: properties}

Here we establish some results around bounding and comparing ratios of sums, which will be useful in working with the privacy ratio.


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}


\section{Appendix: Bit flipping in single dimension}


There are $n+1$ single bit records being sent through a shuffler.  Before sending, each bit is randomized with $\cR$ - that is, flipped with probability $q$  and kept unchanged with probability $p=1-q$.  the outcome $S$ is the sum of randomized bits. Let $D$ be a set of $n$ bits and construct a neighboring pair of datasets by adding to $D$ a set bit and a 0 zero bit.  Then the privacy loss ratio at any given value of $S$ is expressed as:
\begin{equation} \label{eq:} 
R(S|D)= \frac{P(S|D \cup 0)}{P(S|D \cup 1)}
\end{equation}

Each probability allows conditioning on possible values the added bit could generate:
\begin{align}
P(S|D \cup 0) = p \cdot P(S | D ) + q \cdot P( S - 1 | D) 
\end{align}

Indeed, if $0$ bit is randomized to itself (with probability $p$), then $S$ must be generated by $D$ alone, while if $0$ bit was flipped (with probability $q$) then $D$ must generate $S-1$ total bit sum. Similarly 
\begin{align}
P(S|D \cup 1) = p  \cdot P(S -1 | D ) + q \cdot P( S  | D) 
\end{align}

Combining two conditioning expressions into the privacy loss ratio one arrives to:
\begin{equation} \label{eq:plratio}
R(S|D)=  \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  } = \frac{ p \frac{P(S | D )}{P(S - 1| D )} + q } { p + q \frac{P(S | D )}{P(S - 1| D )} }
\end{equation}

\subsection{properties of $R(S|D)$}

Let $\rho(S) = \frac{P(S | D )}{P(S - 1| D )}$ be a \textbf{probability ratio} between adjacent values of $S$.  It's related to $R(S)$ as in:
\[ 
R(S|D) =  \frac{ p \frac{P(S | D )}{P(S - 1| D )} + q } { p + q \frac{P(S | D )}{P(S - 1| D )} } = \frac{q + p\rho(S)}{p + q\rho(S)}
\]
Let $g(x) = \frac{q + px}{p + qx}$, the function $g$ is increasing over $x>0$, since
\[ g'(x) = \frac{p-q}{(p+qx)^2} > 0. \]
Therefore, properties of monotonicity and extrema established for $\rho(S)$  carry over to $R(S)$ as well.
 
 If  $D$ contains $m$ set bits, then the distribution of $S$ is a sum of two binomial distributions (a Poisson Binomial distribution): 
 \[ S  \sim Bin(m,p) + Bin(n-m,q) \]
 
 \begin{lem} \label{lem:rs1}
When 0-bit is replaced with a set bit, the resulting privacy loss ratio $R(S)$ decreases monotonically as $S$ grows, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}
\begin{pf}
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}
\end{pf}

 \begin{lem} \label{lem:rs2}
 If randomization procedure $\cR$ is $(\epsilon, \delta)$-private on a collection $n$ bits, it's also $(\epsilon, \delta)$-private on a collection of $n+1$ bits.  In other words, if $n$ bit are protected with flipping frequency $q$, then all collections of greater size are also protected with same $q$. 
\end{lem}
\begin{pf}
 Let $D$ be a collection of $n$ bits and derive a neighboring collection $D'$ by replacing 0-bit with a set bit.  A privacy loss ratio $R_n(S)$ for outcome $S$ is given below.
  \begin{align*}
R_n(S) = \frac{ P(S|D) }{ P(S|D') }
\end{align*}

Suppose a new 1-bit is added to both collections, then $R_{n+1}(S)$ is derived by conditioning
 \begin{align*}
R_{n+1}(S) = \frac{ P(S|D \cup 1) }{ P(S|D' \cup 1) } = \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p}
\end{align*}
 
By lemma \eqref{lem:rsbound} and lemma  \eqref{lem:rs1} we have
\begin{align}
 & \frac{P(S-1|D)}{P(S-1|D'} \ge  \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p} \ge \frac{P(S|D)}{P(SD'} \\
\implies & R_n(S-1) \ge R_{n+1}(S) \ge R_n(S) \\
\implies & R_n(S) \ge R_{n+1}(S+1) \ge R_n(S+1) \label{lem:rsb2}
\end{align}
 
Suppose $R_n$ is $(\epsilon, \delta)$-private. Since $R_n$ is monotonically decreasing with $S$ (lemma  \eqref{lem:rs1} ), there exist two values $\alpha + \beta \le \delta$, such that $R_n$ is upper bounded on the left at a particular limiting value $S_\alpha$ 
 \begin{align}  \label{lem:rsb3}
 R_n(S_\alpha) \le e^\epsilon \text{ and } P_n(S \le S_\alpha) \le \alpha 
\end{align}
And it's low bounded on the right at a particular limiting value $S_\beta$
 \begin{align}  \label{lem:rsb4}
 R_n(S_\beta) \ge \frac{1}{e^\epsilon} \text{ and } P_n(S \ge S_\beta) \le \beta
\end{align}
 
Consider the left (upper) bound first, and recall that according to \eqref{lem:rsb2}
\[
 R_n(S_\alpha) \ge R_{n+1}(S_\alpha+1) \ge R_n(S_\alpha+1)
 \]
 
$R_{n+1}(S_\alpha+1)$ is bounded because  $R_n(S_\alpha)$ is bounded per \eqref{lem:rsb3}.  Hence, $R_{n+1}$ could only be over the bound at $S_\alpha$, however the cumulative sum of probabilities up to $S_\alpha$ is always less for $n+1$ bits than for $n$ bits.
 \begin{align}  \label{lem:rsb5}
P_{n+1}(S \le S_\alpha) \le P_{n}(S \le S_\alpha)
\end{align}
We shall prove \eqref{lem:rsb5} in a moment. The important fact is that $R_n$ upper bounds $R_{n+1}$ at the left tail of distribution of $S$. 

Similarly, 


 
 
 
 
 
 
 
 
 
 
 
 
 
 \newpage
 
 
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}
\end{pf}



\begin{lem} \label{lem:rsreduce}
 The privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}

 the privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.







Note that $\frac{P(S | D )}{P(S - 1| D )}$ as a \textbf{probability ratio} between adjacent values of $S$.  It's easy to see that privacy loss ratio maximizes when \textbf{probability ratio} maximizes. 

Recall that $p>q$ and consider two positive values $A$ and $B$ 
\begin{align*}
\frac{ p \cdot A + q } { p + q \cdot A }  \ge \frac{ p \cdot B + q } { p + q \cdot B } \\
p^2A + q^2B \ge p^2 B + q^2 A  \\
A (p^2 - q^2) \ge B (p^2 - q^2)  \\
A \ge B
\end{align*}

The above confirms that the distributions with largest \textbf{probability ratio} also exhibit larger privacy loss ratio. Hence we can focus on studying \textbf{probability ratio}  instead of privacy loss ratio and choose those $D$ that demonstrate sharpest decrease of probabilities in the left tail.




Consider a collection $D$ of $N$ bits, subjected to randomization procedure $\cR$, whereby a bit is flipped with probability $q$ and kept unchanged with probability $p=1-q$.  The outcome of $\cR$ is a $a$ - sum of bits after randomization. The neigboring set $D_m$ is recieved form Assuming that $D$ contains $m$ set bits, we consider a privacy loss ratio $R_s$ computed for the outcome $s$:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 


consisting of $m$ ones and $N-m$ zeros.  Denote probability of number of successes for that collection as $P(S|D)$.
The probability ratio at $s$  is given by:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 

Denote expectation of $s$ as $\mu$:
\[ \mu = mp + (N-m)q \]

For simplicity, denote probabilities at $s$ for $D$ as:
\[ P(s|D) = P_{s} \]

It's known that for all $s < \mu$ , the ratio $R_s$ is greater than $1$ and increasing:

\textbf{Property 1.}
\begin{align}
R_{s-1} = \frac{P_{s-1}}{P_{s-2}} > R_s  = \frac{P_{s}}{P_{s-1}} \\
P^2_{s-1} > P_sP_{s-2} 
\end{align}

Create two collections by adding to D one $1$ and one $0$.  Call them $D_1$ and $D_0$ respectively. The probability of observing $s$ from $D_1$ the is given by:
\[ P(s | D_1) = pP_{s-1} + qP_s \]

Similarly for the second collection (with extra 0):

\[ P(s | D_0) = qP_{s-1} + pP_s \]

Now consider the probability ratio for the collections $D_1$ and $D_0$ collections at some $s$:
\begin{align}
R_s(D_1) = \frac{ pP_{s-1} + qP_s }{pP_{s-2} + qP_{s-1}} \\
R_s(D_0) = \frac{ qP_{s-1} + pP_s }{qP_{s-2} + pP_{s-1}}
\end{align}
 $N$ user bits are subjected to  


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}



\section{JUNK}


Given the independence 



 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ] =  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right ) \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right )  }{    \sum_{h' \in \N^d} \left ( Pr \left [  M(\bbx') = h' \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h'_i]  \right )  } \right )
\end{align*}



More formally, the value of $g_l$ is a sum of binomial distributions 
Where $r$ is the number of set bits (both clear and fake) in the dimension $l$.   This allows us to 



The value of $g_l$ distributed as a sum of two binomial variables.  
\[ g_l  \sim Bin(h_l,p) + Bin(n+m-h_l,q) \]
 
 
Applying \eqref{lem:rsbound} gives bounds of $R(S)$

\begin{equation} \label{eq:rsbounds}
 \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  } R(S|D)=  \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  }
\end{equation}

\end{document}

