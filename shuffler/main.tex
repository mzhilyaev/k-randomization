%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt]{article}
%\documentclass[11pt,draft]{amsart}

% Custom styling.
\usepackage{custom}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}
%% Title matter
\title{Notes}
\author{Maxim Zhilyaev}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amssymb}

\newcommand{\bbx}{\pmb{x}}
\newcommand{\bbz}{\pmb{z}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\reals}{\mathbb{R}} % Real number symbol
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}} % Complex numbers
\newcommand{\integers}{\mathbb{Z}} % Integer symbol
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}} % Rational numbers
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\naturals}{\mathbb{N}} % Natural numbers
\newcommand{\N}{\mathbb{N}}

\newcommand{\Dsp}{\mathcal{D}}
\newcommand{\Ssp}{\mathcal{S}}
\newcommand{\Bsp}{\mathcal{B}}
\newcommand{\Tsp}{\mathcal{T}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\sv}{\mathbf{s}}
\newcommand{\mv}{\mathbf{m}}
\newcommand{\ev}{\mathbf{e}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\pv}{\mathbf{p}}
\newcommand{\Pm}{\mathbf{P}}
\newcommand{\xvt}{\tilde{\xv}}
\newcommand{\yvt}{\tilde{\yv}}
\newcommand{\sm}{\sv^-}
\newcommand{\iv}{\mathbf{i}}
\newcommand{\one}{\boldsymbol{1}}
\newcommand{\zero}{\boldsymbol{0}}
\newcommand{\mb}{\overline{m}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Abstract}
We study a variety of the shuffling protocols for reporting one-hot vectors from multiple users with respect to privacy, sensitivity and practicality.  From a practical standpoint, the cost of shuffling is not zero. Too many shuffled records may render a particular protocol impractical, even though its other metrics show good performance. We specifically consider protocols that minimize the number (but not necessarily the size) messages between a user device and the shuffler.

Assuming that the data comes from a universe $\cX = [d]$ of $d$ elements. Each individual $i \in [n]$ of $n$ users has a data element $x_i\in \cX$ . We will write a data entry in bold $\bbx_i \in \{0, 1\}^d $ to be the one-hot vector where $x_i$ is zero in every position except position $\bbx_i \in \cX$ , where it is one. Furthermore, we will denote a dataset $\bbx = \{x_1,\dots ,x_n\}$ to be a collection of all users' one-hot vectors. We consider multiple mix-net protocols for reporting 1-hot vectors.  A simple one would require each user to donate his data $\bbx_i$ in clear, but, in addition, inject some fake reports $z_j\in \cX$ for $j \in [m]$, and corresponding one-hot vector notation $\bbz_j$, where each data entry is chosen uniformly at random from $\cX$. We then pass $\{\bbx_i : i \in [n]\}$ and $\{\bbz_j : j \in [m]\} $ to an anonymizer that shuffles the data and makes it impossible to determine whether a data record is real or fake.  We call it the "clear-fake records" protocol and show that it provides adequate protection with the cost proportional to $[d]$. Hence if dimensions are not large then the "clear-fake records" protocol is preferred for its simplicity. 

When $[d]$ is significant, the cost of sending and shuffling many fake records becomes prohibitive. Another protocol is developed,  which parameters are independent of $[d]$.  It's called a "fake and flip" protocol, whereby  a user still generates true and fake one-hot report vectors that are both randomized by bit flipping before being sent to the shuffler.  This enables adequate protection at reasonable cost. Depending on the data collection setting, various flavors of the "fake and flip" protocol are discussed.

In discussing mathematical properties of the protocols involving randomization we will rely upon results received for a single dimension bit reporting.  Which results we provide in the first sections, along with some theoretical result claiming that if a randomization algorithm  $\cR$ is $(\epsilon, \delta)$-private on a dataset of $n$ elements, it's also  $(\epsilon, \delta)$-private on a dataset of $n+1$ elements, that is adding more elements to the shuffled set does not reduce privacy. These results are, then, used to develop bounds for each protocol.

\section{Differential Privacy Setup}
\label{sec:dp}

A record is an element of some space $\Dsp$, and a database $\xv$ is a vector of $n$ records: $\xv = (x_1,\dots,x_n) \in \Dsp^n$.
A randomized algorithm $\cR$ maps the database into another space: $\map{\cR}{\Dsp^n}{\Ssp}$. 
The result of applying an algorithm to a database is termed an \textbf{transcript}.
The notion of differential privacy for an algorithm $\cR$ is that the resulting transcripts does not change substantially when a record in the database is modified,
\ie transcripts are not sensitive to particular individual records in the database.
Hence, releasing transcript of $\cR$ publicly will not jeopardize privacy, since information regarding individual records cannot be gained by analyzing the outcome of $\cR(\xv)$.

%%% Differential privacy definition

Differential privacy for a randomized algorithm $\cR$ is formulated by comparing the transcripts generated by applying $\cR$ to two very similar databases $\xv,\xv' \in \Dsp^n$.
We say the databases \textbf{differ in one row} if 
$\sum_{i=1}^n I(x_i \neq x'_i) = 1$.  
Such datasets are commonly called  \textbf{neighboring} database or  \textbf{neighbors}.
\begin{defn}
A randomized algorithm $\cR$ is $(\epsilon,\delta)$-\textbf{differentially private} if, 
for any two databases $\xv,\xv' \in \Dsp^n$ differing in one row,
\begin{equation}\label{eq:dpdef}
\P[\cR(\xv) \in S] \leq \exp(\epsilon) \cdot \P[\cR(\xv') \in S] + \delta
\end{equation}
for all $S \cont \Ssp$ (measurable).
\end{defn}
In other words, the outcomes from the two databases databases differing in one row are close in distribution, may be with the exception of very unlikely outcomes whose probability is less than $\delta$

\begin{defn}
A randomized algorithm $\cR$ generates point-wise  $(\epsilon,\delta)$-\textbf{indistinguishable} outcomes for two databases $\xv,\xv' \in \Dsp^n$ when
\begin{equation} \label{eq:dpcnt}
\P \left ( \exp(-\epsilon) \le \frac{\P[\cR(\xv) = s]}{\P[\cR(\xv') = s]} \le  \exp(\epsilon) \right ) \ge 1 - \delta
\end{equation}
\end{defn}

\begin{prop} \label{prop:dpcnt}
A randomized algorithm $\cR$ is $(\epsilon,\delta)$-\textbf{differentially private} if for every pair of neighboring databases $\xv,\xv' \in \Dsp^n$, $\cR$ generates point-wise  $(\epsilon,\delta)$-\textbf{indistinguishable} outcomes.   Per reference [2]

To restate.
\begin{align} \label{eq:dpcnt}
 & \P \left ( \frac{\P[\cR(\xv) = s]}{\P[\cR(\xv') = s]} \le  \exp(\epsilon) \right ) \ge 1 - \delta \text{, for any two neighbors } \xv,\xv' \\
\implies & \cR \text{ is } (\epsilon,\delta)-\textbf{differentially private}
\end{align}

\end{prop}

\subsection{Single record shuffling protocol}

There are $n$ users, each holding a user value $x_i \in \cX$.  User values form a database of user records a dataset $\bbx = \{x_1,\dots ,x_n\}$. Each user applies a randomization procedure $\cR(x): \map{\cR}{\cX}{\Ssp}$, then submits $\cR(x_i)$ to an anonymizer that shuffles the data and makes it impossible to determine whether a data record is real or fake. We call this algorithm 
\[
M(\bbx_1,  \dots , \bbx_n) = \pi (\cR(x_1), \dots , \cR(x_n)) \text{ where } \pi \text{ permutes its elements}. 
\]

If $\Ssp$ finite,  we can write the output of $M$ as a histogram over the entire database, as in $M(\bbx_1,  \dots , \bbx_n) = \sum^n_{i=1} \bbx_i + \sum^m_{j=1} \bbz_j$. Note that rather than inject random noise to these counts, as in central differential privacy, we want to consider \emph{anonymized differential privacy}, where data records are transmitted through a mix net to break any identifiers with each data entry and the server sees the aggregated records in some random order. In this model, there is no trusted server that injects noise to ensure DP. Rather, the user needs to only trust the anonymizer to shuffle real and fake records.

We then consider the privacy loss for a general mechanism $M$. Consider an outcome $h \in \N^d$, which is a histogram over the full dataset domain and neighboring datasets $\bbx$ and $\bbx'$.


Suppose $\Ssp$ is finite.  
If  which is common in cases where the transcript involves integer counts, then the distribution of the transcript $A(\xv)$ can be represented using its pmf $\P[A(\xv) = s]$ for $s \in \Ssp$




\section{Reporting randomized bits in the mix-net model}

It is instructive to first consider the case where each record in the collection consists of a single bit, as the expressions simplify considerably.

When $L=1$, each original and synthetic record is either 1 or 0, and the transformation $R$ flips each record with probability $q$.
Partition the collection space $\Dsp^n$ according to the number of records that are 1:
\[ \Dsp^n = \bigcup_{m = 0}^n \Dsp_m^n
\quad\quad\text{where}\quad\quad
\Dsp_m^n := \bigg\{ \xv\in\Dsp^n \,:\, \sum_{i=1}^n I(x_i = 1) = m \bigg\}.
\]
For $\xv\in\Dsp_m^n$, we have
\[ A(\xv) = \Phi\circ R(\xv) = \big(A_n(m), n - A_n(m)\big), \]
where
\begin{align*}
A_n(m) &:= \sum_{i=1}^n I(R(x_i) = 1)
= \sum_{i:\, x_i = 1} I(R(1) = 1) + \sum_{i:\, x_i = 0} I(R(0) = 1) \\
&\sim Bin(m, p) + Bin(n-m, q),
\end{align*}
a sum of two independent Binomial random variables with support $\{0,\dots,n\}$.
Furthermore, if $\xv\in\Dsp_m^n$ and $\xv,\xv'$ differ in one row, then $\xv'\in\Dsp_{m-1}^n \cup \Dsp_{m+1}^n$.
Defining
\[ \pi_n(s; m) := \frac{\P[A_n(m) = s]}{\P[A_n(m+1) = s]}
\quad\quad\text{for}\ \ 
s\in\{0,\dots,n\}\ \text{and}\ m\in\{0,\dots,n-1\},
\]
the privacy ratio becomes
\[ \pi\big((s, n-s);\,\xv,\xv'\big) =
\begin{cases}
\pi_n(s;m - 1) & x_1 = 1 \\[0.3em]
\inv{\pi_n(s;m)} & x_1 = 0
\end{cases}\, .
\]
Hence, in the $L=1$ case, it suffices to study the behaviour of $\pi_n(s;m)$.


\subsection{Recursive relationship over $n$ and $m$}

The conditioning argument \eqref{eq:prcond} yields a recursive relationship that lets us express the distribution of $A_n$ in terms of that of $A_{n-1}$.

Recall that $A_n(m)$ is the outcome of applying the bit transformation $R$ to $n$ original bits, $m$ of which are 1 and $n-m$ are 0.
For $m \geq 1$, we can condition on the outcome of one of the original 1s:
\[ A_n(m) \sim Ber(p) + Bin(m-1, p) + Bin(n-m, q) \sim Ber(p) + A_{n-1}(m-1), \]
and so
\begin{equation}\label{eq:rec1}
\P[A_n(m) = s] = p\P[A_{n-1}(m-1) = s-1] + q\P[A_{n-1}(m-1) = s].
\end{equation}
If $s = 0$, the first term on the RHS is interpreted as 0, and if $s = n$, the last term is.
Similarly, for $m \leq n-1$, conditioning on an original 0,
\[ A_n(m) \sim Ber(q) + Bin(m, p) + Bin(n-m-1, q) \sim Ber(q) + A_{n-1}(m), \]
from which
\begin{equation}\label{eq:rec0}
\P[A_n(m) = s] = q\P[A_{n-1}(m) = s-1] + p\P[A_{n-1}(m) = s].
\end{equation}

The recursive formulas \eqref{eq:rec1} and \eqref{eq:rec0} give some insight into how the distribution of $A_n(m)$ changes as $n$ and $m$ vary:
\begin{itemize}
\item  as $n$ increases by 1, the probabilities shift slightly, with $\P[A_n(m) = 0] \leq \P[A_{n-1}(m) = 0]$ and
$\P[A_n(m) = s]$ falling between $\P[A_{n-1}(m) = s-1]$ and $\P[A_{n-1}(m) = s]$ for each $s\geq 1$ (\ie the hump of the pmf shifts to the right);
\item the distribution of $A_n(m+1)$ is not so different to that of $A_n(m)$, since $\P[A_n(m) = s]$ and $\P[A_n(m+1) = s]$ both lie between consecutive pmf values of $A_{n-1}(m)$. In particular, this allows us to express the privacy ratio $\pi(s;m)$ in terms of $A_{n-1}(m)$.
\end{itemize}

Writing $P_{n,m}(s) := \P[A_n(m) = s]$, the formulas \eqref{eq:rec1} and \eqref{eq:rec0} can be expressed as
\[ P_{n,m}(s) = pP_{n-1,m-1}(s-1) + qP_{n-1,m-1}(s)
%\quad\text{for}\ \ s = 0,\dots,n,\ \ m = 1,\dots,n
\quad\text{for}\ \ 0 \leq s \leq n,\ \ 1\leq m \leq n 
\]
and
\[ P_{n,m}(s) = qP_{n-1,m}(s-1) + pP_{n-1,m}(s)
%\quad\text{for}\ \ s = 0,\dots,n,\ \ m = 0,\dots,n-1. 
\quad\text{for}\ \ 0 \leq s \leq n,\ \ 0\leq m \leq n-1.
\]

\subsection{The probability ratio}

The probabilities in the privacy ratio represent the likelihood of observing
the same synthetic collection outcome given two different original collections.
In the expression $\pi_n(s;m) = P_{n,m}(s) / P_{n,m+1}(s)$, the probabilities
correspond to the distributions of $A_n(m)$ and $A_n(m+1)$, respectively.
However, using the decomposition \eqref{eq:rec1} and \eqref{eq:rec0}, we can
rewrite $\pi_n$ in terms of probabilites from the same distribution, which is
more convenient to work with.

Applying \eqref{eq:rec0} to the numerator and \eqref{eq:rec1} to the denominator,
we obtain
\[ \pi_n(s;m) =
\frac{qP_{n-1,m}(s-1) + pP_{n-1,m}(s)}{pP_{n-1,m}(s-1) + qP_{n-1,m}(s)} =
\frac{q + p \frac{P_{n-1,m}(s)}{P_{n-1,m}(s-1)}}
    {p + q \frac{P_{n-1,m}(s)}{P_{n-1,m}(s-1)}}
\]
for $s \geq 1$, and $\pi_n(0;m) \equiv p/q$.
Define the \textbf{probability ratio}
\[ \rho_n(s;m) := \frac{P_{n,m}(s)}{P_{n,m}(s-1)}
\qquad\text{for}\ \ 1 \leq s\leq n \]
a ratio of consecutive probabilities from the distribution of $A_n(m)$, and let
$g(x) = \frac{q + px}{p + qx}$, so that $\pi_n = g \circ \rho_{n-1}$.
The function $g$ is increasing over $x>0$, since
\[ g'(x) = \frac{p-q}{(p+qx)^2} > 0. \]
Therefore, properties of monotonicity and extrema established for $\rho_n$ (for
all $n$) carry over to $\pi_n$ as well.

The probability ratio can be expressed in a concise way using the following 
recursive property of the distribution of $A_n(m)$.

\begin{lem}
For $n \geq 1$,
\begin{equation} \label{eq:recP}
(s+1) P_{n,m}(s+1) = \bigg\{ (m-s)\frac{p}{q} + (n-m-s)\frac{q}{p} \bigg\} P_{n,m}(s) + (n-s+1) P_{n,m}(s-1)
\end{equation}
for $0 \leq m \leq n$ and $0 \leq s \leq n-1$ (with $P_{n,m}(-1) := 0$).
\end{lem}
\begin{pf}
We proceed by induction on $n$.
Suppose first $n=1$, $s = 0$.
If $m=1$, then $A_1(1) \sim Ber(p)$, and \eqref{eq:recP} holds since 
$(mp/q + (1-m)q/p) \cdot P_{1,1}(0) = p = P_{1,1}(1)$.
The argument is similar when $m=0$.
Next assume \eqref{eq:recP} holds for $A_{n-1}(m)$, and suppose $m \leq n-1$ and $1 \leq s \leq n-2$.
Observe that
\begin{align*}
\bigg\{ &(m-s)\frac{p}{q} + (n-m-s)\frac{q}{p} \bigg\} P_{n,m}(s) + (n-s+1) P_{n,m}(s-1) \\
&= \bigg\{ (m-s)\frac{p}{q} + (n-1-m-s)\frac{q}{p} \bigg\} \big[qP_{n-1,m}(s-1) + pP_{n-1,m}(s)\big] \\
 &\qquad\qquad + (n-1-s+1) \big[qP_{n-1,m}(s-2) + pP_{n-1,m}(s-1)\big] +
 \frac{q}{p}P_{n,m}(s) + P_{n,m}(s-1) \\
 &= p\bigg[ \bigg\{ (m-s)\frac{p}{q} + (n-1-m-s)\frac{q}{p} \bigg\} P_{n-1,m}(s) + (n-1-s+1)P_{n-1,m}(s-1) \bigg] \\
 &\qquad + q\bigg[ \bigg\{ (m-(s-1))\frac{p}{q} + (n-1-m-(s-1))\frac{q}{p} \bigg\} P_{n-1,m}(s-1) \\
 &\hspace{20em} + (n-1-(s-1)+1)P_{n-1,m}(s-2) \bigg] \\
 &\qquad - \bigg(p + \frac{q^2}{p}\bigg) P_{n-1,m}(s-1) - qP_{n-1,m}(s-2) + \frac{q^2}{p}P_{n-1,m}(s-1) + qP_{n-1,m}(s) \\
 &\qquad + qP_{n-1,m}(s-2) + pP_{n-1,m}(s-1) \\
 &= p(s+1)P_{n-1,m}(s+1) + qsP_{n-1,m}(s) + qP_{n-1,m}(s) \\
 & = (s+1) \big[ qP_{n-1,m}(s) + pP_{n-1,m}(s+1) \big] = (s+1)P_{n,m}(s+1),
\end{align*}
applying the induction hypothesis for $s$ and for $s-1$ together with \eqref{eq:rec0}. If $s=0$, the argument is similar:
\begin{align*}
\bigg\{ m\frac{p}{q} + (n-m)\frac{q}{p} \bigg\} P_{n,m}(0)
&= p\bigg\{ m\frac{p}{q} + (n-1-m)\frac{q}{p} \bigg\} P_{n-1,m}(0) + qP_{n-1,m}(0) \\
 &= pP_{n-1,m}(1) + qP_{n-1,m}(0) = P_{n,m}(1).
\end{align*}
\end{pf}

Given $m$, the probability ratio can be expressed using \eqref{eq:recP}:
\begin{align*}
\rho(s+1;m) &= \frac{m-s}{s+1}\frac{p}{q} + \frac{n-m-s}{s+1}\frac{q}{p} + \frac{n-s+1}{s+1} \frac{1}{\rho(s;m)} \\
\rho(1;m) &= m\frac{p}{q} + (n-m)\frac{q}{p}
\end{align*}
Write
\[ \eta(s) := \frac{n-s+1}{s+1} \qquad\text{and}\qquad
\gamma_m(s) := \frac{1}{s+1} \left[(m-s)\frac{p}{q} + (n-m-s)\frac{q}{p}\right], \]
to get
\begin{equation}\label{eq:probrrec}
\rho(s+1;m) = \eta(s)\inv{\rho(s;m)} + \gamma_m(s)\,; \quad
\rho(1;m) = \gamma_m(0). 
\end{equation}

Note also that $\gamma_m(s)$ can be expressed in terms of
$\EP A_n(m) = \mu_m = nq + m(p-q)$:
\[ (s+1)\gamma_m(s) = \frac{\mu_m - s}{pq} - n + 2s. \]
%\begin{align*}
% &= \frac{(m-s)p^2 + (n-m-s)q^2}{pq}
% = \frac{m(p^2-q^2) + nq^2 - s(p^2 + q^2)}{pq} \\
% &= \frac{nq + m(p-q) - nq + nq^2 - s(1-2pq)}{pq}
% = \frac{\mu_m - s -(n - 2s)pq}{pq} \\
% &= 
%\end{align*} 

The probability ratio has the following properties (TODO):
\begin{itemize}
\item decreasing in $s$ for fixed $m$
\item increasing in $m$ for fixed $s$.
\end{itemize}

\subsection{Bounding the probability ratio}

For $A$ to satisfy local differential privacy, the privacy ratio $\pi_n(s;m)$
must be bounded for all $s$ except for a set of small probability with respect
to the distribution $\P[A_n(m) = \cdot\,]$.
Furthermore, this bound must hold regardless of the original collection
described through $m$.

Fix $\delta > 0$.
Given $m$, we show that the probability ratio for $s\in[\mu_m-\delta, n]$ 
is bounded by a value $\rho(s^*; 0)$, where $s^*$ is expressed in terms of
$\mu_0-\delta$.
Together with the fact that
$P_{n,m}(\mu_m - \delta) \leq P_{n,0}(\mu_0 - \delta)$ (TODO - is this
necessary?),
this implies that the bound for local differential privacy, required to hold
for all $m$, can be computed in terms of $A_n(0)$ alone.
Note that, since $\rho$ is decreasing in $s$ for fixed $m$, it is sufficient
to consider the probability ratio at the smallest integer value belonging to
the interval $[\mu_m-\delta, n]$.

TODO: how to handle the left endpoint. What is the min value of $\delta$?

For $\delta > 0$ let $s_m(\delta) := \lceil \mu_m - \delta \rceil \vee 0$, and define $R_m(\delta) := \rho(s_m(\delta); m)$.
Note that $R_m(\delta) \leq R_m(\delta')$ for $\delta \leq \delta'$, and
$s_m(\delta + 1) = (s_m(\delta) - 1) \vee 0$.

\begin{prop}
\[ R_m(\delta) \leq R_0(\delta + 2)\qquad\text{for}\ \ m = 0,\dots,n \]
provided $\delta > \sigma_0 + 1$, where
$\sigma_0^2 = \var A_n(0) = npq$.
\end{prop}

\begin{pf}
Fix $\delta > \sigma_0 + 1$.
(TODO)
If $s_0(\delta) < 2$

Assume $s_0(\delta) \geq 2$, and suppose $R_m(\delta) > R_0(\delta + 2)$ for
some $m$.
Then, we have
\[ R_0(\delta) \leq R_0(\delta + 1) \leq R_0(\delta + 2) < R_m(\delta) \leq
R_m(\delta + 1),\]
implying that
\[ \inv{R_0(\delta + 2)} =
\frac{R_0(\delta+1) - \gamma_0(s_0(\delta+2))}{\eta(s_0(\delta+2))} > 
\frac{R_m(\delta) - \gamma_m(s_m(\delta+1))}{\eta(s_m(\delta+1))} =
\inv{R_m(\delta + 1)} \]
via \eqref{eq:probrrec}.
Write $s_m := s_m(\delta+1)$, $s_0 := s_0(\delta + 2)$.
Since $R_m(\delta) > R_0(\delta+1)$ by assumption, we obtain:
\begin{equation}\label{eq:deltabdineq}
\big\{\eta(s_m) - \eta(s_0)\big\} R_0(\delta + 1)
 + \big\{\eta(s_0)\gamma_m(s_m) - \eta(s_m)\gamma_0(s_0)\big\} > 0.
\end{equation}
Furthermore,
\[ \eta(s_m) - \eta(s_0)
= \frac{n - s_m + 1}{s_m + 1} - \frac{n - s_0 + 1}{s_0 + 1}
= -\frac{(n + 2)(s_m - s_0)}{(s_0 + 1)(s_m + 1)}, \]
and
\begin{align*}
\eta(s_0)&\gamma_m(s_m) - \eta(s_m)\gamma_0(s_0) \\
&= \frac{n - s_0 + 1}{s_0 + 1}\cdot \frac{(\mu_m - s_m)/pq - n + 2s_m}{s_m + 1} - 
\frac{n - s_m + 1}{s_m + 1}\cdot \frac{(\mu_0-s_0)/ pq - n + 2s_0}{s_0 + 1} \\
 &= \frac{(n+2)(s_m-s_0) + (\mu_0 s_m - \mu_m s_0)/pq +
 (n+1)[\mu_m - \mu_0 - (s_m - s_0)]/pq}{(s_0 + 1)(s_m + 1)},
\end{align*}
so \eqref{eq:deltabdineq} implies
\begin{align}
-(n+2)(s_m - s_0)&(R_0(\delta + 1) - 1) + \nonumber\\
&\frac{\mu_0 (s_m - s_0) - m(p-q) s_0}{pq} +
\frac{(n+1)[m(p-q) - (s_m - s_0)]}{pq} > 0. \label{eq:deltabdineq2}
\end{align}
Now, let
$\delta_0 :=
\delta - \{\lceil \mu_0 - \delta \rceil - (\mu_0 - \delta)\} =
\mu_0 - \lceil \mu_0 - \delta \rceil$, \ie
$\delta_0 = \inf\{ \lambda : s_0(\lambda) = s_0(\delta)\}$.
Then $s_0(\delta) = s_0(\delta_0) = \mu_0 - \delta_0$, an integer, and
$s_m(\delta_0) - s_m(\delta) \in\{0, 1\}$, since
$0 \leq \delta-\delta_0 < 1$.
Consequently,
since
\[ s_m(\delta_0) - s_0(\delta_0) = \lceil \mu_0  + m(p-q) - \delta_0 \rceil -
(\mu_0 - \delta_0) = \lceil m(p-q) \rceil, \]
\begin{align*}
s_m - s_0 &= (s_m(\delta) - 1) - (s_0(\delta) - 2) =
s_m(\delta) - s_m(\delta_0) + \lceil m(p-q) \rceil + 1 \\
 &\in \big\{\lceil m(p-q) \rceil, \lceil m(p-q) \rceil + 1 \big\},
\end{align*}
and
\begin{align*}
\mu_0(s_m - s_0) - m(p-q) s_0 & = \mu_0(s_m-s_0) - m(p-q)(s_0(\delta_0) - 2) \\
 &= \mu_0[(s_m-s_0) - m(p-q)] + m(p-q)(\delta_0 + 2).
\end{align*}
Applying these identities in \eqref{eq:deltabdineq2} gives
\[ 
-(n+2)(s_m - s_0)(R_0(\delta + 1) - 1) +
\frac{m(p-q)(\delta_0 + 2)}{pq}
+ \frac{n+1-\mu_0}{pq}\big(m(p-q) - (s_m-s_0)\big) > 0.
\]
Since $s_m-s_0 \geq m(p-q)$,
\begin{equation}\label{eq:deltabdineq3}
(n+2)(R_0(\delta+1) - 1) <
\frac{\delta_0 + 2}{pq}\frac{m(p-q)}{s_m - s_0} < \frac{\delta_0 + 2}{pq}.
\end{equation}
Next, recall that
$R_0(\delta+1) = P_{n,0}(s_0(\delta+1)) / P_{n,0}(s_0(\delta+1) - 1)$.
Since $P_{n,0}(\cdot) = \P[Bin(n,q) = \cdot\,]$, 
\[ R_0(\delta+1) - 1 =
\frac{n - s_0(\delta+1) + 1}{s_0(\delta+1)}\cdot \frac{q}{p} - 1 =
\frac{\mu_0 - (\mu_0 - \delta_0 - 1) + q}{p(\mu_0 - \delta_0 - 1)} =
\frac{\delta_0 + q + 1}{p(\mu_0 - \delta_0 - 1)}.
\]
Hence, substituting this expression in \eqref{eq:deltabdineq3} yields
\begin{align*}
(\delta_0 + 2)(\mu_0 - \delta_0 - 1) &> (n+2)q(\delta_0 + q + 1) >
\mu_0(\delta_0 + q + 1) \\
\iff
-\delta_0^2 - 3\delta_0 + 2\mu_0 - 2 &>  (1+q)\mu_0 \\
\iff
-\delta_0^2 - 3\delta_0 + npq &> 0,
\end{align*}
which requires that $\delta_0$ lie between the roots of the quadratic equation.
In particular,
\[ \delta_0 \leq -\frac{3}{2} + \frac{1}{2}\sqrt{9 + 4npq} \leq
-\frac{3}{2} + \frac{3}{2} + \sqrt{npq} = \sqrt{npq}. \]
Finally, since $0 \leq \delta-\delta_0 < 1$, we conclude that
\[ \delta = \delta_0 + \delta - \delta_0 < \sigma_0 + 1, \]
contradicting our initial choice of $\delta$.
\end{pf}


\section{Reporting randomized bits in the mix-net model}

There are $n+1$ single bit records being sent through a shuffler.  Before sending, each bit is randomized with $\cR$ - that is, flipped with probability $q$  and kept unchanged with probability $p=1-q$.  the outcome $S$ is the sum of randomized bits. Let $D$ be a set of $n$ bits and construct a neighboring pair of datasets by adding to $D$ a set bit and a 0 zero bit.  Then the privacy loss ratio at any given value of $S$ is expressed as:
\begin{equation} \label{eq:} 
R(S|D)= \frac{P(S|D \cup 0)}{P(S|D \cup 1)}
\end{equation}

Each probability allows conditioning on possible values the added bit could generate:
\begin{align}\label{lem:rs100}
P(S|D \cup 0) = p \cdot P(S | D ) + q \cdot P( S - 1 | D) 
\end{align}

Indeed, if $0$ bit is randomized to itself (with probability $p$), then $S$ must be generated by $D$ alone, while if $0$ bit was flipped (with probability $q$) then $D$ must generate $S-1$ total bit sum. Similarly 
\begin{align}
P(S|D \cup 1) = p  \cdot P(S -1 | D ) + q \cdot P( S  | D) 
\end{align}

Combining two conditioning expressions into the privacy loss ratio one arrives to:
\begin{equation} \label{eq:plratio}
R(S|D)=  \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  } = \frac{ p \frac{P(S | D )}{P(S - 1| D )} + q } { p + q \frac{P(S | D )}{P(S - 1| D )} }
\end{equation}

Let $\rho(S) = \frac{P(S | D )}{P(S - 1| D )}$ be a \textbf{probability ratio} between adjacent values of $S$.  It's related to $R(S)$ as in:
\begin{equation} \label{eq:plratio1}
R(S|D) =  \frac{ p \frac{P(S | D )}{P(S - 1| D )} + q } { p + q \frac{P(S | D )}{P(S - 1| D )} } = \frac{q + p\rho(S)}{p + q\rho(S)}
\end{equation}
Let $g(x) = \frac{q + px}{p + qx}$, the function $g$ is increasing over $x>0$, since
\[ g'(x) = \frac{p-q}{(p+qx)^2} > 0. \]
Therefore, properties of monotonicity and extrema established for $\rho(S)$  carry over to $R(S)$ as well.
 
 If  $D$ contains $m$ set bits, then the distribution of $S$ is a sum of two binomial distributions (a Poisson Binomial distribution): 
 \[ S  \sim Bin(m,p) + Bin(n-m,q) \]
 
 \begin{lem} \label{lem:rs1}
When 0-bit is replaced with a set bit, the resulting privacy loss ratio $R(S)$ decreases monotonically as $S$ grows, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}
\begin{pf}
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}
\end{pf}

 \begin{lem} \label{lem:rs1}
When 0-bit is replaced with a set bit, the resulting privacy loss ratio $R(S)$ decreases monotonically as $S$ grows, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}
\begin{pf}
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}
\end{pf}

\begin{lem} \label{lem:rs3}
Denote a collections of $n$ bits containing $r$ set bits and $n-r$ zero bits as $D_r$. Further denote the corresponding quantities:
\begin{itemize}
\item privacy loss ratio at a particular value $S$ as $R(S|D_r) = \frac{P(S|D_r)}{P(SD'_r)}z$
\item probability ratio at a particular value $S$ as $\rho(S|D_r) = \frac{P(S|D_r)}{P(S-1|D_r)}$
\item expected value of $S$ as $\mu_r = p \cdot r + q \cdot (n-r)$
\end{itemize}
Choose a distance $l$ such that $l \ge npq$, then 
\[ \rho[\mu_r - l|D_r] \le \rho[\mu_0 - (l+2)|D_0] \]
That is, the probability ratio for any collection is bound by the probability ratio of the zero collection. 
\end{lem}
\begin{pf}
PROOF IS INVOLVED AND WILL BE GIVEN LATER IN APPENDIX.  Max needs to fix Dave's notations, skipping for now.
\end{pf}

From lemma \eqref{lem:rs3} immediately follow corollaries below
\begin{cor}
For left deviations $l \ge npq$ from the mean the privacy loss ratio for the collection of $n$ bits is bounded by the privacy loss ratio for the collection of $n$ zero bits
\[ R[\mu_r - l|D_r] \le R[\mu_0 - (l+2)|D_0] \]
\end{cor}
\begin{pf}
From  \eqref{eq:plratio1} \[ R(S|D_r) = \frac{q + p\rho_r(S)}{p + q\rho_r(S)} \] 
Given that $\rho_r(S) \le \rho_0(S)$, we have 
\begin{align*}
R(S|D_r) = \frac{q + p\rho_r(S)}{p + q\rho_r(S)} \le R(S|D_0) = \frac{q + p\rho_0(S)}{p + q\rho_0(S)} \\
qp + q^2\rho_0(S) + p^2 \rho_r(S) + pq \cdot \rho_0(S) \cdot \rho_r(S) \le qp + p^2\rho_0(S) + q^2 \rho_r(S) + pq \cdot \rho_0(S) \cdot \rho_r(S) \\
\rho_r(S) (p^2 - q^2) \le \rho_0(S) (p^2 - q^2)  \\
\rho_r(S) \le \rho_0(S)
\end{align*} 
\end{pf}

The next corollary bounds the right tail of distribution
\begin{cor}
For right deviations $l \ge npq$ from the mean, the privacy loss ratio for the collection of $n$ bits is bounded by the privacy loss ratio for the collection of $n$ set bits
\[ R[\mu_r + l|D_r] \le R[\mu_n + l+2 |D_n] \]
\end{cor}
\begin{pf}
TODO - proving by symmetry between $D_0$ and $D_n$ distributions. 
\end{pf}

Using the bound \eqref{lem:rs201} and the fact that all-zero and all-set bit collections provide identical bounds to the privacy loss ratio for the left and the right side of distribution, we finally arrive to an important theorem.
\begin{prop}
randomization procedure $\cR$ is $(\epsilon, \delta)$-private on a collection $n$ bits, when the flipping frequency $q$ obeys the bound below
\begin{equation} \label{eq:fullbound}
q \ge  \frac { 3 \cdot ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon} - 2 \frac{p-q}{npq} \right ]^2 }
\end{equation}
\end{prop}
Note that the term $2 \frac{p-q}{npq}$ appeared due to $l+2$ correction of both corollaries above.  For sufficiently large $n$, this term diminishes to zero, which simplifies the bound to the form of lemma  \eqref{lem:rs203}

\begin{lem} \label{lem:rs2}
If randomization procedure $\cR$ is $(\epsilon, \delta)$-private on a collection $n$ bits, it's also $(\epsilon, \delta)$-private on a collection of $n+1$ bits.  In other words, if $n$ bit are protected with flipping frequency $q$, then all collections of greater size are also protected with same $q$. 
\end{lem}
\begin{pf}
Let $D_n$ be a collection of $n$ bits and derive a neighboring collection $D'_n$ by replacing a single bit.  Since $\cR$ is $(\epsilon, \delta)$-private for $n$ bits, then for any set of outcomes $W$
\begin{align}\label{lem:rs101}
 && P(\cR(D'_n) \in W) \le \exp(\epsilon)P(\cR(D_n) \in W) + \delta \\
 \implies && P(\cR(D'_n) \in W) - \exp(\epsilon)P(\cR(D_n) \in W) \le \delta
\end{align}
 
 Now add a 0 bit to both $D_n$ and $D'_n$.  The probability of the extended collections $D_{n+1}$ and $D'_{n+1}$ generating a particular outcome $S$ is given by  \eqref{lem:rs100}
 \begin{align}\label{lem:rs102}
P(\cR(D_{n+1}) = S) = P(\cR(D_n)=S) \cdot p + P(\cR(D_n)=S-1) \cdot q
\end{align}

Assume that $\cR(D_{n+1})$ is not $(\epsilon, \delta)$-private, then there must exists a set $W'$ such that 
\begin{align}\label{lem:rs103}
P(\cR(D'_{n+1}) \in W') > \exp(\epsilon)P(\cR(D_{n+1}) \in W') + \delta
\end{align}
 
Suppose $W'$ contains $r$ distinct outcomes $W'=\{S_1, S_2, \cdots, S_r\}$, then
\begin{align}
P(\cR(D_{n+1}) \in W') = \sum_i^r P( \cR(D_{n+1})  = S_i) \\
=  \sum_i^r \left [ P( \cR(D_{n})  = S_i) + P(\cR(D_n)=S_i-1) \cdot q \right ]  \text{ by  \eqref{lem:rs102} } \\
= p \sum_i^r P( \cR(D_{n})  = S_i)  + q \sum_i^r  P( \cR(D_{n})  = S_i - 1)  \label{lem:rs103}
\end{align}
 
 Define a set $W'' = \{S_1 - 1, S_2 - 1, \cdots, S_r - 1\}$, then \eqref{lem:rs100} can be rewritten in the form membership probabilities.  
 \begin{align}
P(\cR(D_{n+1}) \in W') = p \sum_i^r P( \cR(D_{n})  = S_i)  + q \sum_i^r  P( \cR(D_{n})  = S_i - 1) \\
= p \cdot P(\cR(D_n) \in W') + q \cdot P(\cR(D_n) \in W'')
\end{align}

In the same fashion we arrive to the expression of $P(\cR(D'_{n+1}) \in W')$
 \begin{align}
P(\cR(D'_{n+1}) \in W') = p \cdot P(\cR(D'_n) \in W') + q \cdot P(\cR(D'_n) \in W'')
\end{align}

Plugging the above probabilities into  \eqref{lem:rs103}, we arrive to an inequality that must hold if $\cR(D_{n+1})$ is not $(\epsilon, \delta)$-private.
 \begin{align}
 P(\cR(D'_{n+1}) \in W') > \exp(\epsilon)P(\cR(D_{n+1}) \in W') + \delta \\
P(\cR(D'_{n+1}) \in W') - \exp(\epsilon)P(\cR(D_{n+1}) \in W') > \delta \\
p \cdot P(\cR(D'_n) \in W') + q \cdot P(\cR(D'_n) \in W'') - p \cdot P(\cR(D_n) \in W') - q \cdot P(\cR(D_n) \in W'') > \delta \\
p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] > \delta   \label{lem:rs105}
\end{align}

However \eqref{lem:rs101} implies that 
 \begin{align*}
&& \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] \le \delta \\
\text{and} &&  \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le \delta \\
\implies && p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le p\delta + q\delta \\
\implies && p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le \delta (p+q) \\
\implies && p \left [ P(\cR(D'_n) \in W') - P(\cR(D_n) \in W') \right ] + q \left [ P(\cR(D'_n) \in W'') - P(\cR(D_n) \in W'') \right ] \le \delta
\end{align*}

Which contradicts  \eqref{lem:rs105} and proves the lemma.
\end{pf}


\subsection{properties of zero valued collection}
A homogenous collection of $n$ zero bits is an important spacial case, hence we present findings for it below.  Let $D$ consists of $n$ zero bits, then the neighbor $D'$ is achieved by replacing a zero bit with set bit. The outcome of applying procedure c is a sum of randomized bits $S$. The following relationships hold.

\begin{align}
\mu = E(S) = q \cdot n \\
P(S=i | D ) = \binom{n}{i}q^ip^{n-i} \\
P(S=i | D' ) = \binom{n-1}{i}q^{i+1}p^{n-1-i} +   \binom{n-1}{i-1}q^{i-1}p^{n-i+1}  \\
R(i)  = \frac{P(s=i | D')}{P(s=i | D)} = \frac{ \binom{n}{i}q^ip^{n-i} }{  \binom{n-1}{i}q^{i+1}p^{n-1-i} +   \binom{n-1}{i-1}q^{i-1}p^{n-i+1} } \\
\frac{1}{R(i)} =  \frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q}
\end{align}

By applying Chernoff bound to the distribution of $s$, we receive
 \begin{align}
P(|S - \mu| > t\mu) \le 2 e^{- \frac{t^2\mu}{3}}
\end{align}

Setting $t=\sqrt{\frac{3}{\mu} ln\frac{2}{\delta}}$ one arrives to
 \begin{align}
P \left (|S - \mu| >\sqrt{3nq \cdot ln\frac{2}{\delta}} \right ) \le \delta
\end{align}
 
Setting $l=\sqrt{3nq \cdot ln\frac{2}{\delta}}$, one is ensured that values of $P(S \in [\mu - l, \mu + l ]) \ge 1- \delta$. Conditioned on  $S \in [\mu - l, \mu + l ]$ we  bound the privacy loss ratio $R(i)$ in this interval in the following way:
\begin{align}
  && e^\epsilon \ge R(i) \ge e^{-\epsilon} \\ 
 \implies &&  e^{-\epsilon} \le \frac{1}{R(i)} \le e^\epsilon \\
  \implies &&  e^{-\epsilon} \le \frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q} \le e^\epsilon
\end{align}

We first bound the left side of the inequality, setting $i = \mu - l$
\begin{align}
\frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{n-(\mu-l)}{n}\frac{q}{p} + \frac{\mu-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{n-(nq-l)}{n}\frac{q}{p} + \frac{nq-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{np + l}{n}\frac{q}{p} + \frac{nq-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
q + \frac{l}{n}\frac{q}{p} +  p - \frac{l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
1 - \frac{l}{n} \frac{p-q}{pq}  \ge e^{-\epsilon}. \\
l \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q}  
\end{align}

Plugging expression for $l$ one arrives to the bound of $q$
\begin{align}
\sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q} \\
 \frac {(p - q)^2} {q \cdot p^2} \le   \frac { n \left [ 1 - e^{-\epsilon}\right ]^2 } { 3  ln\frac{2}{\delta}}  \\
 \text{ since }  \frac {(p - q)^2} { p^2} \le 1 \text{ , then }  \\
  \frac {(p - q)^2} {q \cdot p^2} \le \frac{1}{q}  \le   \frac { n \left [ 1 - e^{-\epsilon}\right ]^2 } { 3  ln\frac{2}{\delta}}  \\
  q \ge  \frac { 3  ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon}\right ]^2 } \label{lem:rs201}
\end{align}

In a similar fashion, one arrives to the right side bound
\begin{align}
i = \mu + l \\
\frac{l}{n} \frac{p-q}{pq}  \le e^\epsilon - 1 \\
\sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ e^\epsilon - 1  \right ] \frac { npq}{p - q} \\
  q \ge  \frac { 3  ln\frac{2}{\delta}} { n \left [ e^{\epsilon} - 1\right ]^2 }  \label{lem:rs202}
\end{align}

Note that since $x + 1/x \ge 2$, then 
\[ e^{\epsilon} - 1 \ge 1 - e^{-\epsilon} \]
Which implies that if $q$ bound  \eqref{lem:rs201} is met, then \eqref{lem:rs202} is also met, which leads to the following lemma

 \begin{lem} \label{lem:rs203}
 Bit flipping randomization procedure $\cR$ applied to a collection of $n$ zero bits is $(\epsilon, \delta)$-private if the bit flipping frequency $q$ satisfies \eqref{lem:rs201}
 \begin{align*}
  q \ge  \frac { 3 \cdot ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon}\right ]^2 }\end{align*}
\end{lem}

By using the symmetry property of all-zero and all-set bits distributions, one arrives at the the identical statement for all-set bits collection
 \begin{lem} \label{lem:rs204}
 Bit flipping randomization procedure $\cR$ applied to a collection of $n$ set bits is $(\epsilon, \delta)$-private if the bit flipping frequency $q$ satisfies \eqref{lem:rs201}
 \begin{align*}
  q \ge  \frac { 3 \cdot ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon}\right ]^2 }
\end{align*}
\end{lem}


\section{Clear Reports}

Assume that the data comes from a universe $\cX = [d]$ of $d$ elements. Each individual $i \in [n]$ of $n$ users has a data element $x_i\in \cX$ . We will write a data entry in bold $\bbx_i \in \{0, 1\}^d $ to be the one-hot vector where $x_i$ is zero in every position except position $\bbx_i \in \cX$ , where it is one. Furthermore, we will denote a dataset $\bbx = \{x_1,\dots ,x_n\}$ to be a collection of all users' one-hot vectors. We will have each user donate his data $\bbx_i$. Further, we will inject some fake reports $z_j\in \cX$ for $j \in [m]$, and corresponding one-hot vector notation $\bbz_j$, where each data entry is chosen uniformly at random from $\cX$. We then pass $\{\bbx_i : i \in [n]\}$ and $\{\bbz_j : j \in [m]\} $ to an anonymizer that shuffles the data and makes it impossible to determine whether a data record is real or fake. We call this algorithm 
\[
M(\bbx_1,  \dots , \bbx_n) = \pi (\bbx_1, \dots , \bbx_n, \bbz_1, \dots , \bbz_m) \text{ where } \pi \text{ permutes its elements}. 
\]
We then compute the privacy loss of such an algorithm $M$. Equivalently, we could write the output as a histogram over the entire database, as in $M(\bbx_1,  \dots , \bbx_n) = \sum^n_{i=1} \bbx_i + \sum^m_{j=1} \bbz_j$. Note that rather than inject random noise to these counts, as in central differential privacy, we want to consider \emph{anonymized differential privacy}, where data records are transmitted through a mix net to break any identifiers with each data entry and the server sees the aggregated records in some random order. In this model, there is no trusted server that injects noise to ensure DP. Rather, the user needs to only trust the anonymizer to shuffle real and fake records.

We then consider the privacy loss for a general mechanism $M$. Consider an outcome $h \in \N^d$, which is a histogram over the full dataset domain and neighboring datasets $\bbx$ and $\bbx'$.

\begin{align}
L(h) = \log \left ( \frac{\Pr[M(\bbx) = h]}{\Pr[M(\bbx') = h]} \right )
\end{align}

If we can bound $L(h)$ by $\epsilon$ for any outcome $h$ then we say that $M$ is $\epsilon$-DP. If we can bound $L(h)$ by $\epsilon$ with probability at least $1 - \delta$ where the randomness is over $h \sim M(\bbx)$, then we say that $M$ is $(\epsilon, \delta)$-DP.

We now focus on $M$ being the mechanism described above, which injects $m$ fake reports. We can then write the privacy loss in the following way where we assume, without loss of generality, that $\bbx$ and $\bbx'$ only differ in the first record, i.e. $\bbx_i = \bbx'_i$ for all $i \ne 1$.

\begin{align*}
L(h) &= \log \left ( \frac{\Pr[x_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]}{\Pr[x'_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]} \right ) \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \bbx_1 - \sum^n_{i=2} \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h - \bbx'_1 - \sum^n_{i=2} x_i ]} \right )  \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \sum^n_{i=1}  \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h  - \sum^n_{i=1} \bbx_i  - (\bbx'_1 - \bbx_1) ]} \right )
\end{align*}

We denote  $\hat{h}$ to be the histogram of the fake records only $\hat{h} = h - \sum^n_{i=1} \bbx_i$, with respective counts in each histogram bin $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_d\}$.  Then the privacy loss ratio can be written as:

\begin{align*}
L(h) =  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h}  + \bbx_1 - \bbx'_1) ]} \right )
\end{align*}



The one-hot vectors $\bbx_1$  and $\bbx'_1$ may only differ in two positions, let these positions be $\ell$ and $\ell'$. 
$\bbx_1$ and $\bbx'_1$ must have opposite bit-values in positions $i$ and $i'$ (otherwise these vectors are identical). 
Without loss of generality assume $x_{1,\ell} = 1, x_{1,\ell'} = 0$ and $x'_{1,\ell} = 0, x'_{1,\ell'} = 1$.  
Adding $\bbx_1$ adds 1 to $h_i$, while subtracting $\bbx'_1$ removes 1 from $h_\ell'$.
Hence,  if $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d\} $, then  $\hat{h} + \bbx_1 -\bbx'_1 =  \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} -1, \dots, \hat{h}_d\} $.

Further, note that the count the fake bits $\hat{h}_\ell = \sum^m_{j=1} \bbz_{j,\ell}$ is a binomial distribution $h_\ell \sim \text{Bin}(m, 1/d)$, and the distribution of the fake bit counts across the bins takes the multinomial form $ \hat{h} \sim \text{Multinomial}(m,(1/d,\cdots, 1/d))$. We then aim to bound the following quantity.

\begin{align*}
 L(h) &= \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h} + \bbx_1 - \bbx'_1]} \right ) \\
 & = \log  \left ( \frac{ {m \choose \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d}}{ { m \choose hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} - 1, \dots, \hat{h}_d}} \right ) \\
 & = \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) 
\end{align*}

It must be stressed that for a given pair of $(\bbx_1, \bbx'_1)$, the corresponding position pair $(\ell,\ell')$ where their bits are different is fixed, and the privacy loss only surfaces while observing the counts in the corresponding histogram bins $(h_\ell, h_{\ell'})$.  It's entirely possible to see high ratio between counts in some other histogram bins, but it wouldn't contribute to the privacy loss for a concrete pair $(\bbx_1, \bbx'_1)$.  This observation allows us to focus only on a single pair of the histogram bins, ignoring the rest of the histogram as immaterial.  

By applying a Chernoff bound, we have a bound (symmetric for the upper and lower tail) for the sum of the fake bits in any bin $\hat{h}_k = \sum^m_{j=1} z_{j,k},  k \in [d]$

\begin{align*}
 \Pr \left [ \left | \hat{h}_k - \frac{m}{d} \right | > t \frac{m}{d} \right ]  \le 2 e^{- \frac{m}{d} \frac{t^2}{3}}, \qquad \text{ for  } 0 < t < 1.
\end{align*}

Choose $t$ to fit the expression below, hence $t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }$.
%\begin{align}
%2e^{- \frac{m}{d} \frac{t^2}{3}} = \frac{\delta}{2}  \\
%- \frac{m}{d} \frac{t^2}{3} = \log{\frac{\delta}{4}}  \\
% \frac{m}{d} \frac{t^2}{3} = \log{\frac{4}{\delta}} \\
% t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }
%\end{align}
Using this expression for $t$ turns our Chernoff bound into the following,
 
 \begin{align}
 Pr \left [ \left | \bar{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \frac{\delta}{2}
\end{align}

Given any pair of the histogram bins at positions $(\ell,\ell')$, the probability of observing large deviation from the mean in at least one bin obeys the unions bound.
\[
\Pr \left [ \max_{k \in (\ell,\ell')} \left | \hat{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \delta
\]

We then condition on the event that both counts $\hat{h}_l$ or $\hat{h}_{l'}$ fall in the interval $m/d \pm \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}$, which event occurs with probability at least $1 - \delta$.  
Conditioned on there being the given number of fake records, we can upper bound the privacy ratio $L(h)$

 \begin{align}
 L(h) =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \le \log  \left ( \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \le \epsilon 
\end{align}

From the above, we then get a condition on the number of fake records, m, to ensure DP.
 \begin{align*}
& \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^\epsilon  \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) - 1 \ge 0 \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} \left ( \sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1) \right ) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  \ge 0 \\
\implies &\sqrt{  \frac{m}{d}}  \ge  \frac { \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  } { e^\epsilon - 1 } \\
\implies & \frac{m}{d}  \ge 3\log {\frac{4}{\delta}} \left ( \frac{e^\epsilon +1}{e^\epsilon - 1} \right )^2
\end{align*}

As for the lower bound of $L(h)$, it's met if the upper bound is met.
 \begin{align*}
 L(h) & =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \ge \log  \left ( \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \ge - \epsilon  \\
\implies &  \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \ge e^{-\epsilon} \\
 \implies &  \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} \le e^{\epsilon}  \\
\implies &   \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} < \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^{\epsilon} 
\end{align*}


%%%%%%%%%%%%%%%%%%% - but flipping with fake records

\section{Fake records and bit flipping}

We now apply the exact same protocol, whereby users produce $n$ real and $m$ fake reports, but require each 1-hot vector to bit bit-flipped with frequency $q$.  A randomization procedure $\cR(y)$ flips each bit of an arbitrary 1-hot-vector $y$ with probability $q$ and keeps it the same with probability $p=1-q$.  The resulting mechanism $M_r$ becomes a permutation of randomized true and fake records:
\[
M_r(\bbx_1,  \dots , \bbx_n) = \pi (\cR(\bbx_1), \dots , \cR(\bbx_n), \cR(\bbz_1), \dots , \cR(\bbz_m)) \text{ where } \pi \text{ permutes its elements}. 
\]

Without los of generality assume $\bbx_1$ is replaced with $\bbx'_1$ to receive a neighboring data set $\bbx'$.  The outcome is a histogram $g \in \N^d$ containing sums of randomized bits in each dimension, and the privacy loss:

\begin{equation} \label{eq:lgbound}
L(g) = \log \left ( \frac{\Pr[M_r(\bbx) = g]}{\Pr[M_r(\bbx') = g]} \right )
\end{equation}

The combined set $\bbx + \bbz$ gives raise to a histogram $h \in \N^d$ received by applying the before discussed mechanism M (clear true records plus fake records).  Hence, the $Pr [ M_r(x) = g ]$ can be written as a sum of probabilities over the domain of $h$:

 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ]  \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   }{    \sum_{h' \in \N^d}  Pr \left [  M(\bbx') = h' \right ] \cdot   Pr[ g | h']   } \right )
\end{align*}

Noting that
\[
 Pr \left [  M(\bbx) = h \right ]  =  Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]
 \] 
 
And regrouping  the privacy loss ratio to have summands with same $Pr \left [  M(\bbx) = h \right ]$ in identical positions in numerator and denominator, and applying \eqref{lem:rsbound} we have:
 \begin{align*}
& \log \left ( \max_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \text{ , and } \\
& L(g) \ge  log \left ( \min_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right ) \\
\end{align*}

Probabilities $Pr \left [  M(\bbx) = h \right ]$ and $ Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]$ cancel each other out in each ratio, hence giving us the bounds of the privacy loss over domain of $h$ .

\begin{align*}
\log \left ( \max_{h \in \N^d} \left ( \frac{  Pr [ g | h ]   } { Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \ge  log \left ( \min_{h \in \N^d}  \left ( \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right )
\end{align*}

Since bits are flipped independently, the probability of finding certain number of bits in a particular histogram bin $g_l$ depends only on how many not-yet-randomized set bits there are in the dimension $l$, that is the value of $h_l$.  Such independence allows to re-write $Pr [ g | h]$ as a product of probabilities for each dimension.
 \[
Pr [ M_r(x) = g ] = Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] = \prod_{i=1}^d Pr[ g_i | h_i] 
\]

Without loss of generality assume that $\bbx_1$ and $\bbx'_1$ differ in the first and second positions, that is $\bbx_{1,1} = 1, \bbx_{1,2} = 0$ and $\bbx'_{1,1} = 0, \bbx'_{1,2} = 1$, then
\begin{align*}
 & h  - \bbx_1 + \bbx'_1 = \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} \\
 \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{ Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] } { Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} ]  } \\
  \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{  Pr[ g_1 | h_1] Pr[ g_2 | h_2] \prod_{i=3}^d Pr[ g_i | h_i]  } {  Pr[ g_1 | h_1 - 1] Pr[ g_2 | h_2 + 1] \prod_{i=3}^d Pr[ g_i | h_i]  }  \\
   \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } =   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  } 
\end{align*}

Plugging the above formula into \eqref{eq:lgbound}, the privacy loss bounds become:
\begin{align*}
& \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right ) \right ) \ge L(g) \ge  \min_{h \in \N^d}  \left ( \log \left (  \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }   \right ) \right ) \\
\implies &  \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \max_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right) \ge L(g) \text{ , and } \\
&  L(g) \ge \min_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \min_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right)
\end{align*}

Basically, the privacy loss is bound by the sum of privacy losses in each of the affected dimensions.  Which enables relatively simple path to the bound.   We employ the results of lemma \eqref{lem:rs2}, which says that if $\cR$ is $(\epsilon, \delta)$-private on a collection $r$ bits, it's also $(\epsilon, \delta)$-private on collection of $r+1$ bits.  Therefore, a privacy loss could be bounded for the $m$ fake records only, and that will provide sufficient noise for the extra $n$ real records.  

Suppose that $r$ fake records (out of $m$) happened to have zero bits in the affected dimensions ($1$ and $2$).  We will show later that $r \to m$,  for  large $d$.  Then we are bounding the product of privacy loss ratios in the affected dimensions to stay between $e^{-\epsilon}$ and $e^\epsilon$ with probability $1-\delta$.
\begin{align} \label{lem:fk101}
P \left (   e^\epsilon \ge \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \ge \frac{1}{e^\epsilon} \right ) \le 1 - \delta
\end{align}

We achieve condition of \eqref{lem:fk101} by bounding the ratio in each dimension separately.  Suppose that the following holds
\begin{align*} \label{lem:fk101}
&& P \left (   e^\frac{\epsilon}{2} \ge \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1] }  \ge \frac{1}{e^\frac{\epsilon}{2}} \right ) \le 1 - \delta/2 \\
\text{and} && P \left (   e^\frac{\epsilon}{2} \ge \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \ge \frac{1}{e^\frac{\epsilon}{2}} \right ) \le 1 - \delta/2 
\end{align*}

Then, by the union bound, the combined probability of either ratio falling outside its bound is $\delta$, and with probability $1-\delta$, both ratios stay between $e^{-\frac{\epsilon}{2}}$ and $e^\frac{\epsilon}{2}$, hence the product of the ratios is bounded in $[ e^{-\epsilon}, e^\epsilon]$.   

\begin{prop}
\end{prop}


%%% Bounding ratio of sums by max ratio of terms

\section{Appendix: Ratios of sums: properties}

Here we establish some results around bounding and comparing ratios of sums, which will be useful in working with the privacy ratio.


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}


\newpage

References

[1] A Note on Differential Privacy: Defining Resistance to Arbitrary Side Information.  Shiva Prasad Kasiviswanathan Adam Smith
[2] Privacy Odometers and Filters: Pay-as-you-Go Composition. Ryan Rogers, Aaron Roth, Jonathan Ullman, Salil Vadhan


 \newpage
 \section{IGNORE BELOW THIS LINE}
 
 Suppose a new 1-bit is added to both collections, then $R_{n+1}(S)$ is derived by conditioning
 \begin{align*}
R_{n+1}(S) = \frac{ P(S|D \cup 1) }{ P(S|D' \cup 1) } = \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p}
\end{align*}
 
By lemma \eqref{lem:rsbound} and lemma  \eqref{lem:rs1} we have
\begin{align}
 & \frac{P(S-1|D)}{P(S-1|D'} \ge  \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p} \ge \frac{P(S|D)}{P(SD'} \\
\implies & R_n(S-1) \ge R_{n+1}(S) \ge R_n(S) \\
\implies & R_n(S) \ge R_{n+1}(S+1) \ge R_n(S+1) \label{lem:rsb2}
\end{align}
 
Suppose $R_n$ is $(\epsilon, \delta)$-private. Since $R_n$ is monotonically decreasing with $S$ (lemma  \eqref{lem:rs1} ), there exist two values $\alpha + \beta \le \delta$, such that $R_n$ is upper bounded on the left at a particular limiting value $S_\alpha$ 
 \begin{align}  \label{lem:rsb3}
 R_n(S_\alpha) \le e^\epsilon \text{ and } P_n(S \le S_\alpha) \le \alpha 
\end{align}
And it's low bounded on the right at a particular limiting value $S_\beta$
 \begin{align}  \label{lem:rsb4}
 R_n(S_\beta) \ge \frac{1}{e^\epsilon} \text{ and } P_n(S \ge S_\beta) \le \beta
\end{align}
 
Consider the left (upper) bound first, and recall that according to \eqref{lem:rsb2}
\[
 R_n(S_\alpha) \ge R_{n+1}(S_\alpha+1) \ge R_n(S_\alpha+1)
 \]
 
$R_{n+1}(S_\alpha+1)$ is bounded because  $R_n(S_\alpha)$ is bounded per \eqref{lem:rsb3}.  Hence, $R_{n+1}$ could only be over the bound at $S_\alpha$, however the cumulative sum of probabilities up to $S_\alpha$ is always less for $n+1$ bits than for $n$ bits.
 \begin{align}  \label{lem:rsb5}
P_{n+1}(S \le S_\alpha) \le P_{n}(S \le S_\alpha)
\end{align}
We shall prove \eqref{lem:rsb5} in a moment. The important fact is that $R_n$ upper bounds $R_{n+1}$ at the left tail of distribution of $S$. 

Similarly, 
 
 
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}



\begin{lem} \label{lem:rsreduce}
 The privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}

 the privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.







Note that $\frac{P(S | D )}{P(S - 1| D )}$ as a \textbf{probability ratio} between adjacent values of $S$.  It's easy to see that privacy loss ratio maximizes when \textbf{probability ratio} maximizes. 

Recall that $p>q$ and consider two positive values $A$ and $B$ 
\begin{align*}
\frac{ p \cdot A + q } { p + q \cdot A }  \ge \frac{ p \cdot B + q } { p + q \cdot B } \\
p^2A + q^2B \ge p^2 B + q^2 A  \\
A (p^2 - q^2) \ge B (p^2 - q^2)  \\
A \ge B
\end{align*}

The above confirms that the distributions with largest \textbf{probability ratio} also exhibit larger privacy loss ratio. Hence we can focus on studying \textbf{probability ratio}  instead of privacy loss ratio and choose those $D$ that demonstrate sharpest decrease of probabilities in the left tail.




Consider a collection $D$ of $N$ bits, subjected to randomization procedure $\cR$, whereby a bit is flipped with probability $q$ and kept unchanged with probability $p=1-q$.  The outcome of $\cR$ is a $a$ - sum of bits after randomization. The neigboring set $D_m$ is recieved form Assuming that $D$ contains $m$ set bits, we consider a privacy loss ratio $R_s$ computed for the outcome $s$:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 


consisting of $m$ ones and $N-m$ zeros.  Denote probability of number of successes for that collection as $P(S|D)$.
The probability ratio at $s$  is given by:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 

Denote expectation of $s$ as $\mu$:
\[ \mu = mp + (N-m)q \]

For simplicity, denote probabilities at $s$ for $D$ as:
\[ P(s|D) = P_{s} \]

It's known that for all $s < \mu$ , the ratio $R_s$ is greater than $1$ and increasing:

\textbf{Property 1.}
\begin{align}
R_{s-1} = \frac{P_{s-1}}{P_{s-2}} > R_s  = \frac{P_{s}}{P_{s-1}} \\
P^2_{s-1} > P_sP_{s-2} 
\end{align}

Create two collections by adding to D one $1$ and one $0$.  Call them $D_1$ and $D_0$ respectively. The probability of observing $s$ from $D_1$ the is given by:
\[ P(s | D_1) = pP_{s-1} + qP_s \]

Similarly for the second collection (with extra 0):

\[ P(s | D_0) = qP_{s-1} + pP_s \]

Now consider the probability ratio for the collections $D_1$ and $D_0$ collections at some $s$:
\begin{align}
R_s(D_1) = \frac{ pP_{s-1} + qP_s }{pP_{s-2} + qP_{s-1}} \\
R_s(D_0) = \frac{ qP_{s-1} + pP_s }{qP_{s-2} + pP_{s-1}}
\end{align}
 $N$ user bits are subjected to  


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}



\section{JUNK}


Given the independence 



 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ] =  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right ) \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right )  }{    \sum_{h' \in \N^d} \left ( Pr \left [  M(\bbx') = h' \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h'_i]  \right )  } \right )
\end{align*}



More formally, the value of $g_l$ is a sum of binomial distributions 
Where $r$ is the number of set bits (both clear and fake) in the dimension $l$.   This allows us to 



The value of $g_l$ distributed as a sum of two binomial variables.  
\[ g_l  \sim Bin(h_l,p) + Bin(n+m-h_l,q) \]
 
 
Applying \eqref{lem:rsbound} gives bounds of $R(S)$

\begin{equation} \label{eq:rsbounds}
 \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  } R(S|D)=  \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  }
\end{equation}

\end{document}

