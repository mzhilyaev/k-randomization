%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt]{article}
%\documentclass[11pt,draft]{amsart}

% Custom styling.
\usepackage{custom}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}
%% Title matter
\title{Notes}
\author{Maxim Zhilyaev}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amssymb}

\newcommand{\bbx}{\pmb{x}}
\newcommand{\bbz}{\pmb{z}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\reals}{\mathbb{R}} % Real number symbol
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}} % Complex numbers
\newcommand{\integers}{\mathbb{Z}} % Integer symbol
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}} % Rational numbers
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\naturals}{\mathbb{N}} % Natural numbers
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}} % expectation

\newcommand{\Dsp}{\mathcal{D}}
\newcommand{\Ssp}{\mathcal{S}}
\newcommand{\Bsp}{\mathcal{B}}
\newcommand{\Tsp}{\mathcal{T}}
\newcommand{\Hsp}{\mathcal{H}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\sv}{\mathbf{s}}
\newcommand{\mv}{\mathbf{m}}
\newcommand{\ev}{\mathbf{e}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\pv}{\mathbf{p}}
\newcommand{\Pm}{\mathbf{P}}
\newcommand{\xvt}{\tilde{\xv}}
\newcommand{\yvt}{\tilde{\yv}}
\newcommand{\sm}{\sv^-}
\newcommand{\iv}{\mathbf{i}}
\newcommand{\one}{\boldsymbol{1}}
\newcommand{\zero}{\boldsymbol{0}}
\newcommand{\mb}{\overline{m}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Abstract}
We study a variety of the shuffling protocols for reporting one-hot vectors from multiple users with respect to privacy, sensitivity and practicality.  From a practical standpoint, the cost of shuffling is not zero. Too many shuffled records may render a particular protocol impractical, even though its other metrics show good performance. We specifically consider protocols that minimize the number (but not necessarily the size) messages between a user device and the shuffler.

Assuming that the data comes from a universe $\cX = [d]$ of $d$ elements. Each individual $i \in [n]$ of $n$ users has a data element $x_i\in \cX$ . We will write a data entry in bold $\bbx_i \in \{0, 1\}^d $ to be the one-hot vector where $x_i$ is zero in every position except position $\bbx_i \in \cX$ , where it is one. Furthermore, we will denote a dataset $\bbx = \{x_1,\dots ,x_n\}$ to be a collection of all users' one-hot vectors. We consider multiple mix-net protocols for reporting 1-hot vectors.  A simple one would require each user to donate his data $\bbx_i$ in clear, but, in addition, inject some fake reports $z_j\in \cX$ for $j \in [m]$, and corresponding one-hot vector notation $\bbz_j$, where each data entry is chosen uniformly at random from $\cX$. We then pass $\{\bbx_i : i \in [n]\}$ and $\{\bbz_j : j \in [m]\} $ to an anonymizer that shuffles the data and makes it impossible to determine whether a data record is real or fake.  We call it the "clear-fake records" protocol and show that it provides adequate protection with the cost proportional to $[d]$. Hence if dimensions are not large then the "clear-fake records" protocol is preferred for its simplicity. 

When $[d]$ is significant, the cost of sending and shuffling many fake records becomes prohibitive. Another protocol is developed,  which parameters are independent of $[d]$.  It's called a "fake and flip" protocol, whereby  a user still generates true and fake one-hot report vectors that are both randomized by bit flipping before being sent to the shuffler.  This enables adequate protection at reasonable cost. Depending on the data collection setting, various flavors of the "fake and flip" protocol are discussed.

In discussing mathematical properties of the protocols involving randomization we will rely upon results received for a single dimension bit reporting.  Which results we provide in the first sections, along with some theoretical result claiming that if a randomization algorithm  $\cR$ is $(\epsilon, \delta)$-private on a dataset of $n$ elements, it's also  $(\epsilon, \delta)$-private on a dataset of $n+1$ elements, that is adding more elements to the shuffled set does not reduce privacy. These results are, then, used to develop bounds for each protocol.

\section{Differential Privacy Setup}
\label{sec:dp}

A record is an element of some space $\Dsp$, and a database $\xv$ is a vector of $n$ records: $\xv = (x_1,\dots,x_n) \in \Dsp^n$.
A randomized algorithm $\cR$ maps the database into another space: $\map{\cR}{\Dsp^n}{\Ssp}$. 
The result of applying an algorithm to a database is termed an \textbf{transcript}.
The notion of differential privacy for an algorithm $\cR$ is that the resulting transcripts does not change substantially when a record in the database is modified,
\ie transcripts are not sensitive to particular individual records in the database.
Hence, releasing transcript of $\cR$ publicly will not jeopardize privacy, since information regarding individual records cannot be gained by analyzing the outcome of $\cR(\xv)$.

%%% Differential privacy definition

Differential privacy for a randomized algorithm $\cR$ is formulated by comparing the transcripts generated by applying $\cR$ to two very similar databases $\xv,\xv' \in \Dsp^n$.
We say the databases \textbf{differ in one row} if 
$\sum_{i=1}^n I(x_i \neq x'_i) = 1$.  
Such datasets are commonly called  \textbf{neighboring} database or  \textbf{neighbors}.
\begin{defn}
A randomized algorithm $\cR$ is $(\epsilon,\delta)$-\textbf{differentially private} if, 
for any two databases $\xv,\xv' \in \Dsp^n$ differing in one row,
\begin{equation}\label{eq:dpdef}
\P[\cR(\xv) \in S] \leq \exp(\epsilon) \cdot \P[\cR(\xv') \in S] + \delta
\end{equation}
for all $S \cont \Ssp$ (measurable).
\end{defn}
In other words, the outcomes from the two databases databases differing in one row are close in distribution, may be with the exception of very unlikely outcomes whose probability is less than $\delta$

\begin{defn}
A randomized algorithm $\cR$ generates point-wise  $(\epsilon,\delta)$-\textbf{indistinguishable} outcomes for two databases $\xv,\xv' \in \Dsp^n$ when
\begin{equation} \label{eq:dpcnt}
\P \left ( \frac{\P[\cR(\xv) = s]}{\P[\cR(\xv') = s]} \le  \exp(\epsilon) \right ) \ge 1 - \delta
\end{equation}
\end{defn}

\begin{prop} \label{prop:dpcnt}
A randomized algorithm $\cR$ is $(\epsilon,\delta)$-\textbf{differentially private} if for every pair of neighboring databases $\xv,\xv' \in \Dsp^n$, $\cR$ generates point-wise  $(\epsilon,\delta)$-\textbf{indistinguishable} outcomes.   Per reference [2].  (Ryan Rogers, etc..)

To restate.
\begin{align} \label{eq:dpcnt}
 & \P \left ( \frac{\P[\cR(\xv) = s]}{\P[\cR(\xv') = s]} \le  \exp(\epsilon) \right ) \ge 1 - \delta \text{, for any two neighbors } \xv,\xv' \\
\implies & \cR \text{ is } (\epsilon,\delta)-\textbf{differentially private}
\end{align}
\end{prop}

\begin{prop}\label{cr:max} If ${\Ssp}$  if finite, there exists a set $S_m \cont \Ssp$, such that it maximizes the difference
\[ \P[\cR(\xv) \in S]  - \exp(\epsilon) \cdot \P[\cR(\xv') \in S] \]

Suppose ${\Ssp}$  if finite.  Then the for any $S \cont \Ssp$ respective set inclusion probabilities are
\begin{align*}
\P[\cR(\xv) \in S]  = \sum_{s \in S} \P[\cR(\xv) = s]  \\
 \P[\cR(\xv) \in S]  = \sum_{s \in S} \P[\cR(\xv') = s]  \\
\end{align*}

Consider a set  $S_m \cont \Ssp$ containing all and only $s \in S$, such that $\P[\cR(\xv) = s] >  \exp(\epsilon) \P[\cR(\xv') = s]$, that is - all point-wise distinguishable values of $s$.  The  $\sum_{s \in S} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] $ reaches maximum in $S_m$. Indeed, any set $S$ different from $S_m$ will contain either point-wise indistinguishable values of $s$ and they would reduce the sum or miss point-wise distinguishable values of $s$, which would also reduce the sum.   More formally:
\begin{align*}
& \sum_{s \in S_m} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] - \sum_{s \in S} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] \\
= & \sum_{s \in S_m \setminus S} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s]  -  \sum_{s \in S \setminus S_m} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] \\
 & \sum_{s \in S \setminus S_m} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s]  < 0, \text{ since } \forall s \in S \setminus S_m,   \P[\cR(\xv) = s] < \exp(\epsilon)  \P[\cR(\xv') = s] \\
 & \sum_{s \in S_m \setminus S} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s]  \ge 0, \text{ since } \forall s \in S_m \setminus S,   \P[\cR(\xv) = s] > \exp(\epsilon)  \P[\cR(\xv') = s] \\
 \text{hence} &  \sum_{s \in S_m \setminus S} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s]  -  \sum_{s \in S \setminus S_m} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] > 0
\end{align*}
\end{prop}

\begin{prop} \label{prop:equavalent} For finite domains ${\Ssp}$ the $(\epsilon,\delta)$-\textbf{differential privacy} and  $(\epsilon,\delta)$-\textbf{indistinguishability} are equivalent - one implies the other. 

Suppose ${\Ssp}$  if finite.  Then the $(\epsilon,\delta)$-\textbf{differentially private} condition holds if the following holds:
\begin{align}\label{eq:diff}
\sum_{s \in S} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] \le \delta , \forall S \cont \Ssp
\end{align}

And there exists the maximal $S_m$ containing all and only point-wise distinguishable values of $s$.  $S_m$ maximizes the difference below.
 \[ \P[\cR(\xv) \in S]  - \exp(\epsilon) \cdot \P[\cR(\xv') \in S] \]
 
The probability of $s$ being point-wise distinguishable is exactly the probability $P[s \in S_m]$ since it contains just the distinguishable $s$, and hence if  $\cR$ is $(\epsilon,\delta)$-\textbf{differentially private}, then it's also $(\epsilon,\delta)$-\textbf{indistinguishable}.

Now assume $\cR$ is $(\epsilon,\delta)$-\textbf{indistinguishable}, then by definition
\begin{align*}
& \P( \P[\cR(\xv) = s] >  \exp(\epsilon) \P[\cR(\xv') = s] ) < \delta & \\
\implies & P( \P[\cR(\xv) \in S_m] >  \exp(\epsilon) \P[\cR(\xv') \in S_m]) < \delta & \\
\implies & \sum_{s \in S_m} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] < \delta & \\
\implies & \forall S \cont \Ssp,  \sum_{s \in S} \P[\cR(\xv) = s] -  \exp(\epsilon)  \P[\cR(\xv') = s] < \delta  & \text{ by maximality of } S_m
\end{align*}
\end{prop}

\section{Single record shuffling protocol}

There are $n$ users, each holding a user value $x_i \in \cX$.  User values form a database of user records a dataset $\bbx = \{x_1,\dots ,x_n\}$. Each user applies a randomization procedure $\cR(x): \map{\cR}{\cX}{\Ssp}$, then submits $\cR(x_i)$ to an anonymizer that shuffles the data and makes it impossible to determine which user submitted a record. We call this algorithm 
\[
M(x_1,  \dots , x_n) = \pi (\cR(x_1), \dots , \cR(x_n)) \text{ where } \pi \text{ permutes its elements}. 
\]

If $\Ssp$ a finite domain of dimension $d$,  we can write the output of $M$ as a histogram $h \in \N^d$ over the entire domain $\Ssp$.   $M(x_1,  \dots , x_n) = \{h_1, h_2, ..., h_d\}$. Where each histogram value $h_i$ represents the number of users reported a particular value $s_i \in  \Ssp$.   We then describe $M$ as a mapping $M(\bbx):\map{M}{\bbx}{\Hsp_n = \N^d}$, whereby an outcome of $M(\bbx)$ is a particular histogram $h \in \Hsp_n$, that is a histograms of $d$-bins and $n$-records.

\begin{prop} \label{prop:nprot} If $M$ is $(\epsilon,\delta)$-\textbf{differentially private} for $n$ records, then it's $(\epsilon,\delta)$-\textbf{differentially private} for $n+1$ records. Thus, protection for $n$ records is enough for any number of records above $n$

Assume  $M$ is  $(\epsilon,\delta)$-\textbf{differentially private} for $n$ values of $\bbx$. Then for any neighboring datasets $\bbx$ and $\bbx'$ of size $n$, and any $H_n \cont \Hsp_n$
 \begin{align}
 \P[M(\xv) \in H_n] - \exp(\epsilon) \cdot \P[M(\xv') \in H_n]  \le \delta &  & \\
 \sum_{h \in H_n} \left ( \P[M(\xv) = h] -  \exp(\epsilon)  \P[M(\xv') = h]  \right )  \le \delta  &  & \Hsp_n \text{ is finite }  \label{eq:histdiff}
\end{align}
Add another user record $x$, then respective probabilities of an outcome $h=\{h_1, h_2, ..., h_d\}$ produced by $M(\xv \cup x)$ and $M(\xv' \cup x)$ is given below:
 \begin{align}
\P[M(\xv \cup x) = h] = \sum^d_i \P[\cR(x) = s_i]\P(M(\xv) = \{h_1 , \cdots ,h_i -1, \cdots, h_d \} \label{eq:hp1} \\
\P[M(\xv' \cup x) = h] = \sum^d_i \P[\cR(x) = s_i]\P(M(\xv') = \{h_1 , \cdots ,h_i -1, \cdots, h_d \} \label{eq:hp2}
\end{align}

Since $ \Hsp_{n+1}$ if finite, then for every set $H_{n+1} \cont  \Hsp_{n+1}$:
 \begin{align*}
 & \P[M(\xv \cup x) \in H_{n+1}] - \exp(\epsilon) \cdot \P[M(\xv' \cup x) \in H_{n+1}]  & \\
 = & \sum_{h \in H_{n+1}} \left ( \P[M(\xv \cup x) = h] -  \exp(\epsilon)  \P[M(\xv' \cup x) = h]  \right )  & \\
 =  & \sum_{h \in H_{n+1}}   \left (    \begin{matrix}
\sum^d_i \P[\cR(x) = s_i]\P(M(\xv) = \{h_1 , \cdots ,h_i -1, \cdots, h_d \}  \\ 
 -  \exp(\epsilon)\sum^d_i \P[\cR(x) = s_i]\P(M(\xv') = \{h_1 , \cdots ,h_i -1, \cdots, h_d \}   
\end{matrix}
  \right) &  \eqref{eq:hp1}  and  \eqref{eq:hp2} 
\end{align*}

After rearranging the oder of summation and combining terms with same $ \P[\cR(x) = s_i]$, we have:
 \begin{align*}
& \P[M(\xv \cup x) \in H_{n+1}] - \exp(\epsilon) \cdot \P[M(\xv' \cup x) \in H_{n+1}]  \\
= & \begin{pmatrix} 
& \P[\cR(x) = s_1] \sum_{h \in H_{n+1}}  \left (   \P(M(\xv) = \{h_1 -1 , h_2, \cdots, h_d \}  -  \exp(\epsilon)\P(M(\xv') = \{h_1 - 1, h_2,  \cdots, h_d \}   \right) \\
+ & \P[\cR(x) = s_2] \sum_{h \in H_{n+1}}  \left (   \P(M(\xv) = \{h_1, h_2 - 1, \cdots, h_d \}  -  \exp(\epsilon)\P(M(\xv') = \{h_1,  h_2 - 1, \cdots, h_d \}   \right) \\
& \cdots \cdots \\
+  & \P[\cR(x) = s_d] \sum_{h \in H_{n+1}}  \left (   \P(M(\xv) = \{h_1, h_2, \cdots, h_d -1 \}  -  \exp(\epsilon)\P(M(\xv') = \{h_1, h_2,  \cdots, h_d -1 \}   \right) 
\end{pmatrix}
 \end{align*}
Note that each sum of the form  
\[ \sum_{h \in H_{n+1}}  \left (   \P(M(\xv) = \{h_1, \cdots, h_i-1, \cdots, h_d \}  -  \exp(\epsilon)\P(M(\xv') = \{h_1 - 1,  \cdots, h_i-1, \cdots, h_d \}   \right) \]
is done over histograms of size $n$, and represents a differential privacy difference, which by assumption  \eqref{eq:histdiff} bounded by $\delta$
\[ \P[M(\xv) \in H_n] - \exp(\epsilon) \cdot \P[M(\xv') \in H_n] \le \delta \]

From here:
 \begin{align*}
& \P[M(\xv \cup x) \in H_{n+1}] - \exp(\epsilon) \cdot \P[M(\xv' \cup x) \in H_{n+1}]  \\
= & \begin{pmatrix} 
& \P[\cR(x) = s_1] \sum_{h \in H_{n+1}}  \left (   \P(M(\xv) = \{h_1 -1 , h_2, \cdots, h_d \}  -  \exp(\epsilon)\P(M(\xv') = \{h_1 - 1, h_2,  \cdots, h_d \}   \right) \\
+ & \P[\cR(x) = s_2] \sum_{h \in H_{n+1}}  \left (   \P(M(\xv) = \{h_1, h_2 - 1, \cdots, h_d \}  -  \exp(\epsilon)\P(M(\xv') = \{h_1,  h_2 - 1, \cdots, h_d \}   \right) \\
& \cdots \cdots \\
+  & \P[\cR(x) = s_d] \sum_{h \in H_{n+1}}  \left (   \P(M(\xv) = \{h_1, h_2, \cdots, h_d -1 \}  -  \exp(\epsilon)\P(M(\xv') = \{h_1, h_2,  \cdots, h_d -1 \}   \right) 
\end{pmatrix} \\
= & \sum^d_i  \P[\cR(x) = s_i] \left (   \P[M(\xv) \in H^i_n] - \exp(\epsilon) \cdot \P[M(\xv') \in H^i_n] \right ) \\
\le &  \sum^d_i  \P[\cR(x) = s_i]  \delta \\
= & \delta \sum^d_i  \P[\cR(x) = s_i]  
 \end{align*}
 
 Since $ \sum^d_i  \P[\cR(x) = s_i] =1$, we arrive to the desired proof
 \[  \P[M(\xv \cup x) \in H_{n+1}] - \exp(\epsilon) \cdot \P[M(\xv' \cup x) \in H_{n+1}]   \le \delta \]

\end{prop}


\section{Single bit shuffling protocol}

We now consider an important scenario where users report bit values, which they flip and send randomized result to the shuffler. 
There are $n$ users, each holding a value $x \in \{0,1\}$.  User bits form a dataset $D  \cont \Dsp = \{0,1\}^n$. 
Each user applies a randomization procedure $\cR(x): \map{\cR}{\{0,1\}}{\{0,1\}}$, which flips the original bit value with probability $q$ and keeps it unchanged with probability $p=1-q$.  
A user, then submits a randomized bit $\cR(x_i)$ to an anonymizer that shuffles the data and makes impossible to trace a reported bit to its sender. As before, we consider algorithm
\[
M(D) = \pi (\cR(x_1), \dots , \cR(x_n)) \text{ where } \pi \text{ permutes its elements}. 
\]

Since the the reported bits are shuffled, the measurer can only add them up, and the outcome $M(D)$ is uniquely determined by the sum of the reported randomized bits $s \in \Ssp = \{0,1,\cdots,n\}$. Let $D$ be a set of $n$ bits and construct a neighboring pair of datasets by adding to $D$ a set bit $1$ and a zero bit $0$.  Note that the domain $\Ssp$ is finite.  Hence, due to proposition \ref{prop:equavalent}, there are two equivalent expression for the $(\epsilon,\delta)$-\textbf{differential privacy} condition.

The algorithm $M$ is differentially private if, for any subset $Z \cont  \Ssp$

\begin{align} \label{eq:bited} 
 & P[M(D \cup 0) \in Z] \le e^{\epsilon}P[M(D \cup 1) \in Z] + \delta \\
\& & P[M(D \cup 1) \in Z] \le e^{\epsilon}P[M(D \cup 0) \in Z] + \delta 
\end{align}

Equivalently, the algorithm $M$ is differentially private, if it is $(\epsilon,\delta)$-\textbf{indistinguishable}: 
\begin{align} \label{eq:bitind} 
P \left (   e^{-\epsilon} \le \frac{P(M(D \cup 0) = s)}{P(M(D \cup 1) = s)} \le e^{\epsilon} \right ) \ge 1 - \delta, \forall s \in \{0,1,\cdots,n+1\}
\end{align}

The quantity in parenthesis is referred as  a \textbf{privacy loss ratio} $R$.  For every instance of $D$, one can express the privacy loss ratio for a particular outcome $s$ as:
\begin{equation} \label{eq:1} 
R(s|D)= \frac{P(M(D \cup 0) = s)}{P(M(D \cup 1) = s)}  = \frac{P(s|D \cup 0) )}{P(s|D \cup 1)} 
\end{equation}

Note that \ref{eq:bitind} requires both $R$ and its reciprocal $1/R$ be $\delta$-bounded by $e^\epsilon$. Whereby the first case corresponds to replacing a user bit 1 with 0 bit, and the second is reversed. We shall show the symmetry of both ratios later in the sequel. 

Conditioning on possible values the added bit could generate:
\begin{align}\label{lem:rs100}
P(s|D \cup 0) = p \cdot P(s | D ) + q \cdot P( s - 1 | D) 
\end{align}

Indeed, if $0$ bit is randomized to itself (with probability $p$), then $s$ must be generated by $D$ alone, while if $0$ bit was flipped (with probability $q$) then $D$ must generate $s-1$ total bit sum. Similarly 
\begin{align}
P(s|D \cup 1) = p  \cdot P(s -1 | D ) + q \cdot P( s  | D) 
\end{align}

Combining two conditioning expressions into the privacy loss ratio one arrives to:
\begin{equation} \label{eq:plratio}
R(s|D)=  \frac{p \cdot P(s | D ) + q \cdot P( s - 1 | D) } {  p  \cdot P(s -1 | D ) + q \cdot P( s  | D)  } = \frac{ p \frac{P(s | D )}{P(s - 1| D )} + q } { p + q \frac{P(s | D )}{P(s - 1| D )} }
\end{equation}

Let $\rho(s) = \frac{P(s | D )}{P(s - 1| D )}$ be a \textbf{probability ratio} between adjacent values of $s$.  It's related to $R(s)$ as in:
\begin{equation} \label{eq:plratio1}
R(s|D) =  \frac{ p \frac{P(s | D )}{P(s - 1| D )} + q } { p + q \frac{P(s | D )}{P(s - 1| D )} } = \frac{q + p\rho(s)}{p + q\rho(s)}
\end{equation}
Let $g(x) = \frac{q + px}{p + qx}$, the function $g$ is increasing over $x>0$, since
\[ g'(x) = \frac{p-q}{(p+qx)^2} > 0. \]
Which gives as an important corollary 
\begin{cor}\label{cr:phr} Properties of monotonicity and extrema established for $\rho(s)$  carry over to $R(s)$ .
\end{cor}
 
 \begin{lem} \label{lem:rs1}
The privacy loss ratio $R(s)$ decreases monotonically as $s$ grows, reaching its maximum in $s=0$ and minimum in $s=n$.
\end{lem}
\begin{pf}
 Suppose $D$ contains $m$ set bits, then the distribution of $s$ is a sum of two binomial distributions, and is  a Poisson Binomial distribution.
\begin{equation}
 s  \sim Bin(m,p) + Bin(n-m,q)  \label{lem:bpd} 
 \end{equation}
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(s)^2  > P(s-1) \cdot P(S+1) & \\
\implies &  \rho(s-1) > \rho(s)  & \\
\implies & R(s-1) > R(s) & \text{ by } \ref{cr:phr}
\end{align*}
\end{pf}

According to corollary \ref{cr:max} there exists a set $S_m$ containing only point-wise distinguishable values of $s$.  Then, by lemma \ref{lem:rs1} such set includes only values from 0 to $k$ for which $R(s) > e^\epsilon$.   This immediately gives us an expression for $(\epsilon,\delta)$-\textbf{differential privacy}  in binary case.
\begin{lem} \label{lem:dp_bin}
The algorithm $M$ is $(\epsilon,\delta)$-\textbf{differentially} private if and only if   
\begin{equation} \label{eq:dp_bin}
(2pe^\epsilon - 1)P(k | D ) - (e^\epsilon - 1) \sum^{k} _{i=0}P(i|D) \le \delta, \forall k \in \{0,1,\cdots, n\}
\end{equation}
\begin{pf}
Consider probability $P[s \in S_m|D \cup 0]$, suppose $S_m = \{0,1, \cdots, k\}$, then
\begin{align*}
 & P[s \in S_m|D \cup 0]   \\
 = & \sum^k_{i=0} \left [ p P(i | D ) + q P( i - 1 | D) \right ]  \\
 = & p P(k | D )  + (q+p)P(k-1|D) + (q+p)P(k-2|D) + \cdots + (q+p)P(0|D)  \\
 = & p  P(k | D )  + \sum_{i=0}^{k-1} P(i|D) 
\end{align*}
Similarly
\[ P[s \in S_m|D \cup 0] = q  P(k | D )  + \sum^{k-1} _{i=0}P(i|D) \]

From here, the $(\epsilon,\delta)$-\textbf{differential privacy} condition fulfills when
\begin{align}
P[s \in S_m|D \cup 0] \le e^{\epsilon}P[s \in S_m|D \cup 1] + \delta   \\
p  P(k | D )  + \sum_{i=0}^{k-1} P(i|D) - e^\epsilon \left ( q  P(k | D )  + \sum^{k-1} _{i=0}P(i|D) \right ) \le \delta \\
(p - qe^\epsilon)P(k | D ) - (e^\epsilon - 1) \sum^{k-1} _{i=0}P(i|D) \le \delta \\
(2pe^\epsilon - 1)P(k | D ) - (e^\epsilon - 1) \sum^{k} _{i=0}P(i|D) \le \delta
\end{align}
\end{pf}
\end{lem}

The formula \ref{eq:dp_bin} reveals the very nature of  $(\epsilon,\delta)$-protection for the bit reporting. In essence, it's a difference in PDF and CDF of the underling distribution of $s$.  The left tail probabilities grow as $s$ increases, but so does the cumulative sum of them.  At some point CDF becomes greater than the probability at given $s$, and then all consequent values of $s$ are all pair-wise indistinguishable.  As long as this difference stays under $\delta$, the privacy is preserved.

Despite its simple form, the expression \ref{eq:dp_bin} does not immediately provide a simple way to express $q$ from  $(\epsilon,\delta)$ and the size of the dataset.  A theorem below makes possible to bound $R(s)$ of any distribution of the form  \ref{lem:bpd} with the $R(s)$ of the dataset containing only 0 bits.

\begin{prop} \label{lem:rs3}
Denote a collections of $n$ bits containing $m$ set bits and $n-m$ zero bits as $D_m$. Further denote the corresponding quantities:
\begin{itemize}
\item privacy loss ratio at a particular value $s$ as $R(s|D_m) = \frac{P(s|D_m \cup 0)}{P(s | D_m \cup 1)}$
\item probability ratio at a particular value $s$ as $\rho(s|D_m) = \frac{P(s|D_m)}{P(s-1|D_m)}$
\item expected value of $s$ as $\mu_m = p \cdot m + q \cdot (n-m )$
\end{itemize}
Choose a distance $l$ such that $l \ge npq$, then 
\[ \rho[\mu_m - l|D_r] \le \rho[\mu_0 - (l+2)|D_0] \]
That is, the probability ratio for any collection is bound by the probability ratio of the zero collection. \begin{pf}
PROOF Is INVOLVED AND WILL BE GIVEN LATER IN APPENDIX.  Max needs to fix Dave's notations, skipping for now.
\end{pf}
\end{prop}

From lemma \eqref{lem:rs3} and proposition \ref{lem:rs3} we immediately receive the corollary below that bounds the left tail of the distribution \ref{lem:bpd}
\begin{cor}\label{cor:rs3}
For left deviations $l \ge npq$ from the mean the privacy loss ratio for the collection of $n$ bits is bounded by the privacy loss ratio for the collection of $n$ zero bits
\[ R[\mu_m- l|D_m] \le R[\mu_0 - (l+2)|D_0] \]
\end{cor}


We present the properties of a zero valued dataset below along with derivation of the $(\epsilon,\delta)$-bound for such collection, and a formula  to compute the the flipping frequency $q$. We then apply corollary \ref{cor:rs3} to bound an arbitrary set of bits. 

\subsection{properties of zero valued collection}
Let $D$ consists of $n$ zero bits, the neighboring dataset $D'$ is achieved by replacing a zero bit with set bit. The outcome of applying the algorithm $M$ is a sum of randomized bits $s$. The following relationships hold.

\begin{align}
\mu = E(s) = q \cdot n \\
P(s=i | D ) = \binom{n}{i}q^ip^{n-i} \\
P(s=i | D' ) = \binom{n-1}{i}q^{i+1}p^{n-1-i} +   \binom{n-1}{i-1}q^{i-1}p^{n-i+1}  \\
R(i)  = \frac{P(s=i | D)}{P(s=i | D')} = \frac{ \binom{n}{i}q^ip^{n-i} }{  \binom{n-1}{i}q^{i+1}p^{n-1-i} +   \binom{n-1}{i-1}q^{i-1}p^{n-i+1} } \\
R(i) =  \frac{1} { \frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q} }
\end{align}

By applying Chernoff bound to the distribution of $s$, we receive
 \begin{align}
P(|s - \mu| > t\mu) \le 2 e^{- \frac{t^2\mu}{3}}
\end{align}

Setting $t=\sqrt{\frac{3}{\mu} ln\frac{2}{\delta}}$ one arrives to
 \begin{align}
P \left (|s - \mu| >\sqrt{3nq \cdot ln\frac{2}{\delta}} \right ) \le \delta
\end{align}
 
Setting $l=\sqrt{3nq \cdot ln\frac{2}{\delta}}$, one is ensured that values of $P(s \in [\mu - l, \mu + l ]) \ge 1- \delta$. Conditioned on  $s \in [\mu - l, \mu + l ]$ we  bound the privacy loss ratio $R(i)$ in this interval in the following way:
\begin{align}
  && e^\epsilon \ge R(i) \ge e^{-\epsilon} \\ 
 \implies &&  e^{-\epsilon} \le \frac{1}{R(i)} \le e^\epsilon \\
  \implies &&  e^{-\epsilon} \le \frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q} \le e^\epsilon
\end{align}

We first bound the left side of the inequality, setting $i = \mu - l$
\begin{align}
\frac{n-i}{n}\frac{q}{p} + \frac{i}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{n-(\mu-l)}{n}\frac{q}{p} + \frac{\mu-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{n-(nq-l)}{n}\frac{q}{p} + \frac{nq-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
\frac{np + l}{n}\frac{q}{p} + \frac{nq-l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
q + \frac{l}{n}\frac{q}{p} +  p - \frac{l}{n} \frac{p}{q} \ge e^{-\epsilon} \\
1 - \frac{l}{n} \frac{p-q}{pq}  \ge e^{-\epsilon}. \\
l \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q}  
\end{align}

Plugging expression for $l$ one arrives to the bound of $q$
\begin{align}
\sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q} \\
 \frac {(p - q)^2} {q \cdot p^2} \le   \frac { n \left [ 1 - e^{-\epsilon}\right ]^2 } { 3  ln\frac{2}{\delta}}  \\
 \text{ since }  \frac {(p - q)^2} { p^2} \le 1 \text{ , then }  \\
  \frac {(p - q)^2} {q \cdot p^2} \le \frac{1}{q}  \le   \frac { n \left [ 1 - e^{-\epsilon}\right ]^2 } { 3  ln\frac{2}{\delta}}  \\
  q \ge  \frac { 3  ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon}\right ]^2 } \label{lem:rs201}
\end{align}

In a similar fashion, one arrives to the right side bound
\begin{align}
i = \mu + l \\
\frac{l}{n} \frac{p-q}{pq}  \le e^\epsilon - 1 \\
\sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ e^\epsilon - 1  \right ] \frac { npq}{p - q} \\
  q \ge  \frac { 3  ln\frac{2}{\delta}} { n \left [ e^{\epsilon} - 1\right ]^2 }  \label{lem:rs202}
\end{align}


Since $e^{\epsilon} - 1 \ge 1 - e^{-\epsilon}$, if $q$ bound  \eqref{lem:rs201} is met, then \eqref{lem:rs202} is also met, which leads to the following lemma

 \begin{lem} \label{lem:rs203}
The shuffling algorithm $M$ on a collection of $n$ zero bits is $(\epsilon,\delta)$-\textbf{differentially private} when the flipping frequency $q$ of the randomization procedure $\cR$ satisfy \eqref{lem:rs201}.
 \begin{align*}
  q \ge  \frac { 3 \cdot ln\frac{2}{\delta}} { n \left [ 1 - e^{-\epsilon}\right ]^2 }
  \end{align*}
\end{lem}

\subsection{Properties of an arbitrary distribution of the form \ref{lem:bpd}}

\subsubsection{Symmetry}

Consider a zero collection of $D_0$ of n zero bits, its distribution is  binomial $s \sim Bin(n,q)$.  The distribution for $D_n$ - a collection of $n$ set bits, is also binomial $s \sim Bin(n,p)$. These two distributions are mirror images of each other, which follows directly from the binomial probabilities for each collection, hence the corollary.
 \begin{cor}\label{cor:binsym}
 A binomial distribution with success probability $q$ is a mirror image of a binomial distribution with success probability $p=1-q$
 \begin{pf}
 \begin{align*}
& P(s=i | D_0) = \binom{n}{i} q^ip^{n-i} \\
&  P(s=n-i | D_n) = \binom{n}{n-i} p^{n-i} q^i \\
&  \binom{n}{n-i} = \binom{n}{i}  \\
\implies & P(s=i | D_0) = P(s=n-i | D_n)
 \end{align*}
 \end{pf}
 \end{cor}
 
 This property extends to each $D_m$ per the corollary  below.
 \begin{cor}\label{cor:sym}
Each distribution $D_m$, where $m \le n/2$, has a symmetrical mirror distribution $D_{n-m}$. Where probabilities are related as below.
\[ P(s=i | D_m) = P(s=n-i | D_{n-m}) \]
\begin{pf}
Split $D_m$ into two sets $r$ (which contains $m$ set bits), and $z$ (which contains $n-m$ zero bits).  The probability $P(s=i | D_m)$ can be written as a sum of conditional probabilities of generating certain number of success from $r$ snd $z$.
 \begin{align*}
 P(s=i | D_m) =  \sum_{j=0}^{i} P(s=j | r) P(s = i-j | z)
 \end{align*}
 The collection $D_{n-m}$ again contains two sets - $r'$ with $m$ zero bits, and $z'$ with $n-m$ set bits, which sets have mirror distributions of $r$ snd $z$. Hence
  \begin{align*}
 & P(s=n - i | D_{n-m}) =  \sum_{j=0}^{i} P(s= m - j | r') P(s = n - m - (i-j) | z') \\
 & P(s=j | r) = P(s= m - j | r')  \\
 & P(s = i-j | z) = P(s = n - m - (i-j) | z') \\
 \implies &   \sum_{j=0}^{i} P(s=j | r) P(s = i-j | z) = \sum_{j=0}^{i} P(s= m - j | r') P(s = n - m - (i-j) | z') \\
 \implies & P(s=i | D_m)  = P(s=n - i | D_{n-m})
 \end{align*}
\end{pf}
\end{cor}


\subsubsection{Chernoff bounds of an arbitrary distribution }

This section proves that the Chernoff bound for zero valued collection is also valid to an arbitrary collection of bits.  That is, for any collection of $n$ bits containing $m$ set bits the following holds.
\[ P(|s - \mu_m| > t\mu_m) \le 2 e^{- \frac{t^2\mu_0}{3}}  = 2 e^{- \frac{t^2nq}{3}} \]

\begin{prop} \label{lem:zleft}
Chernoff right tail bound for the distribution $P(s=i|D_0)$ holds for any distribution $P(s=i|D_m)$
\begin{pf}
Recall that the right distribution tail is bounded by its moment generating function, hence for any $D_m$
 \begin{align}
& P(s  > (1+\alpha)\mu_m | D_m)  \le \frac{\E(e^{ts}|D_m)}{e^{t(1+\alpha)\mu_m}} \\
 & \E(e^{ts}|D_m) = (q +pe^{t})^m(p+qe^{t})^{n-m} \\
 \implies & P(s  > (1+\alpha)\mu_m | D_m)  \le \frac{ (q +pe^{t})^m(p+qe^{t})^{n-m}}{e^{t(1+\alpha)\mu_m}}. \label{eq:cbd}
 \end{align}
 Taking the ratio of the right side of the equality for $D_0$ and $D_m$
 \begin{align}
  \frac{ e^{-t(1+\alpha)\mu_0} (p+qe^{t})^n }{ e^{-t(1+\alpha)\mu_m} (q +pe^{t})^m(p+qe^{t})^{n-m} } & = \\
  e^{t(1+\alpha)(\mu_m - \mu_0)} \left ( \frac{p+qe^{t} } { q+pe^{t} } \right )^m
\end{align}
 
 Chernoff right bound is obtained by bounding \ref{eq:cbd} at $t=ln(1+\alpha)$ , at which value the ratio above resolves to
  \begin{align*}
 (1+\alpha)e^{(1+\alpha)(\mu_m - \mu_0)} \left ( \frac{p+q(1+\alpha) } { q+p(1+\alpha) } \right )^m & = \\
 (1+\alpha)e^{(1+\alpha)m(p - q)} \left ( \frac{1+q\alpha } { 1+ p\alpha} \right )^m  & = \\
  (1+\alpha) \left ( e^{1+\alpha} e^{p-q} \frac{1+q\alpha } { 1+ p\alpha} \right )^m
 \end{align*}
 
Note that since $\alpha > 0$, $p>q$ and $p+q=1$, the expression in parenthesis is always greater than $e$.
  \begin{align*}
 & e^{p-q} > 1 \\
 & \frac{1+q\alpha } { 1+ p\alpha}  \ge \frac{1} { 1+ \alpha}  \\
 \implies & e^{1+\alpha} e^{p-q} \frac{1+q\alpha } { 1+ p\alpha}  > \frac{ e^{1+\alpha} } { 1 + \alpha}  > e \\
 \implies  & (1+\alpha) \left ( e^{1+\alpha} e^{p-q} \frac{1+q\alpha } { 1+ p\alpha} \right )^m > 1 \\
  \implies & \frac{\E(e^{ts}|D_0)}{e^{t(1+\alpha)\mu_0}} > \frac{\E(e^{ts}|D_m)}{e^{t(1+\alpha)\mu_m}}
  \end{align*}
  
 Hence,  the Chernoff right tail bound for the distribution $D_0$ also bounds right tail distribution of any $D_m$.
\end{pf}
\end{prop}

\begin{prop} \label{lem:zright}
Chernoff left tail bound of $D_n$ holds for any $D_m$
\begin{pf}
 The left tail of $D_n$ distribution is the right tail of $D_0$ distribution.  Should there exists $D_m$ which left tail not bound by the Chernoff bound of $D_n$, then, by corollary \ref{cor:sym},  there exists a distribution $D_{n-m}$ which right tail is not bound by the Chernoff bound of $D_0$, which contradicts proposition \ref{lem:zleft}
\end{pf}
\end{prop}

Since distributions of $D_0$ and $D_n$ are symmetrical the symmetrical bound of $D_0$ also applies to $D_n$.  Then by two propositions above, both tails are bound by either $D_0$ or $D_n$ bounds, which finally gives the desired theorem. 

\begin{prop} \label{lem:bound}
Any distribution $D_m$ is bounded by the Chernoff bound of $D_0$
 \begin{align}
P(|s - \mu_m| > t\mu_m) \le 2 e^{- \frac{t^2\mu_0}{3}}  = 2 e^{- \frac{t^2nq}{3}} \label{eq:bound}
\end{align}
\end{prop}

An obvious consequence of \ref{eq:bound} is  
\begin{align}
P(|s - \mu_m| > t\mu_0) \le  2 e^{- \frac{t^2nq}{3}} \text{ , because } \mu_0 < \mu_n \label{eq:zbound}
\end{align} 


\subsubsection{Bounding privacy loss for an arbitrary distribution}

Due to the distributional symmetry (corollary \ref{cor:sym}), the reciprocal of the privacy loss ratio $\frac{1}{R(s)}$ behaves like a mirror-image of $R(s)$.  It monotonically grows with $s$, reaches maximum at $n$, and the corollary \ref{cor:rs3} holds for collection $D_n$ with respect to $\frac{1}{R(s)}$.  
\begin{cor} \label{cor:rtail}
For the right deviations $l \ge npq$ from the mean the reciprocal of privacy loss ratio for the collection of $n$ bits is bounded by the reciprocal of the privacy loss ratio for the collection of $n$ set bits
\[ \frac{1}{R[\mu_m + l|D_m]} \le \frac{1}{R[\mu_n + (l+2)|D_n]} \]
\end{cor}

We now ready to prove the main result of this section

\begin{prop} \label{lem:bound}
The algorithm M is $(\epsilon,\delta)$-differentially private on a set of any $n$ bits if the flipping frequency $q$ obeys the following bound:
\begin{align}
 q \ge \frac  { 3  ln\frac{2}{\delta}}  { n \left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{n \left [ 1 - e^{-\epsilon}\right ] }
\end{align}
\begin{pf}
The privacy loss ratio of $D_0$ is symmetrical to its reciprocal in $D_m$
 \begin{align*}
  & P(s=i| D_0) = P(s = n-i | D_n) \\
  & R( | D_0) = \frac{P(i| D_0)} { P(i-1|D_0)} = \frac{P(n-i | D_0)} { P(n - (i-1)|D_0)} = \frac{1}{R(n-i | D_m)}
  \end{align*}

Hence, the upper bound of $R(s|D_0)$ is also an upper bound of $\frac{1}{R(s=i | D_m)}$.  By corollaries \ref{cor:rs3} and  \ref{cor:rtail}:
 \begin{align*}
& R[\mu_m - l|D_m]  \le R[\mu_0 - (l+2)|D_0]  \\
\& &  \frac{1}{R[\mu_m + l|D_m]} \le \frac{1}{R[\mu_n + (l+2)|D_n]} = R(s - (l+2) | D_0) \\
\implies &  R(\mu_m - l | D_m) \le e^{\epsilon}  \\
\& &  \frac{1} {R[\mu_m + l|D_m]} \le e^\epsilon  \text{ , if }   R(\mu_0 - (l+2) | D_0)  \le e^\epsilon
  \end{align*}

In other words, privacy loss at $R(\mu_0 - (l+2) | D_0)$ bounds the privacy loss ratio and its reciprocal on the interval $[\mu_m - l, \mu_m + l ]$ for any $D_m$. 

Now, recall that each distribution of $D_m$ is bounded by \ref{eq:zbound}
\[ P(|s_m - \mu_m| > tnq) \le 2 e^{- \frac{t^2nq}{3}} \label{eq:bound} \]
Setting $t=\sqrt{\frac{3}{nq} ln\frac{2}{\delta}}$ one arrives to
 \begin{align}
P \left (|s_m - \mu_m| >\sqrt{3nq \cdot ln\frac{2}{\delta}} \right ) \le \delta
\end{align}

Which ensures that the number of successes $s_m$ generated by a collection $D_m$ falls into the interval $[\mu_m - l, \mu_m + l ]$  with probability $1-\delta$, where $l=\sqrt{3nq \cdot ln\frac{2}{\delta}}$

\[ P(s \in [\mu_m - l, \mu_m + l ]) \ge 1- \delta\]

Following exact same steps that proved lemma \ref{lem:rs203}, one receives after trivial manipulation:
 \begin{align}
R(\mu_0 - (l+2) | D_0)  \le e^\epsilon \\
l + 2 \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q}  \\
\sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ 1 - e^{-\epsilon}\right ] \frac { npq}{p - q} - 2 \\
 \text{ since } \frac{p-q}{pq}  = \frac{1}{q} - \frac{1}{p} < \frac{1}{q} \\
  q< \frac {pq}{p - q}  \\
 \sqrt{3nq \cdot ln\frac{2}{\delta}} \le \left [ 1 - e^{-\epsilon}\right ] nq - 2 \\
3nq \cdot ln\frac{2}{\delta}  \le \left [ \left [ 1 - e^{-\epsilon}\right ] nq \right ]^2 - 4 \left [ 1 - e^{-\epsilon}\right ]nq\\
 q \ge \frac  { 3  ln\frac{2}{\delta}}  { n \left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{n \left [ 1 - e^{-\epsilon}\right ] } \label{eq:binq}
\end{align}
\end{pf}
\end{prop}

\subsection{Sensitivity}


One extracts estimate of the number of set bits among $n$ users as follows.  Should there be $m$ set bits in the collection, then
\begin{align*}
& \E(s) = pm + (n-m)q \\
& \E(s) = m(p-q) + nq \\
\implies & \bar{m} = \frac{s - nq}{p-q}
\end{align*}

Note that variance of the above estimate $\bar{m}$ is
\[ VAR(\bar{m}) = \frac{VAR(s)}{ (p-q)^2} = \frac{npq}{(p-q)^2}\]

Using expression \ref{eq:binq} for the flip frequency one receives.
\begin{align}
VAR(\bar{m}) = \left [ \frac  { 3  ln\frac{2}{\delta}}  {\left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{\left [ 1 - e^{-\epsilon}\right ] }  \right ] \frac{p}{(p-q)^2}
\end{align}

Fro large $n$ the corresponding $q$ becomes very small, which makes $ \frac{p}{(p-q)^2}$ approach 1, which in turn makes the variance independent of $n$.
Of corse, most applications compute a proportion of $m/n$, which reduces deviation significantly
\begin{align}
VAR(\bar{\frac{m}{n}}) = \left [ \frac  { 3  ln\frac{2}{\delta}}  {\left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{\left [ 1 - e^{-\epsilon}\right ] }  \right ] \frac{p}{n^2(p-q)^2} \\
\sigma(\bar{\frac{m}{n}}) = \frac{\sqrt{p}}{n(p-q)} \sqrt{ \frac  { 3  ln\frac{2}{\delta}}  {\left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{\left [ 1 - e^{-\epsilon}\right ] }} \label{eq:bitdev}
\end{align}

Most commonly the deviation of a ratio estimate reduces proportional to $\frac{1}{\sqrt{n}}$.  This method allows to drive down deviation proportional to $1/n$, allowing to receive meaningful results even for small number of participants.

\subsection{fake records protocol}

One obvious limitation revealed by \ref{eq:bitdev} is that for low $n$, the flipping frequency computed by \ref{eq:binq} could be high enough to make $\frac{\sqrt{p}}{(p-q)}$ significant.   We can reduce $\frac{\sqrt{p}}{(p-q)}$  to 1 without by letting users report randomized fake records along with their true records.  Suppose $k$ users randomize a fake zero bit and send it to a shuffler along with their randomized true bits.  Then the collection of the original bits contains $m$ set bits and $n+k - m$ zero bits.  The flipping frequency $q$ is computed from:
\[ q = \frac  { 3  ln\frac{2}{\delta}}  { (n+k) \left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{(n+k) \left [ 1 - e^{-\epsilon}\right ] } \]

Hence, $q$ can be made arbitrary small by increasing $k$, which in turn reduces $\frac{\sqrt{p}}{(p-q)}$  to 1.   On the other hand, the estimate of $m$ is now:
\begin{align*}
& \E(s) = pm + (n+k-m)q \\
& \E(s) = m(p-q) + (n+k)q \\
\implies & \bar{m} = \frac{s - (n+k)q}{p-q}
\end{align*}

The deviation of the estimate is
\begin{align}
& \sigma(\bar{m}) =  \sqrt { \frac{VAR(s)}{ (p-q)^2}} =  \sqrt {\frac{(n+k)pq}{ (p-q)^2} } \\
= &  \frac{\sqrt{p}}{p-q} \left [ \frac  { 3  ln\frac{2}{\delta}}  {\left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{\left [ 1 - e^{-\epsilon}\right ] }  \right ] \label{eq:bins}
\end{align}

Choosing $k$ large enough eliminates the leading term  $\frac{\sqrt{p}}{p-q}$, thus reducing deviation without loss of privacy. This is an interesting property of the fake noise, it enables to increase measurement precision for same privacy settings for the expense of sending move volume to the shuffler.  Of corse, adding volume to the mix-net traffic is not free, at at some point adding fake noise becomes infeasible.  However, for small number of users, it provides desired precision increase. This useful property will be exploited in the following sections that present industrial strength shuffling protocol for indicator vectors. 

\section{Clear Reports Protocol}

We now turn to mix-net model protocols suitable for indicator vectors. The first and the simplest protocol of the kind is when users report their indicator vectors in clear, but some fraction of users also send randomly generated indicator vectors to the shuffler. 

Assume that the data comes from a universe $\cX = [d]$ of $d$ elements. Each individual $i \in [n]$ of $n$ users has a data element $x_i\in \cX$ . We will write a data entry in bold $\bbx_i \in \{0, 1\}^d $ to be the one-hot vector where $x_i$ is zero in every position except position $\bbx_i \in \cX$ , where it is one. Furthermore, we will denote a dataset $\bbx = \{x_1,\dots ,x_n\}$ to be a collection of all users' one-hot vectors. We will have each user donate his data $\bbx_i$. Further, we will inject some fake reports $z_j\in \cX$ for $j \in [m]$, and corresponding one-hot vector notation $\bbz_j$, where each data entry is chosen uniformly at random from $\cX$. We then pass $\{\bbx_i : i \in [n]\}$ and $\{\bbz_j : j \in [m]\} $ to an anonymizer that shuffles the data and makes it impossible to determine whether a data record is real or fake. We call this algorithm 
\[
M(\bbx_1,  \dots , \bbx_n) = \pi (\bbx_1, \dots , \bbx_n, \bbz_1, \dots , \bbz_m) \text{ where } \pi \text{ permutes its elements}. 
\]
We then compute the privacy loss of such an algorithm $M$. Equivalently, we could write the output as a histogram over the entire database, as in $M(\bbx_1,  \dots , \bbx_n) = \sum^n_{i=1} \bbx_i + \sum^m_{j=1} \bbz_j$. Note that rather than inject random noise to these counts, as in central differential privacy, we want to consider \emph{anonymized differential privacy}, where data records are transmitted through a mix net to break any identifiers with each data entry and the server sees the aggregated records in some random order. In this model, there is no trusted server that injects noise to ensure DP. Rather, the user needs to only trust the anonymizer to shuffle real and fake records.

We then consider the privacy loss for a general mechanism $M$. Consider an outcome $h \in \N^d$, which is a histogram over the full dataset domain and neighboring datasets $\bbx$ and $\bbx'$.

\begin{align}
L(h) = \log \left ( \frac{\Pr[M(\bbx) = h]}{\Pr[M(\bbx') = h]} \right )
\end{align}

If we can bound $L(h)$ by $\epsilon$ for any outcome $h$ then we say that $M$ is $\epsilon$-DP. If we can bound $L(h)$ by $\epsilon$ with probability at least $1 - \delta$ where the randomness is over $h \sim M(\bbx)$, then we say that $M$ is $(\epsilon, \delta)$-DP.

We now focus on $M$ being the mechanism described above, which injects $m$ fake reports. We can then write the privacy loss in the following way where we assume, without loss of generality, that $\bbx$ and $\bbx'$ only differ in the first record, i.e. $\bbx_i = \bbx'_i$ for all $i \ne 1$.

\begin{align*}
L(h) &= \log \left ( \frac{\Pr[x_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]}{\Pr[x'_1 + \sum^n_{i=2} \bbx_i + \sum^m_{j=1} \bbz_j = h]} \right ) \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \bbx_1 - \sum^n_{i=2} \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h - \bbx'_1 - \sum^n_{i=2} x_i ]} \right )  \\
&=  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = h - \sum^n_{i=1}  \bbx_i ]}{\Pr[ \sum^m_{j=1} \bbz_j = h  - \sum^n_{i=1} \bbx_i  - (\bbx'_1 - \bbx_1) ]} \right )
\end{align*}

We denote  $\hat{h}$ to be the histogram of the fake records only $\hat{h} = h - \sum^n_{i=1} \bbx_i$, with respective counts in each histogram bin $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_d\}$.  Then the privacy loss ratio can be written as:

\begin{align*}
L(h) =  \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h}  + \bbx_1 - \bbx'_1) ]} \right )
\end{align*}


The one-hot vectors $\bbx_1$  and $\bbx'_1$ may only differ in two positions, let these positions be $\ell$ and $\ell'$. 
$\bbx_1$ and $\bbx'_1$ must have opposite bit-values in positions $i$ and $i'$ (otherwise these vectors are identical). 
Without loss of generality assume $x_{1,\ell} = 1, x_{1,\ell'} = 0$ and $x'_{1,\ell} = 0, x'_{1,\ell'} = 1$.  
Adding $\bbx_1$ adds 1 to $h_i$, while subtracting $\bbx'_1$ removes 1 from $h_\ell'$.
Hence,  if $\hat{h} = \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d\} $, then  $\hat{h} + \bbx_1 -\bbx'_1 =  \{ \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} -1, \dots, \hat{h}_d\} $.

Further, note that the count the fake bits $\hat{h}_\ell = \sum^m_{j=1} \bbz_{j,\ell}$ is a binomial distribution $h_\ell \sim \text{Bin}(m, 1/d)$, and the distribution of the fake bit counts across the bins takes the multinomial form $ \hat{h} \sim \text{Multinomial}(m,(1/d,\cdots, 1/d))$. We then aim to bound the following quantity.

\begin{align*}
 L(h) &= \log \left ( \frac{\Pr[\sum^m_{j=1} \bbz_j = \hat{h} ]}{\Pr[ \sum^m_{j=1} \bbz_j = \hat{h} + \bbx_1 - \bbx'_1]} \right ) \\
 & = \log  \left ( \frac{ {m \choose \hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell, \dots, \hat{h}_{\ell'}, \dots, \hat{h}_d}}{ { m \choose hat{h}_1, \hat{h}_2, \dots , \hat{h}_\ell + 1, \dots, \hat{h}_{\ell'} - 1, \dots, \hat{h}_d}} \right ) \\
 & = \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) 
\end{align*}

It must be stressed that for a given pair of $(\bbx_1, \bbx'_1)$, the corresponding position pair $(\ell,\ell')$ where their bits are different is fixed, and the privacy loss only surfaces while observing the counts in the corresponding histogram bins $(h_\ell, h_{\ell'})$.  It's entirely possible to see high ratio between counts in some other histogram bins, but it wouldn't contribute to the privacy loss for a concrete pair $(\bbx_1, \bbx'_1)$.  This observation allows us to focus only on a single pair of the histogram bins, ignoring the rest of the histogram as immaterial.  

By applying a Chernoff bound, we have a bound (symmetric for the upper and lower tail) for the sum of the fake bits in any bin $\hat{h}_k = \sum^m_{j=1} z_{j,k},  k \in [d]$

\begin{align*}
 \Pr \left [ \left | \hat{h}_k - \frac{m}{d} \right | > t \frac{m}{d} \right ]  \le 2 e^{- \frac{m}{d} \frac{t^2}{3}}, \qquad \text{ for  } 0 < t < 1.
\end{align*}

Choose $t$ to fit the expression below, hence $t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }$.
%\begin{align}
%2e^{- \frac{m}{d} \frac{t^2}{3}} = \frac{\delta}{2}  \\
%- \frac{m}{d} \frac{t^2}{3} = \log{\frac{\delta}{4}}  \\
% \frac{m}{d} \frac{t^2}{3} = \log{\frac{4}{\delta}} \\
% t = \sqrt{  \frac{3d}{m} \log{\frac{4}{\delta}} }
%\end{align}
Using this expression for $t$ turns our Chernoff bound into the following,
 
 \begin{align}
 Pr \left [ \left | \bar{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \frac{\delta}{2}
\end{align}

Given any pair of the histogram bins at positions $(\ell,\ell')$, the probability of observing large deviation from the mean in at least one bin obeys the unions bound.
\[
\Pr \left [ \max_{k \in (\ell,\ell')} \left | \hat{h}_k - \frac{m}{d} \right | >  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}} } \right ]  \le \delta
\]

We then condition on the event that both counts $\hat{h}_l$ or $\hat{h}_{l'}$ fall in the interval $m/d \pm \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}$, which event occurs with probability at least $1 - \delta$.  
Conditioned on there being the given number of fake records, we can upper bound the privacy ratio $L(h)$

 \begin{align}
 L(h) =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \le \log  \left ( \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \le \epsilon 
\end{align}

From the above, we then get a condition on the number of fake records, m, to ensure DP.
 \begin{align*}
& \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^\epsilon  \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) - 1 \ge 0 \\
\implies & \frac{m}{d} (e^\epsilon - 1) -  \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} (e^\epsilon +1) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} \left ( \sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1) \right ) \ge 0 \\
\implies &\sqrt{  \frac{m}{d}} (e^\epsilon - 1)  - \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  \ge 0 \\
\implies &\sqrt{  \frac{m}{d}}  \ge  \frac { \sqrt{  3 \log{\frac{4}{\delta}}} (e^\epsilon +1)  } { e^\epsilon - 1 } \\
\implies & \frac{m}{d}  \ge 3\log {\frac{4}{\delta}} \left ( \frac{e^\epsilon +1}{e^\epsilon - 1} \right )^2
\end{align*}

As for the lower bound of $L(h)$, it's met if the upper bound is met.
 \begin{align*}
 L(h) & =  \log  \left ( \frac{\hat{h}_\ell + 1}{\hat{h}_{\ell'}} \right ) \ge \log  \left ( \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} } \right ) \ge - \epsilon  \\
\implies &  \frac{ m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \ge e^{-\epsilon} \\
 \implies &  \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} \le e^{\epsilon}  \\
\implies &   \frac{  m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}}  { m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1} < \frac{ m/d + \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}} + 1}{  m/d - \sqrt{  \frac{3m}{d} \log{\frac{4}{\delta}}}} \le e^{\epsilon} 
\end{align*}

An obvious disadvantage of this protocol is its infeasibility for large $d$. If vector dimensions is many thousands, sending millions of fake records through the mix-net becomes prohibitively expensive.  Which problem can be resolved by adding bit flipping to both true and fake records.

%%%%%%%%%%%%%%%%%%% - but flipping with fake records

\section{Fake records and bit flipping}

We now apply the exact same protocol with a bit flipping twist. Users produce $n$ real and $m$ fake reports, but flips bits of either 1-hot vector frequency $q$ before sending to the shuffler.  More formally, a randomization procedure $\cR(y)$ flips each bit of an arbitrary 1-hot-vector $y$ with probability $q$ and keeps it the same with probability $p=1-q$.  The resulting mechanism $M_r$ becomes a permutation of randomized true and fake records:
\[
M_r(\bbx_1,  \dots , \bbx_n) = \pi (\cR(\bbx_1), \dots , \cR(\bbx_n), \cR(\bbz_1), \dots , \cR(\bbz_m)) \text{ where } \pi \text{ permutes its elements}. 
\]

Without loss of generality assume $\bbx_1$ is replaced with $\bbx'_1$ to receive a neighboring data set $\bbx'$.  The outcome is a histogram $g \in \N^d$ containing sums of randomized bits in each dimension, and the privacy loss:

\begin{equation} \label{eq:lgbound}
L(g) = \log \left ( \frac{\Pr[M_r(\bbx) = g]}{\Pr[M_r(\bbx') = g]} \right )
\end{equation}

The combined set $\bbx + \bbz$ gives raise to a histogram $h \in \N^d$ received by applying the before discussed mechanism M (clear true records plus fake records).  Hence, the $Pr [ M_r(x) = g ]$ can be written as a sum of probabilities over the domain of $h$:

 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ]  \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   }{    \sum_{h' \in \N^d}  Pr \left [  M(\bbx') = h' \right ] \cdot   Pr[ g | h']   } \right )
\end{align*}

Noting that
\[
 Pr \left [  M(\bbx) = h \right ]  =  Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]
 \] 
 
And regrouping  the privacy loss ratio to have summands with same $Pr \left [  M(\bbx) = h \right ]$ in identical positions in numerator and denominator, and applying \eqref{lem:rsbound} we have:
 \begin{align*}
& \log \left ( \max_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \text{ , and } \\
& L(g) \ge  log \left ( \min_{h \in \N^d} \left ( \frac{  Pr \left [  M(\bbx) = h \right ] \cdot  Pr [ g | h ]   } {   Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ] \cdot  Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right ) \\
\end{align*}

Probabilities $Pr \left [  M(\bbx) = h \right ]$ and $ Pr \left [  M(\bbx') = h - \bbx_1 + \bbx'_1 \right ]$ cancel each other out in each ratio, hence giving us the bounds of the privacy loss over domain of $h$ .

\begin{align*}
\log \left ( \max_{h \in \N^d} \left ( \frac{  Pr [ g | h ]   } { Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }    \right ) \right ) \ge L(g) \ge  log \left ( \min_{h \in \N^d}  \left ( \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  }  \right ) \right )
\end{align*}

Since bits are flipped independently, the probability of finding certain number of bits in a particular histogram bin $g_l$ depends only on how many not-yet-randomized set bits there are in the dimension $l$, that is the value of $h_l$.  Such independence allows to re-write $Pr [ g | h]$ as a product of probabilities for each dimension.
 \[
Pr [ M_r(x) = g ] = Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] = \prod_{i=1}^d Pr[ g_i | h_i] 
\]

Without loss of generality assume that $\bbx_1$ and $\bbx'_1$ differ in the first and second positions, that is $\bbx_{1,1} = 1, \bbx_{1,2} = 0$ and $\bbx'_{1,1} = 0, \bbx'_{1,2} = 1$, then
\begin{align*}
 & h  - \bbx_1 + \bbx'_1 = \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} \\
 \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{ Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1, {h}_2, , \dots, {h}_d\} ] } { Pr [    \{ {g}_1, {g}_2, , \dots, {g}_d\} |  \{ {h}_1 - 1, {h}_2 + 1, , \dots, {h}_d\} ]  } \\
  \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } = \frac{  Pr[ g_1 | h_1] Pr[ g_2 | h_2] \prod_{i=3}^d Pr[ g_i | h_i]  } {  Pr[ g_1 | h_1 - 1] Pr[ g_2 | h_2 + 1] \prod_{i=3}^d Pr[ g_i | h_i]  }  \\
   \implies &  \frac{  Pr [ g | h ]   } {    Pr [ g | h  - \bbx_1 + \bbx'_1  ]  } =   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  } 
\end{align*}

Plugging the above formula into \eqref{eq:lgbound}, the privacy loss bounds become:
\begin{align*}
& \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right ) \right ) \ge L(g) \ge  \min_{h \in \N^d}  \left ( \log \left (  \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }   \right ) \right ) \\
\implies &  \max_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \max_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right) \ge L(g) \text{ , and } \\
&  L(g) \ge \min_{h \in \N^d}  \left ( \log \left (   \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \right ) \right )  + \min_{h \in \N^d}  \left ( \log  \left (\frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \right) \right)
\end{align*}

Basically, the privacy loss is bound by the sum of privacy losses in each of the affected dimensions.  Which enables relatively simple path to the bound.   

\begin{prop}
Bit flipping shuffling algorithm $M$ applied to a collection of $n$ true and $m$ fake records is $(\epsilon, \delta)$-private when the bit flipping frequency obeys the bound below:
\begin{align}
q \ge \frac  { 3  ln\frac{4}{\delta}}  { (n + m) \left [ 1 - e^{-\epsilon/2}\right ] ^2}  + \frac{4}{(n + m) \left [ 1 - e^{-\epsilon/2}\right ] } 
\end{align}
\begin{pf}
We are bounding the product of privacy loss ratios in the affected dimensions to stay between $e^{-\epsilon}$ and $e^\epsilon$ with probability $1-\delta$.
\begin{align} \label{lem:fk101}
P \left (   e^\epsilon \ge \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1]  } \cdot \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \ge \frac{1}{e^\epsilon} \right ) \le 1 - \delta
\end{align}
We achieve the condition above by bounding the ratio in each dimension separately.  Suppose that the following holds
\begin{align} \label{lem:fk102}
& P \left (   e^\frac{\epsilon}{2} \ge \frac{  Pr[ g_1 | h_1]  } {  Pr[ g_1 | h_1 - 1] }  \ge \frac{1}{e^\frac{\epsilon}{2}} \right ) \le 1 - \delta/2 \\
\& & P \left (   e^\frac{\epsilon}{2} \ge \frac{  Pr[ g_2 | h_2]  } {  Pr[ g_2 | h_2 + 1]  }  \ge \frac{1}{e^\frac{\epsilon}{2}} \right ) \le 1 - \delta/2 
\end{align}

Then, by the union bound, the combined probability of either ratio falling outside its bound is $\delta$, and with probability $1-\delta$, both ratios stay between $e^{-\frac{\epsilon}{2}}$ and $e^\frac{\epsilon}{2}$, hence the product of the ratios is bounded in $[ e^{-\epsilon}, e^\epsilon]$.   

By proposition \ref{lem:bound} an algorithm $M$ that shuffles results of the bit flipping procedure  $\cR$ executed over a set of $n+m$ bits is $(\epsilon, \delta)$-private if the bit flipping frequency is chosen according to \ref{eq:binq}:
\[ q \ge \frac  { 3  ln\frac{2}{\delta}}  { (n+m) \left [ 1 - e^{-\epsilon}\right ] ^2}  + \frac{4}{(n+m) \left [ 1 - e^{-\epsilon}\right ] } \]

Plugging  $(\epsilon, \delta)$ settings needed to satisfy condition \ref{lem:fk102}, one arrives to the  bound of the flipping frequency
\[ q \ge \frac  { 3  ln\frac{4}{\delta}}  { (n + m) \left [ 1 - e^{-\epsilon/2}\right ] ^2}  + \frac{4}{(n + m) \left [ 1 - e^{-\epsilon/2}\right ] } \]
\end{pf}
\end{prop}

The above choice of $q$ introduces sensitivity that closely follows that of the binary case \ref{eq:bins}.    The estimate of the sum of randomized bits in each bucket can be extracted as follows.  Denote $r$ be a number of the fake set bits (before randomization) in each bucket. $r$ is a binomial random variable with success probability $\frac{1}{d}$.
\[ r \sim Bin(m, 1/d) \]
Assuming a number of true set bits in the same bucket is $z$ and the total sum of randomized bits is $s$, we have the expectation equation below: 
\begin{align}
& s = p(z + r)  + (n+m - r - z)q \\
& \E(s) = pz + \E(r)p + (n+m)q - \E(r)q - zq\\
& \E(s) = z(p-q) + \E(r)(p-q) + (n+m)q \\
\implies & \bar{z} = \frac{s - \frac{m}{d}(p-q) - (n+m)q}{p-q} =  \frac{s - (n+m)q}{p-q}  - \frac{m}{d} \label{eq:estimate}
\end{align}

The variance of the estimate is
\begin{align*}
VAR( \bar{z} ) =  \frac{VAR(s)}{(p-q)^2}  + VAR(r) = \frac{(n+m)pq}{(p-q)^2} + \frac{m}{d}(1-1/d)
\end{align*}


In practice, it's the percentage of overall population falling into each bucket sought.  In which case, the deviation is reduced by $n$, and the ratio per bucket estimate has deviation as below:
\begin{align}
\sigma( \frac{\bar{z}}{n} ) =  \frac{1}{n} \sqrt { \frac{p}{(p-q)^2} \left [   \frac  { 3  ln\frac{4}{\delta}}  { \left [ 1 - e^{-\epsilon/2}\right ] ^2}  + \frac{4}{ \left [ 1 - e^{-\epsilon/2}\right ] }   \right ] + \frac{m}{d}(1-1/d) } \label{eq:estimate_var}
\end{align}

Expressions \ref{eq:estimate} and \ref{eq:estimate_var} give insight on how the fake noise machinery may help.  If $d >> m$, then the term $\frac{m}{d}(1-1/d)$ becomes negligible, which reduces sensitivity to the single bit case, and the number of fake records is chosen to minimize $\frac{\sqrt{p}}{p-q}$ enough without incurring large communication cost. 

\section{Appendix: Ratios of sums: properties}

Here we establish some results around bounding and comparing ratios of sums, which will be useful in working with the privacy ratio.


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}


\section{Appendix B - proof of zero collection maximality}

It is instructive to first consider the case where each record in the collection consists of a single bit, as the expressions simplify considerably.

When $L=1$, each original and synthetic record is either 1 or 0, and the transformation $R$ flips each record with probability $q$.
Partition the collection space $\Dsp^n$ according to the number of records that are 1:
\[ \Dsp^n = \bigcup_{m = 0}^n \Dsp_m^n
\quad\quad\text{where}\quad\quad
\Dsp_m^n := \bigg\{ \xv\in\Dsp^n \,:\, \sum_{i=1}^n I(x_i = 1) = m \bigg\}.
\]
For $\xv\in\Dsp_m^n$, we have
\[ A(\xv) = \Phi\circ R(\xv) = \big(A_n(m), n - A_n(m)\big), \]
where
\begin{align*}
A_n(m) &:= \sum_{i=1}^n I(R(x_i) = 1)
= \sum_{i:\, x_i = 1} I(R(1) = 1) + \sum_{i:\, x_i = 0} I(R(0) = 1) \\
&\sim Bin(m, p) + Bin(n-m, q),
\end{align*}
a sum of two independent Binomial random variables with support $\{0,\dots,n\}$.
Furthermore, if $\xv\in\Dsp_m^n$ and $\xv,\xv'$ differ in one row, then $\xv'\in\Dsp_{m-1}^n \cup \Dsp_{m+1}^n$.
Defining
\[ \pi_n(s; m) := \frac{\P[A_n(m) = s]}{\P[A_n(m+1) = s]}
\quad\quad\text{for}\ \ 
s\in\{0,\dots,n\}\ \text{and}\ m\in\{0,\dots,n-1\},
\]
the privacy ratio becomes
\[ \pi\big((s, n-s);\,\xv,\xv'\big) =
\begin{cases}
\pi_n(s;m - 1) & x_1 = 1 \\[0.3em]
\inv{\pi_n(s;m)} & x_1 = 0
\end{cases}\, .
\]
Hence, in the $L=1$ case, it suffices to study the behaviour of $\pi_n(s;m)$.


\subsection{Recursive relationship over $n$ and $m$}

The conditioning argument \eqref{eq:prcond} yields a recursive relationship that lets us express the distribution of $A_n$ in terms of that of $A_{n-1}$.

Recall that $A_n(m)$ is the outcome of applying the bit transformation $R$ to $n$ original bits, $m$ of which are 1 and $n-m$ are 0.
For $m \geq 1$, we can condition on the outcome of one of the original 1s:
\[ A_n(m) \sim Ber(p) + Bin(m-1, p) + Bin(n-m, q) \sim Ber(p) + A_{n-1}(m-1), \]
and so
\begin{equation}\label{eq:rec1}
\P[A_n(m) = s] = p\P[A_{n-1}(m-1) = s-1] + q\P[A_{n-1}(m-1) = s].
\end{equation}
If $s = 0$, the first term on the RHS is interpreted as 0, and if $s = n$, the last term is.
Similarly, for $m \leq n-1$, conditioning on an original 0,
\[ A_n(m) \sim Ber(q) + Bin(m, p) + Bin(n-m-1, q) \sim Ber(q) + A_{n-1}(m), \]
from which
\begin{equation}\label{eq:rec0}
\P[A_n(m) = s] = q\P[A_{n-1}(m) = s-1] + p\P[A_{n-1}(m) = s].
\end{equation}

The recursive formulas \eqref{eq:rec1} and \eqref{eq:rec0} give some insight into how the distribution of $A_n(m)$ changes as $n$ and $m$ vary:
\begin{itemize}
\item  as $n$ increases by 1, the probabilities shift slightly, with $\P[A_n(m) = 0] \leq \P[A_{n-1}(m) = 0]$ and
$\P[A_n(m) = s]$ falling between $\P[A_{n-1}(m) = s-1]$ and $\P[A_{n-1}(m) = s]$ for each $s\geq 1$ (\ie the hump of the pmf shifts to the right);
\item the distribution of $A_n(m+1)$ is not so different to that of $A_n(m)$, since $\P[A_n(m) = s]$ and $\P[A_n(m+1) = s]$ both lie between consecutive pmf values of $A_{n-1}(m)$. In particular, this allows us to express the privacy ratio $\pi(s;m)$ in terms of $A_{n-1}(m)$.
\end{itemize}

Writing $P_{n,m}(s) := \P[A_n(m) = s]$, the formulas \eqref{eq:rec1} and \eqref{eq:rec0} can be expressed as
\[ P_{n,m}(s) = pP_{n-1,m-1}(s-1) + qP_{n-1,m-1}(s)
%\quad\text{for}\ \ s = 0,\dots,n,\ \ m = 1,\dots,n
\quad\text{for}\ \ 0 \leq s \leq n,\ \ 1\leq m \leq n 
\]
and
\[ P_{n,m}(s) = qP_{n-1,m}(s-1) + pP_{n-1,m}(s)
%\quad\text{for}\ \ s = 0,\dots,n,\ \ m = 0,\dots,n-1. 
\quad\text{for}\ \ 0 \leq s \leq n,\ \ 0\leq m \leq n-1.
\]

\subsection{The probability ratio}

The probabilities in the privacy ratio represent the likelihood of observing
the same synthetic collection outcome given two different original collections.
In the expression $\pi_n(s;m) = P_{n,m}(s) / P_{n,m+1}(s)$, the probabilities
correspond to the distributions of $A_n(m)$ and $A_n(m+1)$, respectively.
However, using the decomposition \eqref{eq:rec1} and \eqref{eq:rec0}, we can
rewrite $\pi_n$ in terms of probabilites from the same distribution, which is
more convenient to work with.

Applying \eqref{eq:rec0} to the numerator and \eqref{eq:rec1} to the denominator,
we obtain
\[ \pi_n(s;m) =
\frac{qP_{n-1,m}(s-1) + pP_{n-1,m}(s)}{pP_{n-1,m}(s-1) + qP_{n-1,m}(s)} =
\frac{q + p \frac{P_{n-1,m}(s)}{P_{n-1,m}(s-1)}}
    {p + q \frac{P_{n-1,m}(s)}{P_{n-1,m}(s-1)}}
\]
for $s \geq 1$, and $\pi_n(0;m) \equiv p/q$.
Define the \textbf{probability ratio}
\[ \rho_n(s;m) := \frac{P_{n,m}(s)}{P_{n,m}(s-1)}
\qquad\text{for}\ \ 1 \leq s\leq n \]
a ratio of consecutive probabilities from the distribution of $A_n(m)$, and let
$g(x) = \frac{q + px}{p + qx}$, so that $\pi_n = g \circ \rho_{n-1}$.
The function $g$ is increasing over $x>0$, since
\[ g'(x) = \frac{p-q}{(p+qx)^2} > 0. \]
Therefore, properties of monotonicity and extrema established for $\rho_n$ (for
all $n$) carry over to $\pi_n$ as well.

The probability ratio can be expressed in a concise way using the following 
recursive property of the distribution of $A_n(m)$.

\begin{lem}
For $n \geq 1$,
\begin{equation} \label{eq:recP}
(s+1) P_{n,m}(s+1) = \bigg\{ (m-s)\frac{p}{q} + (n-m-s)\frac{q}{p} \bigg\} P_{n,m}(s) + (n-s+1) P_{n,m}(s-1)
\end{equation}
for $0 \leq m \leq n$ and $0 \leq s \leq n-1$ (with $P_{n,m}(-1) := 0$).
\end{lem}
\begin{pf}
We proceed by induction on $n$.
Suppose first $n=1$, $s = 0$.
If $m=1$, then $A_1(1) \sim Ber(p)$, and \eqref{eq:recP} holds since 
$(mp/q + (1-m)q/p) \cdot P_{1,1}(0) = p = P_{1,1}(1)$.
The argument is similar when $m=0$.
Next assume \eqref{eq:recP} holds for $A_{n-1}(m)$, and suppose $m \leq n-1$ and $1 \leq s \leq n-2$.
Observe that
\begin{align*}
\bigg\{ &(m-s)\frac{p}{q} + (n-m-s)\frac{q}{p} \bigg\} P_{n,m}(s) + (n-s+1) P_{n,m}(s-1) \\
&= \bigg\{ (m-s)\frac{p}{q} + (n-1-m-s)\frac{q}{p} \bigg\} \big[qP_{n-1,m}(s-1) + pP_{n-1,m}(s)\big] \\
 &\qquad\qquad + (n-1-s+1) \big[qP_{n-1,m}(s-2) + pP_{n-1,m}(s-1)\big] +
 \frac{q}{p}P_{n,m}(s) + P_{n,m}(s-1) \\
 &= p\bigg[ \bigg\{ (m-s)\frac{p}{q} + (n-1-m-s)\frac{q}{p} \bigg\} P_{n-1,m}(s) + (n-1-s+1)P_{n-1,m}(s-1) \bigg] \\
 &\qquad + q\bigg[ \bigg\{ (m-(s-1))\frac{p}{q} + (n-1-m-(s-1))\frac{q}{p} \bigg\} P_{n-1,m}(s-1) \\
 &\hspace{20em} + (n-1-(s-1)+1)P_{n-1,m}(s-2) \bigg] \\
 &\qquad - \bigg(p + \frac{q^2}{p}\bigg) P_{n-1,m}(s-1) - qP_{n-1,m}(s-2) + \frac{q^2}{p}P_{n-1,m}(s-1) + qP_{n-1,m}(s) \\
 &\qquad + qP_{n-1,m}(s-2) + pP_{n-1,m}(s-1) \\
 &= p(s+1)P_{n-1,m}(s+1) + qsP_{n-1,m}(s) + qP_{n-1,m}(s) \\
 & = (s+1) \big[ qP_{n-1,m}(s) + pP_{n-1,m}(s+1) \big] = (s+1)P_{n,m}(s+1),
\end{align*}
applying the induction hypothesis for $s$ and for $s-1$ together with \eqref{eq:rec0}. If $s=0$, the argument is similar:
\begin{align*}
\bigg\{ m\frac{p}{q} + (n-m)\frac{q}{p} \bigg\} P_{n,m}(0)
&= p\bigg\{ m\frac{p}{q} + (n-1-m)\frac{q}{p} \bigg\} P_{n-1,m}(0) + qP_{n-1,m}(0) \\
 &= pP_{n-1,m}(1) + qP_{n-1,m}(0) = P_{n,m}(1).
\end{align*}
\end{pf}

Given $m$, the probability ratio can be expressed using \eqref{eq:recP}:
\begin{align*}
\rho(s+1;m) &= \frac{m-s}{s+1}\frac{p}{q} + \frac{n-m-s}{s+1}\frac{q}{p} + \frac{n-s+1}{s+1} \frac{1}{\rho(s;m)} \\
\rho(1;m) &= m\frac{p}{q} + (n-m)\frac{q}{p}
\end{align*}
Write
\[ \eta(s) := \frac{n-s+1}{s+1} \qquad\text{and}\qquad
\gamma_m(s) := \frac{1}{s+1} \left[(m-s)\frac{p}{q} + (n-m-s)\frac{q}{p}\right], \]
to get
\begin{equation}\label{eq:probrrec}
\rho(s+1;m) = \eta(s)\inv{\rho(s;m)} + \gamma_m(s)\,; \quad
\rho(1;m) = \gamma_m(0). 
\end{equation}

Note also that $\gamma_m(s)$ can be expressed in terms of
$\EP A_n(m) = \mu_m = nq + m(p-q)$:
\[ (s+1)\gamma_m(s) = \frac{\mu_m - s}{pq} - n + 2s. \]
%\begin{align*}
% &= \frac{(m-s)p^2 + (n-m-s)q^2}{pq}
% = \frac{m(p^2-q^2) + nq^2 - s(p^2 + q^2)}{pq} \\
% &= \frac{nq + m(p-q) - nq + nq^2 - s(1-2pq)}{pq}
% = \frac{\mu_m - s -(n - 2s)pq}{pq} \\
% &= 
%\end{align*} 

The probability ratio has the following properties (TODO):
\begin{itemize}
\item decreasing in $s$ for fixed $m$
\item increasing in $m$ for fixed $s$.
\end{itemize}

\subsection{Bounding the probability ratio}

For $A$ to satisfy local differential privacy, the privacy ratio $\pi_n(s;m)$
must be bounded for all $s$ except for a set of small probability with respect
to the distribution $\P[A_n(m) = \cdot\,]$.
Furthermore, this bound must hold regardless of the original collection
described through $m$.

Fix $\delta > 0$.
Given $m$, we show that the probability ratio for $s\in[\mu_m-\delta, n]$ 
is bounded by a value $\rho(s^*; 0)$, where $s^*$ is expressed in terms of
$\mu_0-\delta$.
Together with the fact that
$P_{n,m}(\mu_m - \delta) \leq P_{n,0}(\mu_0 - \delta)$ (TODO - is this
necessary?),
this implies that the bound for local differential privacy, required to hold
for all $m$, can be computed in terms of $A_n(0)$ alone.
Note that, since $\rho$ is decreasing in $s$ for fixed $m$, it is sufficient
to consider the probability ratio at the smallest integer value belonging to
the interval $[\mu_m-\delta, n]$.

TODO: how to handle the left endpoint. What is the min value of $\delta$?

For $\delta > 0$ let $s_m(\delta) := \lceil \mu_m - \delta \rceil \vee 0$, and define $R_m(\delta) := \rho(s_m(\delta); m)$.
Note that $R_m(\delta) \leq R_m(\delta')$ for $\delta \leq \delta'$, and
$s_m(\delta + 1) = (s_m(\delta) - 1) \vee 0$.

\begin{prop}
\[ R_m(\delta) \leq R_0(\delta + 2)\qquad\text{for}\ \ m = 0,\dots,n \]
provided $\delta > \sigma_0 + 1$, where
$\sigma_0^2 = \var A_n(0) = npq$.
\end{prop}

\begin{pf}
Fix $\delta > \sigma_0 + 1$.
(TODO)
If $s_0(\delta) < 2$

Assume $s_0(\delta) \geq 2$, and suppose $R_m(\delta) > R_0(\delta + 2)$ for
some $m$.
Then, we have
\[ R_0(\delta) \leq R_0(\delta + 1) \leq R_0(\delta + 2) < R_m(\delta) \leq
R_m(\delta + 1),\]
implying that
\[ \inv{R_0(\delta + 2)} =
\frac{R_0(\delta+1) - \gamma_0(s_0(\delta+2))}{\eta(s_0(\delta+2))} > 
\frac{R_m(\delta) - \gamma_m(s_m(\delta+1))}{\eta(s_m(\delta+1))} =
\inv{R_m(\delta + 1)} \]
via \eqref{eq:probrrec}.
Write $s_m := s_m(\delta+1)$, $s_0 := s_0(\delta + 2)$.
Since $R_m(\delta) > R_0(\delta+1)$ by assumption, we obtain:
\begin{equation}\label{eq:deltabdineq}
\big\{\eta(s_m) - \eta(s_0)\big\} R_0(\delta + 1)
 + \big\{\eta(s_0)\gamma_m(s_m) - \eta(s_m)\gamma_0(s_0)\big\} > 0.
\end{equation}
Furthermore,
\[ \eta(s_m) - \eta(s_0)
= \frac{n - s_m + 1}{s_m + 1} - \frac{n - s_0 + 1}{s_0 + 1}
= -\frac{(n + 2)(s_m - s_0)}{(s_0 + 1)(s_m + 1)}, \]
and
\begin{align*}
\eta(s_0)&\gamma_m(s_m) - \eta(s_m)\gamma_0(s_0) \\
&= \frac{n - s_0 + 1}{s_0 + 1}\cdot \frac{(\mu_m - s_m)/pq - n + 2s_m}{s_m + 1} - 
\frac{n - s_m + 1}{s_m + 1}\cdot \frac{(\mu_0-s_0)/ pq - n + 2s_0}{s_0 + 1} \\
 &= \frac{(n+2)(s_m-s_0) + (\mu_0 s_m - \mu_m s_0)/pq +
 (n+1)[\mu_m - \mu_0 - (s_m - s_0)]/pq}{(s_0 + 1)(s_m + 1)},
\end{align*}
so \eqref{eq:deltabdineq} implies
\begin{align}
-(n+2)(s_m - s_0)&(R_0(\delta + 1) - 1) + \nonumber\\
&\frac{\mu_0 (s_m - s_0) - m(p-q) s_0}{pq} +
\frac{(n+1)[m(p-q) - (s_m - s_0)]}{pq} > 0. \label{eq:deltabdineq2}
\end{align}
Now, let
$\delta_0 :=
\delta - \{\lceil \mu_0 - \delta \rceil - (\mu_0 - \delta)\} =
\mu_0 - \lceil \mu_0 - \delta \rceil$, \ie
$\delta_0 = \inf\{ \lambda : s_0(\lambda) = s_0(\delta)\}$.
Then $s_0(\delta) = s_0(\delta_0) = \mu_0 - \delta_0$, an integer, and
$s_m(\delta_0) - s_m(\delta) \in\{0, 1\}$, since
$0 \leq \delta-\delta_0 < 1$.
Consequently,
since
\[ s_m(\delta_0) - s_0(\delta_0) = \lceil \mu_0  + m(p-q) - \delta_0 \rceil -
(\mu_0 - \delta_0) = \lceil m(p-q) \rceil, \]
\begin{align*}
s_m - s_0 &= (s_m(\delta) - 1) - (s_0(\delta) - 2) =
s_m(\delta) - s_m(\delta_0) + \lceil m(p-q) \rceil + 1 \\
 &\in \big\{\lceil m(p-q) \rceil, \lceil m(p-q) \rceil + 1 \big\},
\end{align*}
and
\begin{align*}
\mu_0(s_m - s_0) - m(p-q) s_0 & = \mu_0(s_m-s_0) - m(p-q)(s_0(\delta_0) - 2) \\
 &= \mu_0[(s_m-s_0) - m(p-q)] + m(p-q)(\delta_0 + 2).
\end{align*}
Applying these identities in \eqref{eq:deltabdineq2} gives
\[ 
-(n+2)(s_m - s_0)(R_0(\delta + 1) - 1) +
\frac{m(p-q)(\delta_0 + 2)}{pq}
+ \frac{n+1-\mu_0}{pq}\big(m(p-q) - (s_m-s_0)\big) > 0.
\]
Since $s_m-s_0 \geq m(p-q)$,
\begin{equation}\label{eq:deltabdineq3}
(n+2)(R_0(\delta+1) - 1) <
\frac{\delta_0 + 2}{pq}\frac{m(p-q)}{s_m - s_0} < \frac{\delta_0 + 2}{pq}.
\end{equation}
Next, recall that
$R_0(\delta+1) = P_{n,0}(s_0(\delta+1)) / P_{n,0}(s_0(\delta+1) - 1)$.
Since $P_{n,0}(\cdot) = \P[Bin(n,q) = \cdot\,]$, 
\[ R_0(\delta+1) - 1 =
\frac{n - s_0(\delta+1) + 1}{s_0(\delta+1)}\cdot \frac{q}{p} - 1 =
\frac{\mu_0 - (\mu_0 - \delta_0 - 1) + q}{p(\mu_0 - \delta_0 - 1)} =
\frac{\delta_0 + q + 1}{p(\mu_0 - \delta_0 - 1)}.
\]
Hence, substituting this expression in \eqref{eq:deltabdineq3} yields
\begin{align*}
(\delta_0 + 2)(\mu_0 - \delta_0 - 1) &> (n+2)q(\delta_0 + q + 1) >
\mu_0(\delta_0 + q + 1) \\
\iff
-\delta_0^2 - 3\delta_0 + 2\mu_0 - 2 &>  (1+q)\mu_0 \\
\iff
-\delta_0^2 - 3\delta_0 + npq &> 0,
\end{align*}
which requires that $\delta_0$ lie between the roots of the quadratic equation.
In particular,
\[ \delta_0 \leq -\frac{3}{2} + \frac{1}{2}\sqrt{9 + 4npq} \leq
-\frac{3}{2} + \frac{3}{2} + \sqrt{npq} = \sqrt{npq}. \]
Finally, since $0 \leq \delta-\delta_0 < 1$, we conclude that
\[ \delta = \delta_0 + \delta - \delta_0 < \sigma_0 + 1, \]
contradicting our initial choice of $\delta$.
\end{pf}





\newpage

References

[1] A Note on Differential Privacy: Defining Resistance to Arbitrary Side Information.  Shiva Prasad Kasiviswanathan Adam Smith
[2] Privacy Odometers and Filters: Pay-as-you-Go Composition. Ryan Rogers, Aaron Roth, Jonathan Ullman, Salil Vadhan


 \newpage
 \section{IGNORE BELOW THIS LINE}
 
 Suppose a new 1-bit is added to both collections, then $R_{n+1}(S)$ is derived by conditioning
 \begin{align*}
R_{n+1}(S) = \frac{ P(S|D \cup 1) }{ P(S|D' \cup 1) } = \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p}
\end{align*}
 
By lemma \eqref{lem:rsbound} and lemma  \eqref{lem:rs1} we have
\begin{align}
 & \frac{P(S-1|D)}{P(S-1|D'} \ge  \frac { P(S|D)q + P(S-1|D)p } { P(S|D')q + P(S-1|D')p} \ge \frac{P(S|D)}{P(SD'} \\
\implies & R_n(S-1) \ge R_{n+1}(S) \ge R_n(S) \\
\implies & R_n(S) \ge R_{n+1}(S+1) \ge R_n(S+1) \label{lem:rsb2}
\end{align}
 
Suppose $R_n$ is $(\epsilon, \delta)$-private. Since $R_n$ is monotonically decreasing with $S$ (lemma  \eqref{lem:rs1} ), there exist two values $\alpha + \beta \le \delta$, such that $R_n$ is upper bounded on the left at a particular limiting value $S_\alpha$ 
 \begin{align}  \label{lem:rsb3}
 R_n(S_\alpha) \le e^\epsilon \text{ and } P_n(S \le S_\alpha) \le \alpha 
\end{align}
And it's low bounded on the right at a particular limiting value $S_\beta$
 \begin{align}  \label{lem:rsb4}
 R_n(S_\beta) \ge \frac{1}{e^\epsilon} \text{ and } P_n(S \ge S_\beta) \le \beta
\end{align}
 
Consider the left (upper) bound first, and recall that according to \eqref{lem:rsb2}
\[
 R_n(S_\alpha) \ge R_{n+1}(S_\alpha+1) \ge R_n(S_\alpha+1)
 \]
 
$R_{n+1}(S_\alpha+1)$ is bounded because  $R_n(S_\alpha)$ is bounded per \eqref{lem:rsb3}.  Hence, $R_{n+1}$ could only be over the bound at $S_\alpha$, however the cumulative sum of probabilities up to $S_\alpha$ is always less for $n+1$ bits than for $n$ bits.
 \begin{align}  \label{lem:rsb5}
P_{n+1}(S \le S_\alpha) \le P_{n}(S \le S_\alpha)
\end{align}
We shall prove \eqref{lem:rsb5} in a moment. The important fact is that $R_n$ upper bounds $R_{n+1}$ at the left tail of distribution of $S$. 

Similarly, 
 
 
 As show by Wang, Y. H. (1993). "On the number of successes in independent trials", for any Poisson Binomial distribution, the probability of consecutive values are related as follows
 \begin{align*}
 & P(S)^2  > P(S-1) \cdot P(S+1) \\
\implies &  \rho(S-1) > \rho(S) \\
\implies & R(S-1) > R(S)
\end{align*}



\begin{lem} \label{lem:rsreduce}
 The privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.
\end{lem}

 the privacy loss reduces as $S$ increases, reaching its maximum in $S=0$ and minimum in $S=N$.







Note that $\frac{P(S | D )}{P(S - 1| D )}$ as a \textbf{probability ratio} between adjacent values of $S$.  It's easy to see that privacy loss ratio maximizes when \textbf{probability ratio} maximizes. 

Recall that $p>q$ and consider two positive values $A$ and $B$ 
\begin{align*}
\frac{ p \cdot A + q } { p + q \cdot A }  \ge \frac{ p \cdot B + q } { p + q \cdot B } \\
p^2A + q^2B \ge p^2 B + q^2 A  \\
A (p^2 - q^2) \ge B (p^2 - q^2)  \\
A \ge B
\end{align*}

The above confirms that the distributions with largest \textbf{probability ratio} also exhibit larger privacy loss ratio. Hence we can focus on studying \textbf{probability ratio}  instead of privacy loss ratio and choose those $D$ that demonstrate sharpest decrease of probabilities in the left tail.




Consider a collection $D$ of $N$ bits, subjected to randomization procedure $\cR$, whereby a bit is flipped with probability $q$ and kept unchanged with probability $p=1-q$.  The outcome of $\cR$ is a $a$ - sum of bits after randomization. The neigboring set $D_m$ is recieved form Assuming that $D$ contains $m$ set bits, we consider a privacy loss ratio $R_s$ computed for the outcome $s$:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 


consisting of $m$ ones and $N-m$ zeros.  Denote probability of number of successes for that collection as $P(S|D)$.
The probability ratio at $s$  is given by:
\[ R_s = \frac{P(s|D)}{P(s-1|D)} \] 

Denote expectation of $s$ as $\mu$:
\[ \mu = mp + (N-m)q \]

For simplicity, denote probabilities at $s$ for $D$ as:
\[ P(s|D) = P_{s} \]

It's known that for all $s < \mu$ , the ratio $R_s$ is greater than $1$ and increasing:

\textbf{Property 1.}
\begin{align}
R_{s-1} = \frac{P_{s-1}}{P_{s-2}} > R_s  = \frac{P_{s}}{P_{s-1}} \\
P^2_{s-1} > P_sP_{s-2} 
\end{align}

Create two collections by adding to D one $1$ and one $0$.  Call them $D_1$ and $D_0$ respectively. The probability of observing $s$ from $D_1$ the is given by:
\[ P(s | D_1) = pP_{s-1} + qP_s \]

Similarly for the second collection (with extra 0):

\[ P(s | D_0) = qP_{s-1} + pP_s \]

Now consider the probability ratio for the collections $D_1$ and $D_0$ collections at some $s$:
\begin{align}
R_s(D_1) = \frac{ pP_{s-1} + qP_s }{pP_{s-2} + qP_{s-1}} \\
R_s(D_0) = \frac{ qP_{s-1} + pP_s }{qP_{s-2} + pP_{s-1}}
\end{align}
 $N$ user bits are subjected to  


\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right) \geq  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq \min \left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \\ 
\end{align*}

where $\lambda_1 + \cdots + \lambda_m = 1$.  Then
 \begin{align*}
  \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} = \sum_{i=1}^m \frac{a_i}{b_i} \lambda_i  \leq  \sum_{i=1}^m \max \left ( \frac{a_i}{b_i} \right ) \lambda_i  = \max \left ( \frac{a_i}{b_i} \right ) \sum_{i=1}^m \lambda_i = \max \left ( \frac{a_i}{b_i} \right ) \\ 
\end{align*}
The low bound is derived in a similar fashion. 
\end{pf}



\section{JUNK}


Given the independence 



 \begin{align*}
 & Pr [ M_r(x) = g ] = \sum_{h \in \N^d} Pr \left [  M(\bbx) = h \right ] \cdot Pr [ g | h ] =  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right ) \\
\implies &  L(g) =  \log  \left ( \frac{  \sum_{h \in \N^d} \left ( Pr \left [  M(\bbx) = h \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h_i]  \right )  }{    \sum_{h' \in \N^d} \left ( Pr \left [  M(\bbx') = h' \right ] \cdot  \prod_{i=1}^d Pr[ g_i | h'_i]  \right )  } \right )
\end{align*}



More formally, the value of $g_l$ is a sum of binomial distributions 
Where $r$ is the number of set bits (both clear and fake) in the dimension $l$.   This allows us to 



The value of $g_l$ distributed as a sum of two binomial variables.  
\[ g_l  \sim Bin(h_l,p) + Bin(n+m-h_l,q) \]
 
 
Applying \eqref{lem:rsbound} gives bounds of $R(S)$

\begin{equation} \label{eq:rsbounds}
 \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  } R(S|D)=  \frac{p \cdot P(S | D ) + q \cdot P( S - 1 | D) } {  p  \cdot P(S -1 | D ) + q \cdot P( S  | D)  }
\end{equation}

\end{document}

